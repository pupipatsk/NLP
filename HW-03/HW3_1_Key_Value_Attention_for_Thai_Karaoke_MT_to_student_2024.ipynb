{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfUmXr1D1ZSR"
      },
      "source": [
        "# Key-Value Attention for Thai Karaoke Character-level Machine Translation (Many-to-Many, encoder-decoder)\n",
        "\n",
        "In this homework, you will create an MT model with attention mechnism that coverts names of Thai 2019 MP candidates from Thai script to Roman(Latin) script. E.g. นิยม-->niyom\n",
        "\n",
        "The use of Pytorch Lightning is optional but recommended. You can use Pytorch if you prefer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "18KMSkqZ-Pt-"
      },
      "outputs": [],
      "source": [
        "# !pip install lightning wandb\n",
        "# !wget https://github.com/Phonbopit/sarabun-webfont/raw/master/fonts/thsarabunnew-webfont.ttf\n",
        "# !curl -L -O https://github.com/Phonbopit/sarabun-webfont/raw/master/fonts/thsarabunnew-webfont.ttf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKCBCWKARZEx",
        "outputId": "6ad8a585-fd68-44ee-deac-37143137cbcc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wandb\n",
        "# !wandb login $WANDB_API_KEY\n",
        "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ka2TN8IV1ZSU"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "mpl.font_manager.fontManager.addfont('thsarabunnew-webfont.ttf') # 3.2+\n",
        "mpl.rc('font', family='TH Sarabun New')\n",
        "import torch\n",
        "# import torchtext\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import lightning as L\n",
        "import numpy as np\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-f_s6vX1ZSZ"
      },
      "source": [
        "## Load Dataset\n",
        "We have generated a toy dataset using names of Thai MP candidates in 2019 Thai General Election from elect.in.th's github(https://github.com/codeforthailand/dataset-election-62-candidates) and tltk (https://pypi.org/project/tltk/) library to convert them into Roman script.\n",
        "\n",
        "```\n",
        "ไกรสีห์ kraisi\n",
        "พัชรี phatri\n",
        "ธีระ thira\n",
        "วุฒิกร wutthikon\n",
        "ไสว sawai\n",
        "สัมภาษณ์  samphat\n",
        "วศิน wasin\n",
        "ทินวัฒน์ thinwat\n",
        "ศักดินัย sakdinai\n",
        "สุรศักดิ์ surasak\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jte-Csrf-4kd",
        "outputId": "a1ba364b-64c2-4875-873c-90bc1808a4be"
      },
      "outputs": [],
      "source": [
        "# !wget https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW8/mp_name_th_en.csv\n",
        "# !curl -L -O https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW8/mp_name_th_en.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L9zXp7KH1ZSa"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "with open('mp_name_th_en.csv') as csvfile:\n",
        "    readCSV = csv.reader(csvfile, delimiter=',')\n",
        "    name_th = []\n",
        "    name_en = []\n",
        "    for row in readCSV:\n",
        "        temp_th = row[0]\n",
        "        temp_en = row[1]\n",
        "\n",
        "        name_th.append(temp_th)\n",
        "        name_en.append(temp_en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCsqrXxu1ZSe"
      },
      "outputs": [],
      "source": [
        "for th, en in zip(name_th[:10],name_en[:10]):\n",
        "    print(th,en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvW8xqT81ZSh"
      },
      "source": [
        "## TODO1: Preprocess dataset\n",
        "* You will need 2 vocabularies (1 for input and another for output)\n",
        "* DON'T FORGET TO INCLUDE special token for padding (for both input and output)\n",
        "* DON'T FORGET TO INCLUDE special token for the end of word symbol (output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rv1Xd9A1ZSi",
        "outputId": "dece74ae-a492-41a7-f07d-2798157c7fcc"
      },
      "outputs": [],
      "source": [
        "#Preprocessing\n",
        "input_chars = list(set(''.join(name_th)))\n",
        "output_chars = list(set(''.join(name_en)))\n",
        "data_size, vocab_size = len(name_th), len(input_chars)+1 #+1 for PADDING\n",
        "output_vocab_size = len(output_chars)+2 #+2 for special end of sentence token, PADDING\n",
        "print('There are %d lines and %d unique characters in your input data.' % (data_size, vocab_size))\n",
        "print('There are %d lines and %d unique characters in your output data.' % (len(name_en), output_vocab_size))\n",
        "\n",
        "maxlen = len( max(name_th, key=len)) #max input length\n",
        "maxlen_out = len( max(name_en, key=len)) #max output length\n",
        "print(\"Max input length:\", maxlen)\n",
        "print(\"Max output length:\", maxlen_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO:\n",
        "# input(TH), special tokens: <PAD>\n",
        "sorted_chars= sorted(input_chars)\n",
        "sorted_chars.insert(0,\"<PAD>\")\n",
        "\n",
        "# output(EN), special tokens: <PAD>, </s>\n",
        "sorted_output_chars = sorted(output_chars)\n",
        "sorted_output_chars.insert(0,\"<PAD>\")\n",
        "sorted_output_chars.insert(1,\"</s>\")\n",
        "\n",
        "def create_tokenizer(sorted_chars: list[str]):\n",
        "    \"\"\"\n",
        "    Create a character-level tokenizer with mappings and encoding/decoding functions.\n",
        "\n",
        "    Args:\n",
        "        sorted_chars (list of str): Sorted list of unique characters.\n",
        "\n",
        "    Returns:\n",
        "        - 'chr_to_int_mapping': Character-to-integer mapping.\n",
        "        - 'int_to_chr_mapping': Integer-to-character mapping.\n",
        "        - 'encode': Function to encode a string into a list of integers.\n",
        "        - 'decode': Function to decode a list of integers into a string.\n",
        "    \"\"\"\n",
        "    chr_to_int_mapping = {ch: i for i, ch in enumerate(sorted_chars)} # chr_to_int_mapping[char] = int\n",
        "    int_to_chr_mapping = {i: ch for i, ch in enumerate(sorted_chars)} # int_to_chr_mapping[int] = char\n",
        "    encode = lambda s: [chr_to_int_mapping[c] for c in s] # encoder: take a string, output a list of integers\n",
        "    decode = lambda l: ''.join([int_to_chr_mapping[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "    return chr_to_int_mapping, int_to_chr_mapping, encode, decode\n",
        "\n",
        "input_stoi, input_itos, input_encode, input_decode = create_tokenizer(sorted_chars)\n",
        "output_stoi, output_itos, output_encode, output_decode = create_tokenizer(sorted_output_chars)\n",
        "\n",
        "# ex.\n",
        "print(input_encode(name_th[0]))\n",
        "print(input_decode(input_encode(name_th[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO:\n",
        "from typing import Callable, List\n",
        "\n",
        "def preprocess_name_to_tensor(\n",
        "    name_data: List[str], encode_fn: Callable[[List[str]], List[int]]\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Convert a list of names into a tensor of encoded integers.\n",
        "\n",
        "    Args:\n",
        "        name_data (List[str]): List of names (strings) to be encoded.\n",
        "        encode_fn (Callable[[List[str]], List[int]]): Encoding function to convert characters to integers.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor of encoded names. Padded if `pad=True`.\n",
        "    \"\"\"\n",
        "    lst = []\n",
        "    for name_str in name_data:\n",
        "        name_lst = [c for c in name_str]\n",
        "        lst.append(torch.tensor(encode_fn(name_lst)))  # Encode and convert to tensor\n",
        "\n",
        "    # Pad sequences to ensure uniform length\n",
        "    return nn.utils.rnn.pad_sequence(lst, batch_first=True, padding_value=0)\n",
        "\n",
        "X = preprocess_name_to_tensor(name_th, input_encode)\n",
        "Y = preprocess_name_to_tensor(name_en, output_encode)\n",
        "\n",
        "# ex.\n",
        "print(X.shape, X)\n",
        "print(Y.shape, Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yirzlseC9NS"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# TODO:\n",
        "class NameDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X.long()\n",
        "        self.y = y.long()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\"x\": self.X[idx], \"y\": self.y[idx]}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "\n",
        "# TODO:\n",
        "class NameDataModule(L.LightningDataModule):\n",
        "    def __init__(self, train_data, y, batch_size, num_workers=0):\n",
        "        super().__init__()\n",
        "        self.train_data = train_data\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "    def setup(self, stage: str):\n",
        "        pass\n",
        "\n",
        "    def collate_fn(self, batch, num_classes=len(input_stoi)):\n",
        "        one_hot_x = torch.stack(\n",
        "            [F.one_hot(b[\"x\"], num_classes=num_classes) for b in batch]\n",
        "        )\n",
        "        return {\"x\": one_hot_x.float(), \"y\": torch.stack([b[\"y\"] for b in batch])}\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_dataset = NameDataset(self.train_data, self.y)\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=self.collate_fn,\n",
        "            num_workers=self.num_workers,\n",
        "        )\n",
        "        return train_loader\n",
        "\n",
        "\n",
        "# Initialize the DataModule\n",
        "batch_size = 16\n",
        "data_module = NameDataModule(X, Y, batch_size=batch_size, num_workers=0)\n",
        "# ex.\n",
        "for batch in data_module.train_dataloader():\n",
        "    print(batch[\"x\"].shape, batch[\"y\"].shape)\n",
        "    print(\"(batch size, max sequence length, num classes)\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFSG1FqK1ZSy"
      },
      "source": [
        "# Attention Mechanism\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAlOrhbismQp"
      },
      "source": [
        "## TODO 2: Code your own (key-value) attention mechnism\n",
        "* PLEASE READ: you DO NOT have to follow all the details in (Daniluk, et al. 2017). You just need to create a key-value attention mechanism where the \"key\" part of the mechanism is used for attention score calculation, and the \"value\" part of the mechanism is used to encode information to create a context vector.  \n",
        "* fill code for one_step_attention function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "avnlc6p9BZDv"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "def one_step_attention(h, s_prev, fc_1, fc_2):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        h (torch.Tensor): Encoder outputs of shape (batch, seq_len, hidden_dim_enc).\n",
        "        s_prev (torch.Tensor): Previous decoder hidden state of shape (batch, hidden_dim_dec).\n",
        "    \"\"\"\n",
        "    # Split into Key-Value\n",
        "    key, value = torch.split(h, h.size(-1)//2, dim=-1)\n",
        "    # key: (batch, seq_len, key_dim=hidden_dim_enc/2), value: (batch, seq_len, value_dim=hidden_dim_enc/2)\n",
        "\n",
        "    # Do concat with s_prev.\n",
        "    #hint: you will need to use s_prev.repeat(...) somehow so that it has the same dimension as the key\n",
        "    #hint2: s_prev.unsqueeze() could also be useful\n",
        "    s_prev = s_prev.unsqueeze(1) # (batch, 1, hidden_dim_dec)\n",
        "    s_prev = s_prev.repeat(1, key.size(1), 1) # (batch, seq_len, hidden_dim_dec)\n",
        "    concat = torch.cat([key, s_prev], dim=-1) # (batch, seq_len, key_dim(=hidden_dim_enc/2) + hidden_dim_dec)\n",
        "\n",
        "    # Attention function:\n",
        "    # use layer(s) from your model to calculate attention_score and then softmax\n",
        "    # fc_1: (batch, seq_len, key_dim + hidden_dim_dec) -> (batch, seq_len, attention_hidden_dim=hidden_dim_enc)\n",
        "    # fc_2: (batch, seq_len, attention_hidden_dim=hidden_dim_enc) -> (batch, seq_len, 1)\n",
        "    attention_score = F.relu(fc_2(F.tanh(fc_1(concat)))) # (batch, seq_len, 1)\n",
        "    attention_score = F.softmax(attention_score, dim=1) # (batch, seq_len, 1)\n",
        "\n",
        "    # calculate a context vector\n",
        "    context = torch.mul(attention_score, value) # (batch, seq_len, value_dim=hidden_dim_enc/2)\n",
        "    context = torch.sum(context, dim=1) # (batch, value_dim=hidden_dim_enc/2)\n",
        "    return context, attention_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zWN02ZtuOIU"
      },
      "source": [
        "# Translation Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0phyUQYg1ZS8"
      },
      "source": [
        "## TODO3: Create and train your encoder/decoder model here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Ji_rUPhK1ZS9"
      },
      "outputs": [],
      "source": [
        "class AttentionModel(L.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        learning_rate=0.01,\n",
        "        criterion=nn.CrossEntropyLoss(),\n",
        "        input_vocab: dict = input_stoi,\n",
        "        output_vocab: dict = output_stoi,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # TODO:\n",
        "        self.n_h = 32  # hidden dimensions for encoder\n",
        "        self.n_s = 64  # hidden dimensions for decoder\n",
        "        self.learning_rate = learning_rate\n",
        "        self.criterion = criterion\n",
        "        self.input_vocab = input_vocab\n",
        "        self.output_vocab = output_vocab\n",
        "\n",
        "        # Encoder: can be any RNN of your choice -> Bidirectional LSTM\n",
        "        # nn.LSTM return: output, (h_n, c_n)\n",
        "        # BiLSTM-Output: h (batch_size, seq_len, 2*n_h), _ (h_n, c_n)\n",
        "        self.encoder = nn.LSTM(\n",
        "            len(self.input_vocab), self.n_h, batch_first=True, bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Decoder: has to be (any) RNNCell since we will need to calculate attention for each timestep manually\n",
        "        self.decoder_lstm_cell = nn.LSTMCell(input_size=self.n_h, hidden_size=self.n_s)\n",
        "\n",
        "        # Attention:\n",
        "        attention_input_dim = self.n_h + self.n_s  # 32+64=96\n",
        "        attention_hidden_dim = 2 * self.n_h  # 2*64=128\n",
        "        self.attention_fc_1 = nn.Linear(attention_input_dim, attention_hidden_dim)\n",
        "        self.attention_fc_2 = nn.Linear(\n",
        "            attention_hidden_dim, 1\n",
        "        )  # Output: scalar attention score\n",
        "\n",
        "        # Output layer:\n",
        "        self.output_layer = nn.Linear(\n",
        "            in_features=self.n_s, out_features=len(self.output_vocab)\n",
        "        )\n",
        "\n",
        "    def forward(self, src, return_attention=False, max_output_length=maxlen_out):\n",
        "        # TODO:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Input sequence (batch_size, seq_len).\n",
        "            return_attention: use only when you want to get the attention scores for visualizing\n",
        "        \"\"\"\n",
        "        # pass the input to the encoder\n",
        "        h, _ = self.encoder(src)  # h: (batch_size, seq_len, 2 * n_h)\n",
        "\n",
        "        # Initialize the LSTM states:\n",
        "        # We have to do this since we are using LSTMCell (https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html)\n",
        "        # these states will get updated while we are decoding\n",
        "        batch_size = src.shape[0]\n",
        "        decoder_s = torch.randn(batch_size, self.n_s).to(\n",
        "            self.decoder_lstm_cell.weight_ih.device\n",
        "        )\n",
        "        decoder_c = torch.randn(batch_size, self.n_s).to(\n",
        "            self.decoder_lstm_cell.weight_ih.device\n",
        "        )\n",
        "\n",
        "        # Initialize prediction tensor\n",
        "        prediction = torch.zeros(\n",
        "            (batch_size, max_output_length, len(self.output_vocab))\n",
        "        ).to(self.decoder_lstm_cell.weight_ih.device)\n",
        "\n",
        "        # Iterate until max_output_length (Decoding)\n",
        "        attention_scores = []  # to store the score for each step\n",
        "        for t in range(max_output_length):\n",
        "            # Perform one step of the attention mechanism to calculate the context vector at timestep t\n",
        "            context, attention_score = one_step_attention(\n",
        "                h, decoder_s, self.attention_fc_1, self.attention_fc_2\n",
        "            )\n",
        "            attention_scores.append(attention_score)\n",
        "            # Feed the context vector to the decoder.\n",
        "            decoder_input = context\n",
        "            decoder_s, decoder_c = self.decoder_lstm_cell(\n",
        "                decoder_input, (decoder_s, decoder_c)\n",
        "            )\n",
        "            # Pass the decoder hidden output to the output layer (softmax)\n",
        "            out = self.output_layer(decoder_s)  # (batch_size, len(self.output_vocab))\n",
        "            # Put the predicted output into the list for this timestep\n",
        "            prediction[:, t] = out\n",
        "\n",
        "        return (prediction, attention_scores if return_attention else None)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        src = batch[\"x\"]\n",
        "        target = batch[\"y\"]\n",
        "        prediction, _ = self(src)\n",
        "        prediction = prediction.reshape(-1, len(self.output_vocab))\n",
        "        target = target.reshape(-1)\n",
        "        loss = self.criterion(prediction, target)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        src = batch[\"x\"]\n",
        "        with torch.no_grad():\n",
        "            prediction, attention_scores = self(src, return_attention=True)\n",
        "            prediction = F.softmax(prediction, dim=-1)\n",
        "            prediction = torch.argmax(prediction, dim=-1)\n",
        "            for pred in prediction:\n",
        "                print(\"\".join(self.output_vocab.lookup_tokens(pred.cpu().numpy())))\n",
        "        return prediction, attention_scores\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return optim.Adam(self.parameters(), lr=self.learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZMi782c1ZTQ"
      },
      "outputs": [],
      "source": [
        "from lightning import Trainer\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "\n",
        "# Initialize\n",
        "# wandb_logger = WandbLogger(project=\"hw3-1-attention\")\n",
        "wandb_logger = None\n",
        "trainer = L.Trainer(max_epochs=100, logger=wandb_logger)\n",
        "model = AttentionModel(\n",
        "    learning_rate=0.01,\n",
        "    criterion=nn.CrossEntropyLoss(),\n",
        "    input_vocab=input_stoi,\n",
        "    output_vocab=output_stoi,\n",
        ")\n",
        "batch_size = 16\n",
        "data_module = NameDataModule(X, Y, batch_size=batch_size, num_workers=0)\n",
        "\n",
        "# Training...\n",
        "trainer.fit(model, data_module)\n",
        "\n",
        "# Save model\n",
        "torch.save(model, \"attention-model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5BLw1Ir1ZTT"
      },
      "source": [
        "# Test Your Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRLjZzBMtCdA"
      },
      "source": [
        "## TODO4: Test your model on 5 examples of your choice including your name!\n",
        "\n",
        "Example Output:\n",
        "```\n",
        "prayutthatha</s></s>aa</s></s>a</s>\n",
        "somchai</s></s></s></s>a</s></s>a</s></s></s></s></s>\n",
        "thanathon</s></s></s></s></s></s></s></s></s></s></s>\n",
        "newin</s>i</s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
        "suthep</s>he</s></s></s></s></s></s></s></s></s></s></s>\n",
        "prawit</s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
        "chatchachatti</s></s>i</s></s></s></s>\n",
        "```\n",
        "\n",
        "<font color='blue'>Paste your model predictions in MyCourseVille</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6stNACsUP9h-"
      },
      "outputs": [],
      "source": [
        "EXAMPLES = ['ประยุทธ','สมชาย','ธนาธร','เนวิน','สุเทพ','ประวิตร์','ชัชชาติ']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbolC8XIhR3t"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsN71S9uQ9wo"
      },
      "outputs": [],
      "source": [
        "output = trainer.predict(model, predict_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o3893RL1ZT8"
      },
      "source": [
        "## TODO 5: Show your visualization of attention scores on one of your example\n",
        "\n",
        "<font color='blue'>Paste your visualization image in MyCourseVille</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHysSqYJ1ZUA"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdktVnMv1ZTh"
      },
      "outputs": [],
      "source": [
        "prediction, attention_scores = zip(*output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "BF6HD99lYlgQ",
        "outputId": "caaa0716-5b99-43e8-950f-dd0127d0fbb7"
      },
      "outputs": [],
      "source": [
        "ax = sns.heatmap(attn_viz, linewidth=0.5)\n",
        "ax.set_yticklabels(output_text,rotation=30)\n",
        "ax.set_xticklabels(xlabels,rotation=60)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1UkIsCztaMS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "datascience",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
