{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iU5fRQwhEdJy"
   },
   "source": [
    "# Subword Tokenization\n",
    "\n",
    "In this exercise, we will learn how to train our own subword tokenizers with different algorithms: BPE and Unigram. We will use `sentencepiece`, a library from Google to help create our tokenizers.\n",
    "\n",
    "## Ref:\n",
    "https://github.com/google/sentencepiece/blob/master/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI9gRZlUE80g"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1pOsV-jaW975"
   },
   "outputs": [],
   "source": [
    "# !wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/pra-apai-manee-ch1-50.txt\n",
    "# !wget https://github.com/Knight-H/thai-lm/raw/refs/heads/master/data/kratoo-40000000-40002000.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSiDpG9WE-cT"
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OQd7M6gLWPLN"
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OifbmMIstzs8"
   },
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-FnIDvb1lMuh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1060318"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pantip_text = []\n",
    "with open('kratoo-40000000-40002000.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "    for json_str in json_list:\n",
    "        result = json.loads(json_str)\n",
    "        pantip_text.append(f\"{result['title']}\\n{result['content']}\\n\")\n",
    "sum([len(t) for t in pantip_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yaaQVXZ8A0j1"
   },
   "outputs": [],
   "source": [
    "with open(\"pra-apai-manee-ch1-50.txt\") as f:\n",
    "  pra_apai_manee_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LksJKc9MA5F_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1100605"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(t) for t in pra_apai_manee_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RbdfkF-vAoie"
   },
   "outputs": [],
   "source": [
    "pantip_train_text = pantip_text[:int(len(pantip_text)*0.8)]\n",
    "pantip_test_text = pantip_text[int(len(pantip_text)*0.8):]\n",
    "\n",
    "pam_train_text = pra_apai_manee_data[:int(len(pra_apai_manee_data)*0.8)] #pam = pra_apai_manee\n",
    "pam_test_text = pra_apai_manee_data[int(len(pra_apai_manee_data)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list to txt file\n",
    "def convert_list_to_txt(lst_data: list, filename: str):\n",
    "    str_data = \"\".join(lst_data)\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(str_data)\n",
    "\n",
    "convert_list_to_txt(pantip_train_text, \"pantip-train.txt\")\n",
    "convert_list_to_txt(pantip_test_text, \"pantip-test.txt\")\n",
    "convert_list_to_txt(pam_train_text, \"pam-train.txt\")\n",
    "convert_list_to_txt(pam_test_text, \"pam-test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhwcH0Aot1XI"
   },
   "source": [
    "## Run tokenizer training\n",
    "\n",
    "The Python wrapper provides multiple APIs for training our tokenizers\n",
    "\n",
    "1. `spm.SentencePieceTrainer.train(input='input.txt', model_prefix='m', vocab_size=vocab_size, model_type=model_type)`\n",
    "  <br> This will output the tokenizer files `m.model` and `m.vocab` that can be later loaded into `SentencePieceProcessor`.\n",
    "  <br><br>\n",
    "2. `spm.SentencePieceTrainer.train(sentence_iterator=iterator, model_writer=obj_with_write_method, vocab_size=vocab_size, model_type=model_type)`\n",
    "  <br> This method will require a file object e.g. `obj_with_write_method = io.BytesIO()`. The advantage of this method is you can run sentencepiece on environments that have limited access to the local file system. But you will still have to save the model file if you want to re-use the model else you will have to train it again.\n",
    "<br><br>\n",
    "3.  `spm.SentencePieceTrainer.train('--input=input.txt --model_prefix=m --vocab_size=vocab_size --model_type=model_type')`\n",
    "<br> Same as no.1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3XeFFYw-T_0"
   },
   "source": [
    "### Unigram tokenizer\n",
    "\n",
    "We are going to start with training a unigram tokenizer. You can use any method of training one. Make sure to set vocab_size to 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bFCfHphd15g9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: pam-train.txt\n",
      "  input_format: \n",
      "  model_prefix: unigram-pam\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: pam-train.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 16314 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=885139\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.954% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=63\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99954\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 16314 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=411778\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 275348 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 16314\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 32181\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 32181 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=49394 obj=42.2355 num_tokens=128673 num_tokens/piece=2.60503\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=40979 obj=36.7371 num_tokens=130046 num_tokens/piece=3.17348\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=30367 obj=36.9771 num_tokens=139359 num_tokens/piece=4.58916\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=29880 obj=36.5016 num_tokens=139582 num_tokens/piece=4.67142\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22205 obj=37.6771 num_tokens=149629 num_tokens/piece=6.73853\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22104 obj=37.2684 num_tokens=149717 num_tokens/piece=6.7733\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=16512 obj=38.7389 num_tokens=161069 num_tokens/piece=9.75466\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=16473 obj=38.3341 num_tokens=161183 num_tokens/piece=9.78468\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12338 obj=40.0409 num_tokens=172981 num_tokens/piece=14.0202\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12331 obj=39.6638 num_tokens=172997 num_tokens/piece=14.0294\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9245 obj=41.4482 num_tokens=185092 num_tokens/piece=20.0208\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9242 obj=41.1146 num_tokens=185096 num_tokens/piece=20.0277\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6930 obj=43.0791 num_tokens=198272 num_tokens/piece=28.6107\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6930 obj=42.7435 num_tokens=198274 num_tokens/piece=28.611\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5196 obj=44.8072 num_tokens=211985 num_tokens/piece=40.7977\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5196 obj=44.4723 num_tokens=211972 num_tokens/piece=40.7952\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3897 obj=46.7216 num_tokens=226813 num_tokens/piece=58.202\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3897 obj=46.3429 num_tokens=226820 num_tokens/piece=58.2037\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2922 obj=48.8639 num_tokens=244481 num_tokens/piece=83.6691\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2922 obj=48.4277 num_tokens=244676 num_tokens/piece=83.7358\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2191 obj=51.2638 num_tokens=265878 num_tokens/piece=121.35\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2191 obj=50.7382 num_tokens=265883 num_tokens/piece=121.352\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1643 obj=53.8622 num_tokens=291545 num_tokens/piece=177.447\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1643 obj=53.2395 num_tokens=291545 num_tokens/piece=177.447\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1232 obj=56.564 num_tokens=320433 num_tokens/piece=260.092\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1232 obj=55.8787 num_tokens=320433 num_tokens/piece=260.092\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1100 obj=57.1231 num_tokens=332377 num_tokens/piece=302.161\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1100 obj=56.8968 num_tokens=332377 num_tokens/piece=302.161\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: unigram-pam.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: unigram-pam.vocab\n"
     ]
    }
   ],
   "source": [
    "# TODO: Train\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='pam-train.txt',\n",
    "    model_prefix='unigram-pam',\n",
    "    vocab_size=1000,\n",
    "    model_type='unigram'\n",
    ")\n",
    "\n",
    "sp_pam_unigram = spm.SentencePieceProcessor(model_file='unigram-pam.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdXPaoW3_v2T"
   },
   "source": [
    "### Q1 MCV\n",
    "\n",
    "How many tokens did you get when tokenizing the following sentence with your unigram tokenizer: <br>\n",
    "'‡∏≠‡∏£‡∏∏‡∏ì‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡πå ‡∏â‡∏±‡∏ô‡πÄ‡∏≠‡∏≤‡∏°‡πÄ‡∏´‡∏™‡∏µ‡∏°‡∏≤‡∏´‡∏≤‡∏° ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏™‡∏ö‡∏≤‡∏¢‡∏î‡∏µ‡πÑ‡∏´‡∏°'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "J1bO3s-z-PLb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_pam = spm.SentencePieceProcessor(model_file='unigram-pam.model') # for later question\n",
    "len(sp_pam_unigram.encode('‡∏≠‡∏£‡∏∏‡∏ì‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡πå ‡∏â‡∏±‡∏ô‡πÄ‡∏≠‡∏≤‡∏°‡πÄ‡∏´‡∏™‡∏µ‡∏°‡∏≤‡∏´‡∏≤‡∏° ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏™‡∏ö‡∏≤‡∏¢‡∏î‡∏µ‡πÑ‡∏´‡∏°', out_type=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKkc1D-hAFxl"
   },
   "source": [
    "### BPE Tokenizer\n",
    "\n",
    "Now try training a BPE tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AiXj57rh-PIv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: pam-train.txt\n",
      "  input_format: \n",
      "  model_prefix: bpe-pam\n",
      "  model_type: BPE\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: pam-train.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 16314 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=885139\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.954% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=63\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99954\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 16314 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 16314\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 32181\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8839 min_freq=120\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3834 size=20 all=3652 active=2394 piece=‡πÉ‡∏´\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2412 size=40 all=5419 active=4161 piece=‡πâ‡∏≠‡∏á\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1781 size=60 all=7328 active=6070 piece=‡πÑ‡∏î\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1469 size=80 all=9357 active=8099 piece=‚ñÅ‡∏û‡∏£‡∏∞\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1191 size=100 all=11534 active=10276 piece=‡∏≤‡∏ß\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1168 min_freq=139\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1002 size=120 all=13824 active=3191 piece=‡∏π‡∏Å\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=876 size=140 all=16549 active=5916 piece=‡πÄ‡∏°\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=748 size=160 all=19242 active=8609 piece=‡∏Ñ‡∏ß‡∏≤‡∏°\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=677 size=180 all=21716 active=11083 piece=‡∏Å‡πç‡∏≤\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=626 size=200 all=24479 active=13846 piece=‡∏™‡∏≤‡∏£\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=623 min_freq=85\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=566 size=220 all=26945 active=3535 piece=‡∏Ç‡∏∂‡πâ‡∏ô\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=509 size=240 all=29932 active=6522 piece=‡∏£‡∏ö\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=470 size=260 all=32718 active=9308 piece=‡πÅ‡∏•\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=436 size=280 all=35246 active=11836 piece=‡πâ‡πç‡∏≤\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=404 size=300 all=37519 active=14109 piece=‡∏ù‡πâ‡∏≤\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=404 min_freq=48\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=382 size=320 all=39880 active=4139 piece=‡∏ô‡πâ‡∏≠‡∏¢\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=356 size=340 all=42278 active=6537 piece=‡∏ô‡∏∂‡∏Å\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=336 size=360 all=44997 active=9256 piece=‡πÅ‡∏ó\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=320 size=380 all=47070 active=11329 piece=‡∏Ç‡∏±‡∏î\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=301 size=400 all=49390 active=13649 piece=‡∏ï‡∏£‡∏≤\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=300 min_freq=33\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=285 size=420 all=51579 active=4541 piece=‡∏û‡∏π\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=269 size=440 all=53523 active=6485 piece=‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=257 size=460 all=55855 active=8817 piece=‡πà‡∏≤‡∏ß\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=244 size=480 all=57964 active=10926 piece=‚ñÅ‡∏Å‡πá\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=234 size=500 all=60309 active=13271 piece=‡∏µ‡∏¢‡∏Å\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=234 min_freq=25\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=224 size=520 all=62342 active=4958 piece=‡∏≤‡∏∞\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=205 size=540 all=64337 active=6953 piece=‡∏£‡∏∏\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=199 size=560 all=66395 active=9011 piece=‡∏ß‡∏≤‡∏ó\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=194 size=580 all=68294 active=10910 piece=‡∏≠‡πà‡∏≠‡∏ô\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=186 size=600 all=70205 active=12821 piece=‡∏ä‡∏ô\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=186 min_freq=20\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=178 size=620 all=71920 active=5149 piece=‚ñÅ‡∏ã‡∏∂‡πà‡∏á\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=171 size=640 all=73659 active=6888 piece=‡∏û‡∏±‡∏Å‡∏ï‡∏£‡πå\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=164 size=660 all=75411 active=8640 piece=‚ñÅ‡∏Ç‡∏∂‡πâ‡∏ô\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=159 size=680 all=77310 active=10539 piece=‡∏û‡∏£‡∏±‡πà‡∏á\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=155 size=700 all=79085 active=12314 piece=‡∏≠‡∏∑‡πâ‡∏ô\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=155 min_freq=17\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=149 size=720 all=80846 active=5691 piece=‡∏™‡∏±‡∏Å\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=144 size=740 all=82546 active=7391 piece=‡πÄ‡∏ä‡∏©‡∏ê‡∏≤\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=140 size=760 all=84297 active=9142 piece=‚ñÅ‡∏Å‡∏£‡∏∞\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=136 size=780 all=85826 active=10671 piece=‡∏Å‡πç‡∏≤‡∏õ‡∏±‡πà‡∏ô\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=132 size=800 all=87264 active=12109 piece=‚ñÅ‡∏®‡∏£‡∏µ‡∏™‡∏∏‡∏ß‡∏£‡∏£‡∏ì\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=131 min_freq=15\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=128 size=820 all=88742 active=5808 piece=‡∏™‡∏ö‡∏≤‡∏¢\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=123 size=840 all=90098 active=7164 piece=‡πÑ‡∏´‡∏•\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=119 size=860 all=91598 active=8664 piece=‡∏ä‡∏≤‡∏ï‡∏¥\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=115 size=880 all=93143 active=10209 piece=‡πÑ‡∏ó\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=113 size=900 all=94526 active=11592 piece=‡∏Ñ‡πç‡∏≤‡∏ô‡∏±‡∏ö\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=113 min_freq=13\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=108 size=920 all=95958 active=6087 piece=‡πÅ‡∏õ\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: bpe-pam.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: bpe-pam.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input='pam-train.txt',\n",
    "    model_prefix='bpe-pam',\n",
    "    vocab_size=1000,\n",
    "    model_type='bpe'\n",
    ")\n",
    "\n",
    "sp_pam_bpe = spm.SentencePieceProcessor(model_file='bpe-pam.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrQwGmL5AMXc"
   },
   "source": [
    "### Q2 MCV\n",
    "\n",
    "How many tokens did you get when tokenizing the following sentence with your BPE tokenizer: <br>\n",
    "'‡∏≠‡∏£‡∏∏‡∏ì‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡πå ‡∏â‡∏±‡∏ô‡πÄ‡∏≠‡∏≤‡∏°‡πÄ‡∏´‡∏™‡∏µ‡∏°‡∏≤‡∏´‡∏≤‡∏° ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏™‡∏ö‡∏≤‡∏¢‡∏î‡∏µ‡πÑ‡∏´‡∏°'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0AXuzyaN-PEr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe = spm.SentencePieceProcessor(model_file='bpe-pam.model') # for later question\n",
    "len(sp_pam_bpe.encode('‡∏≠‡∏£‡∏∏‡∏ì‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡πå ‡∏â‡∏±‡∏ô‡πÄ‡∏≠‡∏≤‡∏°‡πÄ‡∏´‡∏™‡∏µ‡∏°‡∏≤‡∏´‡∏≤‡∏° ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢‡∏™‡∏ö‡∏≤‡∏¢‡∏î‡∏µ‡πÑ‡∏´‡∏°', out_type=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbb6C6-IS_Ly"
   },
   "source": [
    "These are some of your vocabs. Note that you will see \"‚ñÅ\" (U+2581) in every type of tokenizer in SentencePiece since it makes it possible to perform detokenization \\(unsplit your sentences\\) without relying on language-specific resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Aa9j6XrTKjyA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> | <s> | </s> | ‚ñÅ | ‡∏≤ | ‡πÄ | ‡∏ô | ‡∏° | ‡∏¢ | ‡∏Å | ‡∏£ | ‡∏ß | ‡∏î | ‡∏™ | ‡∏á | ‡∏ö | ‡∏Ñ | ‡∏°‡∏≤ | ‡∏≠ | ‡∏• | ‡∏à‡∏∞ | ‡∏ó | ‡πÉ‡∏´‡πâ | ‡∏´ | ‡πÑ‡∏õ | ‡πÑ‡∏°‡πà | ‡πÅ | ‡∏ß‡πà‡∏≤ | ‡∏û | ‡∏∏ | ‡∏µ | ‡πè | ‡∏Ø | ‡∏Ç | ‡∏ä | ‡πÄ‡∏õ‡πá‡∏ô | ‡∏û‡∏£‡∏∞ | ‡πÇ | ‡∏ó‡∏µ‡πà | ‡πÉ‡∏à | ‚ñÅ‡∏à‡∏∞ | ‡∏à | ‡∏∞ | ‡∏¥ | ‡∏ï | ‡∏Å‡πá | ‡∏≠‡∏¢‡∏π‡πà | ‡∏õ | ‡πÑ‡∏î‡πâ | ‡πà | ‡πÑ | ‡πÄ‡∏Ç‡πâ‡∏≤ | ‡∏π | ‚ñÅ‡∏û‡∏£‡∏∞ | ‡πâ‡∏≤ | ‡∏ï‡∏≤‡∏° | ‡πÉ‡∏ô | ‡πâ | ‚ñÅ‡πÅ‡∏•‡πâ‡∏ß | ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô | ‡∏£‡∏≤ | ‡∏® | ‡πÄ‡∏à‡πâ‡∏≤ | ‡πÄ‡∏´‡πá‡∏ô | ‡∏•‡∏≤ | ‡∏Å‡∏±‡∏ô | ‡∏± | ‡∏´‡∏≤ | ‡∏ô‡∏≤‡∏á | ‡∏ó‡∏£‡∏á | ‡∏õ‡∏£‡∏∞ | ‡πå | ‡∏¢‡∏≤ | ‡∏±‡∏Å | ‡πç‡∏≤ | ‡∏ã | ‡∏≤‡∏ô | ‡∏±‡∏á | ‡∏â | ‡∏≠‡∏á‡∏Ñ‡πå | ‡∏±‡∏î | ‡πÅ‡∏•‡πâ‡∏ß | ‡∏≠‡∏ô | ‡∏î‡∏π | ‡∏ñ | ‡∏î‡πâ‡∏ß‡∏¢ | ‡∏°‡∏µ | ‚ñÅ‡∏à‡∏∂‡∏á | ‡∏ô‡∏µ‡πâ | ‡πà‡∏≤ | ‡∏ú | ‡∏ô‡πâ‡∏≠‡∏á | ‡πÅ‡∏ï‡πà | ‡∏ó‡πç‡∏≤ | ‚ñÅ‡∏ô‡∏≤‡∏á | ‚ñÅ‡πÉ‡∏´‡πâ | ‡∏£‡∏±‡∏Å | ‡∏û‡∏µ‡πà | ‡∏Ñ‡∏¥‡∏î | ‡∏•‡∏π‡∏Å | ‡∏û‡∏≤ | ‡∏£‡∏π‡πâ | ‡∏Å‡∏≤‡∏£ | ‡∏Å‡∏±‡∏ö | ‡∏±‡∏ô | ‡∏´‡∏ô‡πâ‡∏≤ | ‡∏Å‡∏£‡∏∞ | ‡∏ß‡∏ô | ‡∏≠‡∏≠‡∏Å | ‡πà‡∏≠ | ‡πÄ‡∏Ç‡∏≤ | ‡∏ñ‡∏∂‡∏á | ‡∏£‡∏∞ | ‡∏Ç‡πâ‡∏≤ | ‡∏±‡∏ö | ‡∏û‡∏• | ‡∏ô‡∏±‡πà‡∏á | ‡∏ó‡∏±‡πâ‡∏á | ‡∏´‡∏ô | ‡∏£‡∏±‡∏ö | ‡∏© | ‡∏Å‡∏• | ‡∏ß‡∏á | ‡∏•‡∏á | ‡∏ù | ‡∏Å‡∏£ | ‡∏û‡∏£ | ‡∏Ñ‡∏ß‡∏≤‡∏° | ‡πÄ‡∏™‡∏µ‡∏¢ | ‡∏î‡∏µ | ‡∏Ç‡∏∂‡πâ‡∏ô | ‡∏≠‡∏á | ‡πà‡∏á | ‡∏ò | ‚ñÅ‡πÅ‡∏ï‡πà | ‡∏Ñ‡∏ô | ‡∏Å‡∏•‡∏±‡∏ö | ‚ñÅ‡∏ù‡πà‡∏≤‡∏¢ | ‡πâ‡∏ô | ‡∏≠‡∏î | ‡∏† | ‡∏´‡∏£‡∏∑‡∏≠ | ‡∏ï‡∏£ | ‡∏∑‡∏≠ | ‡∏ü‡∏±‡∏á | ‡πÅ‡∏°‡πà | ‚ñÅ‡πÑ‡∏°‡πà | ‡πÑ‡∏ß‡πâ | ‡∏¢‡∏±‡∏á | ‚ñÅ‡πÄ‡∏´‡πá‡∏ô | ‡∏ô‡∏≤ | ‡∏Ç‡∏≠ | ‡∏°‡∏¥ | ‡∏ô‡πâ‡πç‡∏≤ | ‡∏´‡∏• | ‡∏î‡∏±‡∏á | ‚ñÅ‡∏û‡∏≠ | ‚ñÅ‡∏ó‡∏±‡πâ‡∏á | ‡∏ä‡πà‡∏ß‡∏¢ | ‡∏™‡∏° | ‡∏ô‡∏±‡πâ‡∏ô | ‡∏£‡∏¥ | ‡∏ó‡∏±‡∏û | ‡∏ï‡πâ‡∏≠‡∏á | ‡∏ß‡∏±‡∏ô | ‡∏≠‡∏≤ | ‡∏ô‡πâ‡∏≠‡∏¢ | ‡∏£‡∏ö | ‡∏¥‡∏ô | ‡∏≠‡∏¢‡πà‡∏≤ | ‡πÄ‡∏≠‡∏≤ | ‡∏à‡∏ô | ‡πÄ‡∏£‡∏≤ | ‡∏™‡∏∏‡∏î | ‡πÄ‡∏™‡∏µ‡∏¢‡∏á | ‡∏Ç‡πâ‡∏≤‡∏á | ‡∏´‡∏•‡∏±‡∏á | ‡∏ï‡∏µ | ‡∏ï‡∏±‡∏ß | ‡∏•‡∏∞ | ‡∏™‡∏∏ | ‡∏ß‡∏±‡∏á | ‡∏ó‡∏∏‡∏Å | ‡πà‡∏ô | ‡∏∂‡∏Å | ‡∏ô‡∏∂‡∏Å | ‡πÄ‡∏ù‡πâ‡∏≤ | ‡∏ô‡∏≤‡∏¢ | ‡∏ù‡∏£‡∏±‡πà‡∏á | ‡∏ó‡∏π‡∏• | ‡πÄ‡∏™ | ‡∏ß‡∏¥ | ‡∏õ‡∏• | ‚ñÅ‡∏ñ‡∏∂‡∏á | ‡∏ï‡∏≤‡∏¢ | ‡πÉ‡∏Ñ‡∏£ | ‡∏≠‡∏Å | ‡∏≠‡∏± | ‡∏ï‡∏≤ | ‡πÄ‡∏£‡∏∑‡∏≠ | ‡∏à‡∏∂‡∏á | ‡πÅ‡∏• | ‡∏µ‡πà | ‡∏±‡πà‡∏á | ‡πÅ‡∏™‡∏ô | ‡∏™‡∏≠‡∏á | ‡∏Ç‡∏≠‡∏á | ‡πá | ‡∏•‡∏µ | ‡∏µ‡πâ | ‡∏à‡∏¥‡∏ï | ‡∏´‡∏°‡∏≤‡∏¢ | ‡πâ‡∏° | ‡πÅ‡∏à‡πâ‡∏á | ‡∏±‡πà‡∏ô | ‡∏™‡∏±‡πà‡∏á | ‡∏£‡∏≤‡∏ä | ‡∏û‡∏¥ | ‡πÄ‡∏´ | ‡∏´‡∏≤‡∏¢ | ‡πâ‡∏≠‡∏á | ‡πÄ‡∏°‡∏∑‡∏≠‡∏á | ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ | ‡∏Å‡∏•‡∏≤‡∏á | ‡∏Å‡∏©‡∏±‡∏ï‡∏£‡∏¥‡∏¢‡πå | ‡∏¢‡∏¥‡πà‡∏á | ‡∏ï‡∏£‡∏±‡∏™ | ‡∏∂‡∏á | ‡πÄ‡∏•‡∏¢ | ‡πÄ‡∏•‡πà‡∏≤ | ‡∏ó‡∏≤‡∏á | ‡∏∏‡∏î | ‡∏®‡∏£‡∏µ | ‡πÄ‡∏Ñ‡∏¢ | ‡πÑ‡∏´‡∏ô | ‡∏™‡∏≤‡∏° | ‡∏´‡∏ô‡∏µ | ‡∏ì | ‡∏°‡∏±‡∏ô | ‡∏∑‡πâ‡∏≠ | ‡∏Ñ‡πà‡∏≠‡∏¢ | ‡∏ä‡∏≤‡∏¢ | ‡∏û‡∏£‡∏≤‡∏´‡∏°‡∏ì‡πå | ‚ñÅ‡∏≠‡∏¢‡πà‡∏≤ | ‡∏ç | ‡∏ó‡∏µ | ‡∏ô‡∏¥ | ‡∏ô‡πà‡∏≤ | ‡∏™‡∏¥‡πâ‡∏ô | ‡∏â‡∏±‡∏ô | ‡∏Å‡∏≤‡∏¢ | ‡∏•‡∏±‡∏á‡∏Å‡∏≤ | ‚ñÅ‡∏î‡πâ‡∏ß‡∏¢ | ‡∏Ñ‡∏≠‡∏¢ | ‡∏ö‡∏≠‡∏Å | ‡∏™‡∏¥ | ‡∏ü | ‡∏™‡∏á‡∏™‡∏≤‡∏£ | ‡∏û‡πà‡∏≠ | ‡∏¢‡∏á | ‡∏à‡∏£‡∏¥‡∏á | ‡∏ä‡∏≤‡∏ß | ‡∏ñ‡∏≤‡∏° | ‡πÑ‡∏£ | ‡∏ó‡∏´‡∏≤‡∏£ | ‡∏ï‡∏±‡πâ‡∏á | ‚ñÅ‡∏≠‡∏±‡∏ô | ‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß | ‡∏õ‡∏£ | ‡∏ú‡∏π‡πâ | ‡∏û‡∏ß‡∏Å | ‡∏™‡∏≤‡∏£ | ‡∏ä‡∏° | ‡∏®‡∏∂‡∏Å | ‡∏Ñ‡πç‡∏≤ | ‚ñÅ‡πÄ‡∏õ‡πá‡∏ô | ‡∏ó‡∏≠‡∏á | ‡∏≠‡∏ö | ‡πÉ‡∏´‡∏ç‡πà | ‡∏ñ‡∏∑‡∏≠ | ‡∏™‡∏≤‡∏ß | ‡∏û‡∏£‡∏∞‡∏≠‡∏†‡∏±‡∏¢ | ‡∏à‡∏á | ‡∏™‡∏≤ | ‡∏à‡∏±‡∏ö | ‡∏±‡πâ‡∏ô | ‡∏û‡∏•‡∏≤‡∏á | ‚ñÅ‡∏°‡∏≤ | ‡∏¢‡∏Å | ‚ñÅ‡∏ö‡πâ‡∏≤‡∏á | ‡πÑ‡∏û‡∏£‡πà | ‡∏•‡∏° | ‡∏•‡πâ‡∏ß‡∏ô | ‚ñÅ‡∏ï‡πà‡∏≤‡∏á | ‡∏£‡πâ‡∏≠‡∏¢ | ‡∏û‡∏ö | ‡∏á‡∏≤‡∏° | ‡πÅ‡∏Å‡∏•‡πâ‡∏á | ‡∏≠‡∏≤‡∏¢ | ‡∏à‡∏∞‡πÑ‡∏î‡πâ | ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á | ‡∏≠‡∏¢‡πà‡∏≤‡∏á | ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á | ‡∏Å‡∏•‡∏±‡∏ß | ‡∏•‡∏≤‡∏¢ | ‡∏à‡πç‡∏≤ | ‡∏ï‡πà‡∏≤‡∏á | ‡∏™‡∏¥‡∏ô‡∏™‡∏°‡∏∏‡∏ó‡∏£ | ‚ñÅ‡∏û‡∏ß‡∏Å | ‡∏°‡πâ‡∏≤ | ‡∏•‡πç‡∏≤ | ‡∏ô‡∏µ‡πà | ‡∏ú‡∏≤ | ‡πÅ‡∏Å‡πâ‡∏ß | ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ | ‚ñÅ‡∏Ñ‡∏£‡∏±‡πâ‡∏ô | ‚ñÅ‡∏à‡∏ô | ‚ñÅ‡πÅ‡∏°‡πâ‡∏ô | ‡∏™‡∏≤‡∏¢ | ‡∏û‡∏±‡∏ô | ‡∏û‡∏£‡∏∞‡∏≠‡∏á‡∏Ñ‡πå | ‡∏û‡∏£‡πâ‡∏≠‡∏° | ‡∏ß‡∏≤‡∏¢ | ‡∏ä‡∏¥‡∏á | ‡∏´‡πâ‡∏≠‡∏á | ‡∏£‡πâ‡∏≠‡∏á | ‡∏™‡∏π‡πâ | ‚ñÅ‡∏à‡∏á | ‡∏•‡∏¥ | ‡∏£‡∏≤‡∏¢ | ‡∏•‡πà‡∏≠ | ‡∏à‡∏≤‡∏Å | ‡πâ‡∏ß | ‡∏ó‡πà‡∏≤‡∏ô | ‡∏£‡∏≠‡∏á | ‡πÄ‡∏î‡∏¥‡∏ô | ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å | ‡∏Ç‡∏±‡∏î | ‡πÄ‡∏´‡∏•‡πà‡∏≤ | ‡∏Å‡∏∏‡∏°‡∏≤‡∏£ | ‡∏ú‡∏• | ‡∏õ‡πà‡∏≤ | ‡∏π‡πà | ‡∏Ñ‡∏π‡πà | ‡∏£‡∏π‡∏õ | ‡∏Å‡∏¥‡∏ô | ‡∏û‡∏≠ | ‡∏£‡πà‡πç‡∏≤ | ‡πÇ‡∏â‡∏° | ‚ñÅ‡∏ñ‡πâ‡∏≤ | ‡∏Ñ‡∏á | ‡πà‡∏≤‡∏¢ | ‡πÉ‡∏ä‡πâ | ‡∏ï‡∏≠‡∏ö | ‡∏´‡∏•‡∏á | ‡πÑ‡∏•‡πà | ‡∏à‡∏±‡∏î | ‡∏î‡∏±‡∏ö | ‚ñÅ‡πÄ‡∏°‡∏∑‡πà‡∏≠ | ‡∏ö‡∏ô | ‡∏≠‡πà‡∏≠‡∏ô | ‡πÅ‡∏™‡∏á | ‡∏Ñ‡∏∑‡∏ô | ‡πÉ‡∏™‡πà | ‡πÅ‡∏Ñ‡πâ‡∏ô | ‡∏£‡∏ñ | ‡∏ï‡∏£‡∏á | ‡πÅ‡∏ï‡πà‡∏á | ‡πÅ‡∏ô‡πà | ‡πÄ‡∏ä‡∏¥‡∏ç | ‡∏ä‡∏∑‡πà‡∏ô | ‡∏ñ‡∏ß‡∏≤‡∏¢ | ‡πÇ‡∏´ | ‡∏à‡∏£ | ‡∏°‡∏¥‡πÑ‡∏î‡πâ | ‡∏ô‡∏≠‡∏ô | ‡∏∏‡∏Å | ‡∏ä‡∏ß‡∏ô | ‡πÄ‡∏°‡∏µ‡∏¢ | ‡∏≠‡∏≤‡∏•‡∏±‡∏¢ | ‡πâ‡∏≠‡∏° | ‡∏•‡∏±‡∏ö | ‡πÑ‡∏´‡∏ß | ‚ñÅ‡πÅ‡∏°‡πâ | ‡∏ö‡∏¥‡∏î‡∏≤ | ‡∏´‡∏ç‡∏¥‡∏á | ‡∏´‡∏•‡∏±‡∏ö | ‡∏î‡∏≠‡∏Å | ‡∏Å‡∏•‡πâ‡∏≤ | ‡∏Ç‡∏≤‡∏î | ‡∏à‡∏±‡∏Å | ‡πÑ‡∏°‡πà‡∏°‡∏µ | ‡∏ö‡∏≤‡∏ó | ‡πÄ‡∏™‡∏ô‡∏≤ | ‡∏¢‡πå | ‡∏ä‡πà‡∏≤‡∏á | ‡πÇ‡∏®‡∏Å | ‡∏ß‡∏≤‡∏á | ‡∏ï‡∏¥‡∏î | ‡πÄ‡∏™‡∏£‡πá‡∏à | ‡∏£‡πâ‡∏≠‡∏ô | ‡∏Ñ‡∏∏‡∏ì | ‡∏ú‡∏±‡∏ß | ‡∏ô‡∏±‡∏Å | ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏≤‡∏° | ‡∏û‡∏±‡∏Å‡∏ï‡∏£‡πå | ‡∏´‡∏ô‡πà‡∏≠ | ‡πâ‡∏≠‡∏¢ | ‚ñÅ‡∏ã‡∏∂‡πà‡∏á | ‡∏ï‡∏∞ | ‡∏´‡πâ‡∏≤‡∏° | ‡∏û‡∏£‡∏≤‡∏¢ | ‡∏ü‡πâ‡∏≤ | ‡πÑ‡∏â‡∏ô | ‡πÉ | ‡∏ï‡∏Å | ‡πÄ‡∏°‡∏∑‡πà‡∏≠ | ‡∏¢‡∏® | ‡∏ä‡∏• | ‡∏î‡πç‡∏≤ | ‡∏´‡∏ô‡∏∂‡πà‡∏á | ‡∏ú‡∏±‡∏ô | ‡πÉ‡∏î | ‡∏™‡∏±‡∏Å | ‡∏£‡πâ‡∏≤‡∏¢ | ‡∏ß‡∏¥‡πà‡∏á | ‡πÅ‡∏Å‡πâ | ‡∏¢‡∏≤‡∏° | ‡∏®‡∏£‡∏µ‡∏™‡∏∏‡∏ß‡∏£‡∏£‡∏ì | ‡∏õ‡∏∑‡∏ô | ‡∏Ü‡πà‡∏≤ | ‡∏Ç‡∏±‡∏ö | ‡∏Ç‡∏ß‡∏≤ | ‡πÑ‡∏ü | ‡∏û‡∏π‡∏î | ‡∏´‡∏°‡∏≠‡∏á | ‡∏Å‡πá‡πÑ‡∏°‡πà | ‡∏Å‡πç‡∏≤‡∏•‡∏±‡∏á | ‡∏£‡∏±‡∏Å‡∏©‡∏≤ | ‡πÄ‡∏ä‡πà‡∏ô | ‡∏∏‡πà‡∏° | ‡∏ú‡∏µ | ‡∏´‡∏≤‡∏ç | ‡πÄ‡∏•‡πà‡∏ô | ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠ | ‡∏£‡∏µ‡∏ö | ‡∏ñ‡∏π‡∏Å | ‡∏ä‡∏±‡∏¢ | ‡∏ö‡∏∏‡∏ï‡∏£‡∏µ | ‡∏ü‡∏±‡∏ô | ‡∏ö‡πâ‡∏≤‡∏á | ‡πÄ‡∏≠‡πã‡∏¢ | ‡∏™‡∏á‡∏™‡∏±‡∏¢ | ‡∏ú‡∏¥‡∏î | ‡∏ô‡∏¥‡πà‡∏á | ‡∏ä‡∏∑‡πà‡∏≠ | ‡πÄ‡∏ñ‡∏¥‡∏î | ‡∏ú‡πà‡∏≠‡∏ô | ‡∏´‡∏•‡∏≤‡∏ô | ‡∏™‡∏µ‡πà | ‡∏ä‡∏≤‡∏ï‡∏¥ | ‡∏≠‡∏µ | ‡∏õ‡∏≤‡∏Å | ‡∏ä‡πâ‡∏≤ | ‡∏∂ | ‡πÅ‡∏ï‡∏Å | ‡∏ï‡∏£‡∏≤ | ‡∏£‡∏ì | ‡∏•‡∏≠‡∏á | ‡∏õ‡∏µ | ‡∏´‡∏°‡∏≠ | ‡πÄ‡∏à‡πâ‡∏≤‡∏û‡∏£‡∏≤‡∏´‡∏°‡∏ì‡πå | ‡∏û‡∏µ‡πà‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á | ‡∏ï‡πà‡∏≠ | ‡∏û‡∏•‡∏≠‡∏¢ | ‡πÇ‡∏â‡∏°‡∏¢‡∏á | ‡πÄ‡∏ô‡∏ï‡∏£ | ‡∏´‡∏±‡∏Å | ‡∏Å‡∏≠‡∏î | ‡πÄ‡∏ä‡∏¢ | ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á | ‡∏¢‡∏¥‡πâ‡∏° | ‡∏Ñ‡πà‡πç‡∏≤ | ‡∏ô‡∏≠‡∏Å | ‡∏Ç‡∏ß‡∏±‡∏ç | ‡∏ã‡πâ‡πç‡∏≤ | ‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå | ‡∏ó‡∏∏‡∏Å‡∏Ç‡πå | ‡πÅ‡∏Ç‡∏Å | ‡πÄ‡∏¢‡πá‡∏ô | ‡∏´‡∏ô‡∏±‡∏Å‡∏´‡∏ô‡∏≤ | ‡∏±‡πâ‡∏á | ‡∏õ‡∏¥‡∏î | ‡πÇ‡∏õ‡∏£‡∏î | ‡πâ‡∏á | ‡∏Å‡πç‡∏≤‡∏õ‡∏±‡πà‡∏ô | ‡πÄ‡∏£‡∏µ‡∏¢‡∏á | ‡πÅ‡∏£‡∏á | ‡∏™‡∏¥‡πà‡∏á | ‡πÄ‡∏®‡∏£‡πâ‡∏≤'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_vocabs = [sp_pam.id_to_piece(id) for id in range(sp_pam.get_piece_size())]\n",
    "\" | \".join(unigram_vocabs[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2TsXA0UqN5LN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> | <s> | </s> | ‡πâ‡∏≤ | ‡πà‡∏≤ | ‡∏≠‡∏á | ‡∏£‡∏∞ | ‡πç‡∏≤ | ‡∏£‡∏≤ | ‡∏≠‡∏¢ | ‡πà‡∏á | ‡∏°‡∏≤ | ‡∏à‡∏∞ | ‡∏±‡∏á | ‡∏±‡∏ô | ‚ñÅ‡πÄ | ‡∏≤‡∏¢ | ‡πâ‡∏ß | ‡∏±‡∏ö | ‡∏µ‡πà | ‡∏°‡πà | ‡∏≠‡∏ô | ‡πÉ‡∏´ | ‡∏≤‡∏° | ‡πâ‡∏ô | ‡πá‡∏ô | ‡∏û‡∏£‡∏∞ | ‡∏µ‡∏¢ | ‡∏≤‡∏á | ‡∏Å‡∏• | ‡πâ‡∏á | ‡∏±‡∏Å | ‡∏´‡∏ô | ‡πÉ‡∏´‡πâ | ‡πÑ‡∏°‡πà | ‡∏´‡∏• | ‡πà‡∏ô | ‡∏∂‡∏á | ‚ñÅ‡πÅ | ‡∏ó‡∏± | ‡∏ï‡∏£ | ‡∏≤‡∏£ | ‡πâ‡∏≠‡∏á | ‡πÑ‡∏õ | ‡∏¥‡∏î | ‡∏Ç‡πâ‡∏≤ | ‡∏ß‡πà‡∏≤ | ‡∏´‡∏° | ‡∏Ñ‡∏£ | ‡∏∑‡∏≠ | ‡∏•‡πâ‡∏ß | ‡πÄ‡∏õ | ‡πÄ‡∏™ | ‡∏õ‡∏£‡∏∞ | ‡∏≤‡∏ô | ‡∏±‡πà‡∏á | ‚ñÅ‡πè | ‚ñÅ‡∏Ø | ‡∏ó‡∏µ‡πà | ‡∏≠‡∏Å | ‡πÄ‡∏• | ‡∏¥‡∏ô | ‡πÑ‡∏î | ‡∏û‡∏• | ‡∏ó‡∏£ | ‡∏±‡∏î | ‡∏ô‡∏≤‡∏á | ‡∏∂‡∏Å | ‡πÑ‡∏î‡πâ | ‡∏π‡πà | ‚ñÅ‡∏à‡∏∞ | ‡∏Ñ‡πå | ‡∏µ‡πâ | ‡∏û‡∏£ | ‡πÄ‡∏õ‡πá‡∏ô | ‡∏™‡∏∏ | ‡∏ó‡∏±‡πâ‡∏á | ‡∏≠‡∏° | ‡∏±‡∏¢ | ‡πÄ‡∏£ | ‡∏´‡πá‡∏ô | ‚ñÅ‡∏à | ‚ñÅ‡∏û‡∏£‡∏∞ | ‡∏Å‡πá | ‡πÉ‡∏à | ‡∏≠‡∏≤ | ‡∏∑‡πà | ‡πà‡∏≤‡∏á | ‡∏ï‡πà | ‡∏Å‡∏£ | ‡∏¥‡∏á | ‡∏ß‡∏á | ‡∏ß‡∏ô | ‡∏∑‡∏≠‡∏ô | ‡πÄ‡∏à | ‡∏π‡πâ | ‡∏µ‡∏¢‡∏á | ‡∏≠‡∏¢‡∏π‡πà | ‡∏£‡∏£ | ‡∏ï‡∏≤‡∏° | ‚ñÅ‡∏û | ‡πâ‡∏ß‡∏¢ | ‡∏≤‡∏ß | ‡∏ñ‡∏∂‡∏á | ‡∏Ñ‡∏• | ‡∏±‡πâ‡∏ô | ‡∏£‡∏µ | ‡πÄ‡∏Ç | ‡∏î‡πâ‡∏ß‡∏¢ | ‡∏™‡∏° | ‡∏≠‡∏á‡∏Ñ‡πå | ‡∏™‡∏ô | ‡∏≤‡∏Å | ‚ñÅ‡πÅ‡∏•‡πâ‡∏ß | ‡πÄ‡∏ä | ‡∏±‡∏ß | ‡∏¢‡πå | ‡πÉ‡∏ô | ‡∏Ñ‡∏ß | ‡∏ô‡πâ | ‡∏´‡∏°‡∏∑‡∏≠‡∏ô | ‚ñÅ‡∏™ | ‡∏π‡∏Å | ‡∏≠‡∏ö | ‡∏Å‡∏£‡∏∞ | ‡πÄ‡∏à‡πâ‡∏≤ | ‡∏ó‡∏£‡∏á | ‡∏•‡∏≤ | ‡∏Å‡∏±‡∏ô | ‡∏°‡∏µ | ‡πà‡∏≤‡∏¢ | ‡∏û‡∏£‡∏≤ | ‡∏¥‡πà‡∏á | ‡πÄ‡∏Ç‡πâ‡∏≤ | ‡πÄ‡∏´‡πá‡∏ô | ‡∏¥‡∏ï | ‡∏™‡∏á | ‡∏≠‡∏î | ‡∏ì‡πå | ‡∏ß‡∏¢ | ‡πâ‡∏° | ‡∏Ñ‡∏¥‡∏î | ‡πÄ‡∏° | ‡πÄ‡∏Å | ‡πÄ‡∏î | ‚ñÅ‡∏ô‡∏≤‡∏á | ‡∏ß‡∏≤ | ‡∏∏‡∏Å | ‚ñÅ‡πÉ‡∏´‡πâ | ‡∏î‡∏π | ‡∏´‡∏≤ | ‚ñÅ‡∏≠ | ‚ñÅ‡∏à‡∏∂‡∏á | ‡∏ó‡πç‡∏≤ | ‡∏•‡∏á | ‡∏£‡∏±‡∏Å | ‡πÄ‡∏Ñ | ‡πÅ‡∏•‡πâ‡∏ß | ‡πà‡∏≤‡∏ô | ‡∏û‡∏µ‡πà | ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô | ‡∏±‡πà‡∏ô | ‡∏Ñ‡∏ß‡∏≤‡∏° | ‡∏¢‡∏á | ‡∏≠‡∏¢‡πà‡∏≤ | ‡∏´‡∏£ | ‡∏°‡∏¥ | ‡∏∑‡∏ô | ‡∏ä‡πà | ‡∏Å‡∏≤‡∏£ | ‡∏±‡∏ç | ‚ñÅ‡πÑ‡∏°‡πà | ‡∏ù‡πà‡∏≤‡∏¢ | ‡∏®‡∏£‡∏µ | ‡πâ‡∏≤‡∏á | ‡∏ß‡∏Å | ‡πâ‡∏≠‡∏° | ‡∏∑‡∏≠‡∏á | ‡∏ô‡πâ‡∏≠‡∏á | ‡∏¢‡∏ß | ‡∏û‡∏≤ | ‡πÅ‡∏Å | ‡∏Å‡πç‡∏≤ | ‡πà‡∏≠‡∏ô | ‡∏∑‡πà‡∏ô | ‡∏´‡∏ô‡πâ‡∏≤ | ‡∏¢‡∏≤ | ‡∏î‡∏µ | ‡∏±‡πâ‡∏á | ‚ñÅ‡∏ó‡∏±‡πâ‡∏á | ‡∏õ‡∏£‡∏≤ | ‡∏Ñ‡∏ô | ‡πÄ‡∏ô | ‡∏´‡∏ß | ‡∏£‡∏±‡∏ö | ‡πÅ‡∏ï‡πà | ‡πâ‡∏≤‡∏¢ | ‡∏±‡∏™ | ‡πÄ‡∏´‡∏• | ‡∏î‡∏≤ | ‡∏™‡πç‡∏≤ | ‡∏ô‡∏µ‡πâ | ‡∏™‡∏≤‡∏£ | ‡∏Å‡∏±‡∏ö | ‡∏•‡∏π‡∏Å | ‡∏•‡∏∞ | ‚ñÅ‡∏ï | ‡∏£‡∏π‡πâ | ‡∏∑‡πà‡∏≠ | ‚ñÅ‡∏ù‡πà‡∏≤‡∏¢ | ‡∏∂‡πà‡∏á | ‡∏•‡∏±‡∏á | ‡∏≤‡∏î | ‡∏∑‡πâ | ‡∏Å‡∏≤ | ‡∏Ç‡∏∂ | ‡∏ô‡∏±‡πà‡∏á | ‡πÄ‡∏ó | ‚ñÅ‡πÄ‡∏´‡πá‡∏ô | ‡∏ü‡∏±‡∏á | ‡πâ‡∏≠‡∏¢ | ‡πÑ‡∏£ | ‡∏Ç‡∏∂‡πâ‡∏ô | ‡πÄ‡∏™‡∏µ‡∏¢ | ‚ñÅ‡πÅ‡∏ï‡πà | ‡∏ö‡∏∏ | ‡∏™‡∏≤ | ‡πÑ‡∏ß | ‡∏ó‡∏∏‡∏Å | ‡∏Å‡∏•‡∏±‡∏ö | ‡∏™‡∏∏‡∏î | ‡∏±‡∏ï | ‡πÉ‡∏Ñ‡∏£ | ‡∏ô‡πâ‡πç‡∏≤ | ‡∏ä‡∏≤ | ‡∏∏‡∏î | ‡∏ó‡∏±‡∏û | ‡∏ß‡∏±‡∏ô | ‡∏™‡∏≠‡∏á | ‡∏ô‡∏≤ | ‡∏´‡∏¢ | ‡∏ï‡∏≤ | ‡∏£‡∏ö | ‚ñÅ‡∏°‡∏≤ | ‡πà‡∏≠ | ‡∏´‡∏£‡∏∑‡∏≠ | ‡∏ó‡∏π | ‡∏¢‡∏±‡∏á | ‡∏£‡∏á | ‡∏à‡∏£ | ‡∏õ‡∏£ | ‚ñÅ‡∏ö | ‡πÑ‡∏ß‡πâ | ‡∏î‡∏±‡∏á | ‡∏ß‡∏¥ | ‡∏ä‡πà‡∏ß‡∏¢ | ‡∏õ‡∏• | ‡∏≠‡∏≠‡∏Å | ‡∏±‡∏ï‡∏£ | ‡πÄ‡∏û | ‡∏™‡∏¥ | ‡πÅ‡∏à | ‡πÅ‡∏• | ‡πá‡∏à | ‡∏¥‡∏¢‡πå | ‚ñÅ‡∏û‡∏≠ | ‡∏°‡∏≤‡∏£ | ‡∏Ñ‡πà | ‡∏ß‡∏£‡∏£ | ‡∏´‡∏°‡∏ì‡πå | ‡∏Ñ‡πç‡∏≤ | ‡πÄ‡∏Ç‡∏≤ | ‡∏ô‡∏±‡πâ‡∏ô | ‡∏Å‡∏© | ‡πÄ‡∏¢ | ‡∏Ç‡πâ‡∏≤‡∏á | ‡∏´‡∏°‡∏≤ | ‡πÄ‡∏ß | ‡πÑ‡∏û‡∏£ | ‡∏´‡∏•‡∏±‡∏á | ‡∏à‡∏¥‡∏ï | ‡∏û‡∏£‡∏≤‡∏´‡∏°‡∏ì‡πå | ‡πâ‡πç‡∏≤ | ‚ñÅ‡∏ñ‡∏∂‡∏á | ‡∏Ç‡∏≠ | ‡∏ó‡∏π‡∏• | ‡∏™‡∏≤‡∏° | ‡∏∑‡πâ‡∏≠ | ‡∏ß‡∏≤‡∏¢ | ‡∏≠‡∏† | ‡∏ó‡∏≤‡∏á | ‚ñÅ‡πÅ‡∏° | ‡∏ß‡∏±‡∏á | ‡πÇ‡∏â | ‡πà‡∏° | ‡∏à‡∏ô | ‚ñÅ‡πÄ‡∏õ | ‡∏±‡∏ï‡∏£‡∏¥‡∏¢‡πå | ‡∏∑‡πà‡∏≠‡∏á | ‡∏™‡∏±‡πà‡∏á | ‡πÅ‡∏°‡πà | ‚ñÅ‡∏ä | ‡∏ù‡πâ‡∏≤ | ‡πÇ‡∏â‡∏° | ‡∏£‡∏≤‡∏ä | ‡∏ù‡∏£ | ‚ñÅ‡∏ñ | ‡∏ù‡∏£‡∏±‡πà‡∏á | ‡∏¥‡πå | ‡∏•‡∏° | ‡πÅ‡∏ï | ‚ñÅ‡πÄ‡∏õ‡πá‡∏ô | ‡∏´‡∏≤‡∏£ | ‡∏∑‡πâ‡∏ô | ‡πÄ‡∏´ | ‡πâ‡∏≠‡∏ô | ‡∏ï‡∏≤‡∏¢ | ‡∏∏‡πà‡∏á | ‡∏ï‡∏±‡∏ß | ‡∏≠‡∏¢‡πà‡∏≤‡∏á | ‡∏•‡∏µ | ‡∏ú‡∏π‡πâ | ‡∏ô‡πâ‡∏≠‡∏¢ | ‡∏â‡∏±‡∏ô | ‡∏ï‡∏£‡∏µ | ‡∏Å‡∏∏ | ‡∏©‡∏≤ | ‡∏∏‡∏ó‡∏£ | ‡∏ñ‡∏≤‡∏° | ‡∏Ç‡∏≠‡∏á | ‡∏û‡∏£‡πâ‡∏≠‡∏° | ‡∏ä‡∏µ | ‡∏™‡∏£ | ‡πÄ‡∏≠ | ‡∏∏‡∏á | ‡∏û‡∏•‡∏≤‡∏á | ‡∏ï‡∏µ | ‡∏™‡∏°‡∏∏‡∏ó‡∏£ | ‡∏´‡∏≤‡∏¢ | ‡∏ó‡∏µ | ‡∏ß‡∏£‡∏£‡∏ì | ‡πÄ‡∏•‡∏µ‡πâ | ‡∏ô‡∏∂‡∏Å | ‡∏à‡∏∂‡∏á | ‡∏´‡∏°‡∏≤‡∏¢ | ‚ñÅ‡∏î‡πâ‡∏ß‡∏¢ | ‡∏Ç‡∏ß | ‡∏µ‡∏¢‡∏ô | ‡∏®‡∏∂‡∏Å | ‡πà‡∏≠‡∏á | ‡∏ï‡πâ‡∏≠‡∏á | ‡∏•‡∏±‡∏¢ | ‡∏ö‡∏≤ | ‡∏û‡∏¥ | ‡∏≠‡∏∏ | ‡∏™‡∏∏‡∏ß‡∏£‡∏£‡∏ì | ‡πÇ‡∏¢ | ‡πÄ‡∏£‡∏≤ | ‡∏Å‡∏•‡∏≤‡∏á | ‡πÄ‡∏ù‡πâ‡∏≤ | ‡∏Å‡∏©‡∏±‡∏ï‡∏£‡∏¥‡∏¢‡πå | ‡∏™‡∏∞ | ‡πÅ‡∏ó | ‡∏™‡∏±‡∏¢ | ‡πÅ‡∏à‡πâ‡∏á | ‡∏´‡∏ç | ‚ñÅ‡∏≠‡∏¢‡πà‡∏≤ | ‡∏£‡πç‡∏≤ | ‡∏ï‡∏£‡∏±‡∏™ | ‡∏≠‡∏†‡∏±‡∏¢ | ‡∏ú‡∏• | ‡πÄ‡∏•‡∏¢ | ‡∏µ‡∏¢‡∏ß | ‡πÑ‡∏´‡∏ô | ‡πâ‡∏≤‡∏ß | ‡πÅ‡∏ô | ‡∏¥‡∏î‡∏≤ | ‡∏£‡∏¥ | ‡∏™‡∏≤‡∏ß | ‡∏¥‡πâ‡∏° | ‡πÄ‡∏°‡∏∑‡∏≠‡∏á | ‡πÄ‡∏•‡πà‡∏≤ | ‡∏Ç‡∏±‡∏î | ‡∏Ñ‡πà‡∏≠‡∏¢ | ‡∏†‡∏≤ | ‡πÇ‡∏≠ | ‡πà‡πç‡∏≤ | ‡∏°‡∏±‡∏ô | ‡∏ä‡∏° | ‡∏´‡πå | ‡∏ä‡∏≤‡∏¢ | ‡∏±‡∏• | ‡∏ô‡∏≤‡∏¢ | ‚ñÅ‡πÄ‡∏à | ‡πÄ‡∏™‡∏µ‡∏¢‡∏á | ‡∏¢‡∏¥‡πà‡∏á | ‡∏£‡∏π | ‡πã‡∏¢ | ‡πÄ‡∏õ‡∏• | ‡πÄ‡∏≠‡∏≤ | ‚ñÅ‡πÄ‡∏™ | ‡∏Ñ‡∏á | ‡∏ï‡∏£‡∏≤ | ‡∏´‡πâ‡∏≤ | ‡∏¥‡∏ô‡∏™‡∏°‡∏∏‡∏ó‡∏£ | ‡∏Ñ‡∏≠‡∏¢ | ‡∏´‡∏ç‡∏¥‡∏á | ‡∏´‡∏ô‡∏µ | ‡πâ‡∏≤‡∏ô | ‡∏ç‡∏≤ | ‡∏Ñ‡∏∏ | ‡∏ö‡∏£‡∏£ | ‚ñÅ‡∏õ‡∏£‡∏∞ | ‡∏Å‡∏≤‡∏¢ | ‡∏ó‡∏´‡∏≤‡∏£ | ‚ñÅ‡∏≠‡∏±‡∏ô | ‡∏™‡∏¥‡πâ‡∏ô | ‡∏ó‡∏ò | ‡∏ó‡∏≠‡∏á | ‡∏±‡∏Å‡∏© | ‡∏•‡∏±‡∏á‡∏Å‡∏≤ | ‡∏ô‡∏¥ | ‡∏û‡∏π | ‡∏®‡πå | ‡πà‡∏ß | ‡∏à‡∏≤ | ‡πÉ‡∏´‡∏ç | ‡∏ó‡∏µ‡πà‡∏¢‡∏ß | ‡∏°‡∏ô | ‡πÑ‡∏• | ‡∏à‡∏£‡∏¥‡∏á | ‚ñÅ‡πÄ‡∏à‡πâ‡∏≤ | ‡∏à‡πç‡∏≤ | ‚ñÅ‡∏ö‡πâ‡∏≤‡∏á | ‡∏ö‡∏≠‡∏Å | ‚ñÅ‡∏ï‡πà‡∏≤‡∏á | ‡∏ï‡∏¥ | ‚ñÅ‡πÄ‡∏Ç‡πâ‡∏≤ | ‡πÑ‡∏° | ‡∏®‡∏£ | ‡∏≠‡∏± | ‡πÄ‡∏Ñ‡∏¢ | ‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á | ‡∏Å‡∏£‡∏≤ | ‡πÅ‡∏™‡∏ô | ‚ñÅ‡∏à‡∏ô | ‡∏à‡∏±‡∏ö | ‡∏û‡∏ö | ‡∏Ñ‡∏£‡∏±‡πâ‡∏ô | ‡∏à‡∏á | ‡∏û‡∏ß‡∏Å | ‡∏™‡∏µ | ‡πÑ‡∏Ç | ‡∏©‡∏ê | ‡πÄ‡∏Å‡∏• | ‡∏Ñ‡∏≤ | ‡∏£‡∏° | ‡∏û‡∏±‡∏Å | ‡∏û‡∏±‡∏ô | ‡∏ã‡∏∂‡πà‡∏á | ‡∏´‡∏ô‡∏±‡∏Å | ‡∏ô‡∏µ | ‡πà‡∏≤‡∏ß | ‡∏Å‡∏£‡∏∏‡∏á | ‡∏Å‡∏•‡πâ‡∏á | ‚ñÅ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô | ‡∏Ñ‡∏£‡∏≤ | ‡πÄ‡∏Ñ‡∏£ | ‡∏ó‡πâ‡∏≤‡∏ß | ‡πÉ‡∏™ | ‚ñÅ‡∏û‡∏ß‡∏Å | ‡∏ï‡∏±‡πâ‡∏á | ‡∏´‡∏•‡∏á | ‡∏•‡πâ‡∏ß‡∏ô | ‚ñÅ‡πÑ‡∏õ | ‡∏ú‡∏µ | ‡∏•‡πç‡∏≤ | ‡∏ô‡∏±‡∏Å | ‡∏£‡πâ‡∏≠‡∏á | ‚ñÅ‡∏à‡∏á | ‡∏ó‡∏£‡∏≤ | ‡∏´‡∏ô‡∏≤ | ‚ñÅ‡∏Å‡πá | ‡∏Å‡∏•‡∏±‡∏ß | ‚ñÅ‡∏ó‡∏µ‡πà | ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á | ‡∏≠‡∏≤‡∏¢ | ‡πÄ‡∏£‡∏∑‡∏≠ | ‚ñÅ‡πÅ‡∏°‡πâ‡∏ô | ‡πÄ‡∏ï | ‡πÅ‡∏Ñ | ‡∏¢‡∏Å | ‡∏û‡∏£‡∏≤‡∏∞ | ‡πÉ‡∏´‡∏ç‡πà | ‚ñÅ‡∏Ñ‡∏£‡∏±‡πâ‡∏ô | ‚ñÅ‡∏ô | ‡πÅ‡∏Å‡πâ‡∏ß | ‡∏ñ‡∏∑‡∏≠ | ‚ñÅ‡πÑ‡∏î‡πâ | ‡πÄ‡∏´‡∏•‡∏∑‡∏≠'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_vocabs = [bpe.id_to_piece(id) for id in range(bpe.get_piece_size())]\n",
    "\" | \".join(bpe_vocabs[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eu6QnnRfQyFj"
   },
   "source": [
    "### User-defined symbols\n",
    "\n",
    "Another important concept to know of is User-defined symbols. These special symbols are reserved for a special purpose \\(e.g.\\, the \\<MASK\\> token used in BERT) and will always be tokenized into one token.\n",
    "\n",
    "Refer to the documentation for ways to add these special tokens to your tokenizer.\n",
    "\n",
    "https://github.com/google/sentencepiece/blob/master/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEFOj62ZEdzT"
   },
   "source": [
    "## Train another tokenizer on another domain\n",
    "\n",
    "Now try training another unigram tokenizer on `pantip_text` and we will use it to compare with the unigram tokenizer we trained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "O7-QkA1eMZFf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: pantip-train.txt\n",
      "  input_format: \n",
      "  model_prefix: unigram-pantip\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: pantip-train.txt\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (7271 > 4192).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 9475 sentences\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 20 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=795024\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9506% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=185\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999506\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 8875 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=344183\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 202602 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 8875\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 28564\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 28564 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=41593 obj=36.7304 num_tokens=117107 num_tokens/piece=2.81555\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=35185 obj=31.7493 num_tokens=118246 num_tokens/piece=3.36069\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=26169 obj=31.928 num_tokens=126191 num_tokens/piece=4.82216\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=25868 obj=31.5765 num_tokens=126408 num_tokens/piece=4.88666\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19278 obj=32.403 num_tokens=135618 num_tokens/piece=7.03486\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=19222 obj=32.081 num_tokens=135912 num_tokens/piece=7.07065\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14375 obj=33.0697 num_tokens=145771 num_tokens/piece=10.1406\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14365 obj=32.7542 num_tokens=145994 num_tokens/piece=10.1632\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10770 obj=33.8919 num_tokens=156025 num_tokens/piece=14.487\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10766 obj=33.5958 num_tokens=156169 num_tokens/piece=14.5058\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8071 obj=34.8705 num_tokens=166979 num_tokens/piece=20.6888\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8071 obj=34.5957 num_tokens=167077 num_tokens/piece=20.7009\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6053 obj=35.9697 num_tokens=178378 num_tokens/piece=29.4694\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6053 obj=35.6866 num_tokens=178906 num_tokens/piece=29.5566\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4538 obj=37.2058 num_tokens=190787 num_tokens/piece=42.0421\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4538 obj=36.9041 num_tokens=191139 num_tokens/piece=42.1197\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3403 obj=38.5838 num_tokens=204169 num_tokens/piece=59.9968\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3403 obj=38.2589 num_tokens=204177 num_tokens/piece=59.9991\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2552 obj=40.1566 num_tokens=218658 num_tokens/piece=85.681\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2552 obj=39.7848 num_tokens=218876 num_tokens/piece=85.7665\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1914 obj=41.8605 num_tokens=234410 num_tokens/piece=122.471\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1914 obj=41.4363 num_tokens=234410 num_tokens/piece=122.471\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1435 obj=43.7577 num_tokens=252316 num_tokens/piece=175.83\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1435 obj=43.2646 num_tokens=252333 num_tokens/piece=175.842\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1100 obj=45.6164 num_tokens=272438 num_tokens/piece=247.671\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1100 obj=45.0553 num_tokens=272438 num_tokens/piece=247.671\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: unigram-pantip.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: unigram-pantip.vocab\n"
     ]
    }
   ],
   "source": [
    "#TODO: Train\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=\"pantip-train.txt\",\n",
    "    model_prefix=\"unigram-pantip\",\n",
    "    vocab_size=1000,\n",
    "    model_type=\"unigram\",\n",
    ")\n",
    "\n",
    "# pam trained: sp_pam_unigram\n",
    "sp_pantip_unigram = spm.SentencePieceProcessor(model_file=\"unigram-pantip.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5WOVMbONnYv"
   },
   "source": [
    "## Analyse top tokens on different datasets\n",
    "\n",
    "Use your tokenizers to tokenize the datasets and analyse your most common vocabularies (try 300-400 vocabs with len>1). Hint: tokenize your data and count the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz0GdZ-5YYM9"
   },
   "source": [
    "### To answer\n",
    "What are some notable differences you see between the two vocabs?\n",
    "\n",
    "Write your answer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxxYr0QLbDoU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipjO87HPYl4N"
   },
   "source": [
    "## Using tokenizer across domains\n",
    "\n",
    "One problem you may face is your dataset is very specialized. In that case the tokenizer trained on a general domain may not perform as good as it should when used on your dataset.\n",
    "\n",
    "Next you will try using tokenizers trained on one general domain (on Pantip) and use it on a specialized domain (‡∏û‡∏£‡∏∞‡∏≠‡∏†‡∏±‡∏¢‡∏°‡∏ì‡∏µ) and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4_6JG_l5BXh"
   },
   "source": [
    "### Q3 MCV\n",
    "\n",
    "What percentage increase do you observe when tokenizing the whole ‡∏û‡∏£‡∏∞‡∏≠‡∏†‡∏±‡∏¢‡∏°‡∏ì‡∏µ dataset with a tokenizer trained on Pantip compared to the one trained on ‡∏û‡∏£‡∏∞‡∏≠‡∏†‡∏±‡∏¢‡∏°‡∏ì‡∏µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: \n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 16314 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=885139\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.954% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=63\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99954\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 16314 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=411778\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 275348 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 16314\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 32181\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 32181 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=49394 obj=42.2355 num_tokens=128673 num_tokens/piece=2.60503\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=40979 obj=36.7371 num_tokens=130046 num_tokens/piece=3.17348\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=30367 obj=36.9771 num_tokens=139359 num_tokens/piece=4.58916\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=29880 obj=36.5016 num_tokens=139582 num_tokens/piece=4.67142\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=22205 obj=37.6771 num_tokens=149629 num_tokens/piece=6.73853\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=22104 obj=37.2684 num_tokens=149717 num_tokens/piece=6.7733\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=16512 obj=38.7389 num_tokens=161069 num_tokens/piece=9.75466\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=16473 obj=38.3341 num_tokens=161183 num_tokens/piece=9.78468\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12338 obj=40.0409 num_tokens=172981 num_tokens/piece=14.0202\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12331 obj=39.6638 num_tokens=172997 num_tokens/piece=14.0294\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9245 obj=41.4482 num_tokens=185092 num_tokens/piece=20.0208\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9242 obj=41.1146 num_tokens=185096 num_tokens/piece=20.0277\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6930 obj=43.0791 num_tokens=198272 num_tokens/piece=28.6107\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6930 obj=42.7435 num_tokens=198274 num_tokens/piece=28.611\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5196 obj=44.8072 num_tokens=211985 num_tokens/piece=40.7977\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5196 obj=44.4723 num_tokens=211972 num_tokens/piece=40.7952\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3897 obj=46.7216 num_tokens=226813 num_tokens/piece=58.202\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3897 obj=46.3429 num_tokens=226820 num_tokens/piece=58.2037\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2922 obj=48.8639 num_tokens=244481 num_tokens/piece=83.6691\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2922 obj=48.4277 num_tokens=244676 num_tokens/piece=83.7358\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2191 obj=51.2638 num_tokens=265878 num_tokens/piece=121.35\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2191 obj=50.7382 num_tokens=265883 num_tokens/piece=121.352\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1643 obj=53.8622 num_tokens=291545 num_tokens/piece=177.447\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1643 obj=53.2395 num_tokens=291545 num_tokens/piece=177.447\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1232 obj=56.564 num_tokens=320433 num_tokens/piece=260.092\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1232 obj=55.8787 num_tokens=320433 num_tokens/piece=260.092\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1100 obj=57.1231 num_tokens=332377 num_tokens/piece=302.161\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1100 obj=56.8968 num_tokens=332377 num_tokens/piece=302.161\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: \n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (6941 > 4192).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1412 sentences\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 118 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=459294\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9508% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=189\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999508\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1412 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=200111\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 132410 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1412\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 18499\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 18499 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=28381 obj=39.292 num_tokens=79191 num_tokens/piece=2.79028\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=24049 obj=34.1727 num_tokens=79898 num_tokens/piece=3.3223\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=17861 obj=34.4328 num_tokens=85402 num_tokens/piece=4.78148\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=17651 obj=34.004 num_tokens=85560 num_tokens/piece=4.84732\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=13169 obj=35.0585 num_tokens=92040 num_tokens/piece=6.98914\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=13139 obj=34.6501 num_tokens=92293 num_tokens/piece=7.02435\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9835 obj=35.8793 num_tokens=99445 num_tokens/piece=10.1113\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9829 obj=35.4774 num_tokens=99459 num_tokens/piece=10.1189\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=7370 obj=36.9077 num_tokens=106824 num_tokens/piece=14.4944\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=7367 obj=36.535 num_tokens=106946 num_tokens/piece=14.5169\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5524 obj=38.0433 num_tokens=114354 num_tokens/piece=20.7013\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5522 obj=37.7067 num_tokens=114523 num_tokens/piece=20.7394\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4141 obj=39.4008 num_tokens=122818 num_tokens/piece=29.659\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4141 obj=39.0469 num_tokens=122867 num_tokens/piece=29.6709\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3105 obj=40.8784 num_tokens=131593 num_tokens/piece=42.381\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3105 obj=40.5162 num_tokens=131639 num_tokens/piece=42.3958\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2328 obj=42.4068 num_tokens=140508 num_tokens/piece=60.3557\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2328 obj=42.0628 num_tokens=140802 num_tokens/piece=60.482\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1746 obj=44.1723 num_tokens=150410 num_tokens/piece=86.1455\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1746 obj=43.784 num_tokens=150413 num_tokens/piece=86.1472\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1309 obj=46.1341 num_tokens=161865 num_tokens/piece=123.655\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1309 obj=45.6436 num_tokens=161865 num_tokens/piece=123.655\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1100 obj=47.0956 num_tokens=169312 num_tokens/piece=153.92\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1100 obj=46.7863 num_tokens=169312 num_tokens/piece=153.92\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# pam model on pam data\n",
    "model_writer_ = io.BytesIO()\n",
    "spm.SentencePieceTrainer.train(sentence_iterator=iter(pam_train_text), model_writer=model_writer_, vocab_size = 1000)\n",
    "sp_pam = spm.SentencePieceProcessor(model_proto=model_writer_.getvalue())\n",
    "\n",
    "# pantip model on pantip data\n",
    "model_writer_ = io.BytesIO()\n",
    "spm.SentencePieceTrainer.train(sentence_iterator=iter(pantip_train_text), model_writer=model_writer_, vocab_size = 1000)\n",
    "sp_pantip = spm.SentencePieceProcessor(model_proto=model_writer_.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.42710745209495\n"
     ]
    }
   ],
   "source": [
    "# dataset: pra_apai_manee_data\n",
    "pra_apai_manee_data_str = ''.join(pra_apai_manee_data)\n",
    "print(\n",
    "    (\n",
    "        len(sp_pantip.encode(pra_apai_manee_data_str))\n",
    "        / len(sp_pam.encode(pra_apai_manee_data_str))\n",
    "        - 1\n",
    "    )\n",
    "    * 100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duaCJRO96SX1"
   },
   "source": [
    "### Q4 MCV\n",
    "\n",
    "What percentage increase do you observe when tokenizing the whole Pantip dataset with a tokenizer trained on ‡∏û‡∏£‡∏∞‡∏≠‡∏†‡∏±‡∏¢‡∏°‡∏ì‡∏µ compared to the one trained on Pantip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "axk9gOIgrTYd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.855248847916112\n",
      "round= 13 %\n"
     ]
    }
   ],
   "source": [
    "# dataset: pantip_text\n",
    "pantip_text_str = ''.join(pantip_text)\n",
    "result = (sum(map(len, sp_pam.encode(pantip_text))) / sum(map(len, (sp_pantip.encode(pantip_text))))- 1) * 100\n",
    "print(result)\n",
    "print(f\"round= {round(result)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZYKuamv7-wI"
   },
   "source": [
    "### To answer\n",
    "Why do you think the number of tokens tokenized by the general tokenizer (the one trained on Pantip) has a higher percentage increase compared to the number of tokens tokenized by the specialized tokenizer? (Hint: we fixed vocab size.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gh9a6d7Q8ivJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7j_Cc0p9-5S"
   },
   "source": [
    "## The effect on language models\n",
    "\n",
    "Next, we will see the effect of using \"cross-domain\" tokenizers on Language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiWztANvohhn"
   },
   "source": [
    "### Setup\n",
    "We are going to reuse the code from the last assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pVtSbmVpwOo"
   },
   "outputs": [],
   "source": [
    "# !pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMt5GzLrW4x3"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import lightning as L\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OIs_VS_oo1M"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "  def __init__(self, data, tokenizer, seq_len = 128):\n",
    "\n",
    "    token_ids = [tokenizer.encode(d, add_bos=True, add_eos=True) for d in data]\n",
    "    flatten_token_ids = list(itertools.chain(*token_ids))\n",
    "    encoded = torch.LongTensor(flatten_token_ids)\n",
    "\n",
    "    left_over = len(encoded) % seq_len\n",
    "    encoded = encoded[:len(encoded)-left_over]\n",
    "    self.encoded = encoded.view(-1, seq_len)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.encoded[idx]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hk6vEPiMq34n"
   },
   "outputs": [],
   "source": [
    "class LSTM(L.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, learning_rate, criterion):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size=vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def forward(self, src):\n",
    "        pass\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        src = batch[:, :-1]\n",
    "        target = batch[:, 1:]\n",
    "        prediction = self(src)\n",
    "        prediction = prediction.reshape(-1, self.vocab_size)\n",
    "        target = target.reshape(-1)\n",
    "        loss = self.criterion(prediction, target)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "\n",
    "        src = batch[:, :-1]\n",
    "        target = batch[:, 1:]\n",
    "        with torch.no_grad():\n",
    "          prediction = self(src)\n",
    "        prediction = prediction.reshape(-1, self.vocab_size)\n",
    "        target = target.reshape(-1)\n",
    "        loss = self.criterion(prediction, target)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKhuOygixndB"
   },
   "outputs": [],
   "source": [
    "vocab_size = sp_pam.get_piece_size()\n",
    "embedding_dim = 200\n",
    "hidden_dim = 512\n",
    "num_layers = 3\n",
    "dropout_rate = 0.2\n",
    "lr = 1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_batch_size = 64\n",
    "test_batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOtOE7mr-heY"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8-x9HiPDcpE"
   },
   "source": [
    "<a name=\"no1\"></a>\n",
    "#### 1. Training on Pantip data with Pantip tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUv_A4MTx0Ob"
   },
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    deterministic=True\n",
    ")\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n",
    "\n",
    "pantip_train_dataset = TextDataset(pantip_train_text, sp_pantip)\n",
    "pantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pantip_test_dataset = TextDataset(pantip_test_text, sp_pantip)\n",
    "pantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "pam_train_dataset = TextDataset(pam_train_text, sp_pantip)\n",
    "pam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pam_test_dataset = TextDataset(pam_test_text, sp_pantip)\n",
    "pam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=pantip_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1e-Y1_GYy65g"
   },
   "outputs": [],
   "source": [
    "test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n",
    "\n",
    "print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n",
    "print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n",
    "print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n",
    "print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7s3AmE4nDjmL"
   },
   "source": [
    "<a name=\"no2\"></a>\n",
    "#### 2. Training on Pantip data with Pra apai manee tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfRdW3m1Dmj_"
   },
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    deterministic=True\n",
    ")\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n",
    "\n",
    "pantip_train_dataset = TextDataset(pantip_train_text, sp_pam)\n",
    "pantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pantip_test_dataset = TextDataset(pantip_test_text, sp_pam)\n",
    "pantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "pam_train_dataset = TextDataset(pam_train_text, sp_pam)\n",
    "pam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pam_test_dataset = TextDataset(pam_test_text, sp_pam)\n",
    "pam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=pantip_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwLN1IarD3g9"
   },
   "outputs": [],
   "source": [
    "test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n",
    "\n",
    "print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n",
    "print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n",
    "print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n",
    "print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB8zqptTWcA6"
   },
   "source": [
    "#### To answer\n",
    "\n",
    "The perplexity numbers should indicate that:\n",
    "1. Training the LM with Pra apai manee tokenizer on Pantip (no. [2](#no2)) results in overfitting to Pantip and poor generalization to the Pra apai manee dataset.\n",
    "2. However using the Pantip tokenizer (no. [1](#no1)) results in a much better generalization.\n",
    "\n",
    "Try and come up with some reasons for the results above. <br>\n",
    "Hint:\n",
    "1. think about \"general\" vocabs and domain-specific vocabs.\n",
    "2. what do you think happens to the model when the token ids become longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmHGQf2saPj_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8VPMm7pLdSl"
   },
   "source": [
    "\n",
    "<a name=\"no3\"></a>\n",
    "#### 3. Training on Pra apai manee data with Pantip tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oR5fp-YCLnnU"
   },
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    deterministic=True\n",
    ")\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n",
    "\n",
    "pantip_train_dataset = TextDataset(pantip_train_text, sp_pantip)\n",
    "pantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pantip_test_dataset = TextDataset(pantip_test_text, sp_pantip)\n",
    "pantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "pam_train_dataset = TextDataset(pam_train_text, sp_pantip)\n",
    "pam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pam_test_dataset = TextDataset(pam_test_text, sp_pantip)\n",
    "pam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=pam_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_LhF7w7Lxwo"
   },
   "outputs": [],
   "source": [
    "test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n",
    "\n",
    "print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n",
    "print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n",
    "print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n",
    "print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apk9crJjMLoW"
   },
   "source": [
    "<a name=\"no4\"></a>\n",
    "#### 4. Training on Pra apai manee data with Pra apai manee tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_G7GMBIKLzGK"
   },
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    deterministic=True\n",
    ")\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)\n",
    "\n",
    "pantip_train_dataset = TextDataset(pantip_train_text, sp_pam)\n",
    "pantip_train_loader = DataLoader(pantip_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pantip_test_dataset = TextDataset(pantip_test_text, sp_pam)\n",
    "pantip_test_loader = DataLoader(pantip_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "pam_train_dataset = TextDataset(pam_train_text, sp_pam)\n",
    "pam_train_loader = DataLoader(pam_train_dataset, batch_size = train_batch_size, shuffle = True)\n",
    "\n",
    "pam_test_dataset = TextDataset(pam_test_text, sp_pam)\n",
    "pam_test_loader = DataLoader(pam_test_dataset, batch_size = test_batch_size, shuffle = False)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=pam_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9H753o_JMRFw"
   },
   "outputs": [],
   "source": [
    "test_result = trainer.test(model, dataloaders=[pantip_train_loader, pam_train_loader, pantip_test_loader,pam_test_loader], verbose=False)\n",
    "\n",
    "print(f\"Perplexity on Pantip train set is:\\t{np.exp(test_result[0]['test_loss/dataloader_idx_0'])}\")\n",
    "print(f\"Perplexity on Pra apai manee train set is:\\t{np.exp(test_result[1]['test_loss/dataloader_idx_1'])}\")\n",
    "print(f\"Perplexity on Pantip test set is:\\t{np.exp(test_result[2]['test_loss/dataloader_idx_2'])}\")\n",
    "print(f\"Perplexity on Pra apai manee test set is:\\t{np.exp(test_result[3]['test_loss/dataloader_idx_3'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en9Lmmj4dZ-1"
   },
   "source": [
    "#### To answer\n",
    "\n",
    "The perplexity numbers should indicate that:\n",
    "1. Both LM overfits on Pra apai manee data and performs really bad on Pantip data.\n",
    "2. However using the Pra apai manee tokenizer (no. [4](#no4)) results in a  better generalization than the Pantip tokenizer(no. [3](#no3)).\n",
    "\n",
    "Try and come up with some reasons for the results above. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HlE-mWSMfbv3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
