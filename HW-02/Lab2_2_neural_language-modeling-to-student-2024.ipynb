{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "15QfB7RAuXAc",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Neural Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gucid6KNuXAe"
   },
   "source": [
    "In this Exercise, we will be using Pytorch Lightning to implement our neural LM. Your job will be just to write the forward method of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yL_M2zf4myYa"
   },
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:06:36.649031Z",
     "iopub.status.busy": "2025-01-18T07:06:36.648621Z",
     "iopub.status.idle": "2025-01-18T07:06:38.043082Z",
     "shell.execute_reply": "2025-01-18T07:06:38.042290Z",
     "shell.execute_reply.started": "2025-01-18T07:06:36.648997Z"
    },
    "id": "MRRrn78ZjL54",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-18 07:06:36--  https://github.com/ekapolc/nlp_2019/raw/master/HW4/BEST2010.zip\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW4/BEST2010.zip [following]\n",
      "--2025-01-18 07:06:37--  https://raw.githubusercontent.com/ekapolc/nlp_2019/master/HW4/BEST2010.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7423530 (7.1M) [application/zip]\n",
      "Saving to: ‘BEST2010.zip’\n",
      "\n",
      "BEST2010.zip        100%[===================>]   7.08M  --.-KB/s    in 0.08s   \n",
      "\n",
      "2025-01-18 07:06:37 (87.9 MB/s) - ‘BEST2010.zip’ saved [7423530/7423530]\n",
      "\n",
      "Archive:  BEST2010.zip\n",
      "   creating: BEST2010/\n",
      "  inflating: BEST2010/article.txt    \n",
      "  inflating: BEST2010/encyclopedia.txt  \n",
      "  inflating: BEST2010/news.txt       \n"
     ]
    }
   ],
   "source": [
    "# #download corpus\n",
    "!wget --no-check-certificate https://github.com/ekapolc/nlp_2019/raw/master/HW4/BEST2010.zip\n",
    "!unzip BEST2010.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:06:38.044658Z",
     "iopub.status.busy": "2025-01-18T07:06:38.044388Z",
     "iopub.status.idle": "2025-01-18T07:06:43.208608Z",
     "shell.execute_reply": "2025-01-18T07:06:43.207440Z",
     "shell.execute_reply.started": "2025-01-18T07:06:38.044636Z"
    },
    "id": "SGmYebp38OUl",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightning\n",
      "  Downloading lightning-2.5.0.post0-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.2)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.9.0)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.11.9)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.2)\n",
      "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.6.1)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.5.0.post0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.10)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.16.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.18.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\n",
      "Downloading lightning-2.5.0.post0-py3-none-any.whl (815 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightning\n",
      "Successfully installed lightning-2.5.0.post0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IR4HK5jQm17K"
   },
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:06:43.211036Z",
     "iopub.status.busy": "2025-01-18T07:06:43.210711Z",
     "iopub.status.idle": "2025-01-18T07:06:43.634235Z",
     "shell.execute_reply": "2025-01-18T07:06:43.633391Z",
     "shell.execute_reply.started": "2025-01-18T07:06:43.211010Z"
    },
    "id": "oPE1RqKOrWJ0",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in BEST2010 news training dataset :\t21678\n",
      "Total word counts in BEST2010 news training dataset :\t1042797\n"
     ]
    }
   ],
   "source": [
    "total_word_count = 0\n",
    "best2010 = []\n",
    "with open('BEST2010/news.txt','r',encoding='utf-8') as f:\n",
    "  for i,line in enumerate(f):\n",
    "    line=line.strip()[:-1] #remove the trailing |\n",
    "    total_word_count += len(line.split(\"|\"))\n",
    "    best2010.append(line)\n",
    "\n",
    "train = best2010[:int(len(best2010)*0.7)]\n",
    "test = best2010[int(len(best2010)*0.7):]\n",
    "#Training data\n",
    "train_word_count =0\n",
    "for line in train:\n",
    "    for word in line.split('|'):\n",
    "        train_word_count+=1\n",
    "print ('Total sentences in BEST2010 news training dataset :\\t'+ str(len(train)))\n",
    "print ('Total word counts in BEST2010 news training dataset :\\t'+ str(train_word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQBjqe5arHGX"
   },
   "source": [
    "Here we are going to use a library from huggingface called `tokenizers`. This will help us create a vocabulary and handle the encoding and decoding, i.e., convert text to its corresponding ID (which will be learned by the tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:06:43.635772Z",
     "iopub.status.busy": "2025-01-18T07:06:43.635550Z",
     "iopub.status.idle": "2025-01-18T07:06:44.251214Z",
     "shell.execute_reply": "2025-01-18T07:06:44.250373Z",
     "shell.execute_reply.started": "2025-01-18T07:06:43.635753Z"
    },
    "id": "elwE0gh2rE3C",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import CharDelimiterSplit\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "#Basically, we just use the new tokenizer as our vocab building tool.\n",
    "#In practice, you will have to use a compatible tokenizer like newmm to tokenize the corpus first then do this step\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = CharDelimiterSplit(delimiter=\"|\") #now the tokenizer will split \"|\" for us\n",
    "trainer = WordLevelTrainer(min_frequency=3,  #we can set a frequency threshold for taking a word into our vocab. for this example, words with freq < 3 will be excluded from the vocab.\n",
    "                           special_tokens=[\"[UNK]\", \"<s>\", \"</s>\"]) #these are our special tokens: for unknown, begin-of-sentence, and end-of-sentence, respectively.\n",
    "tokenizer.train_from_iterator(train, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:06:44.252562Z",
     "iopub.status.busy": "2025-01-18T07:06:44.252278Z",
     "iopub.status.idle": "2025-01-18T07:06:44.271917Z",
     "shell.execute_reply": "2025-01-18T07:06:44.268590Z",
     "shell.execute_reply.started": "2025-01-18T07:06:44.252539Z"
    },
    "id": "TrKtjv4PJpg2",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9062"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_vocab()) #same as nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:06:44.272805Z",
     "iopub.status.busy": "2025-01-18T07:06:44.272553Z",
     "iopub.status.idle": "2025-01-18T07:06:44.291814Z",
     "shell.execute_reply": "2025-01-18T07:06:44.291037Z",
     "shell.execute_reply.started": "2025-01-18T07:06:44.272779Z"
    },
    "id": "WqM_jrZwrJpB",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กฎหมาย', 'กับ', 'การ', 'เบียดบัง', 'คน', 'จน', '[UNK]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"กฎหมาย|กับ|การ|เบียดบัง|คน|จน|asdf\").tokens #tokens we get after tokenizing this sentence. unknown words will be tokenized as [UNK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:06:44.293250Z",
     "iopub.status.busy": "2025-01-18T07:06:44.292959Z",
     "iopub.status.idle": "2025-01-18T07:06:44.307104Z",
     "shell.execute_reply": "2025-01-18T07:06:44.306616Z",
     "shell.execute_reply.started": "2025-01-18T07:06:44.293221Z"
    },
    "id": "1r1pJ1B_sp9j",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[242, 28, 5, 8883, 22, 190, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"กฎหมาย|กับ|การ|เบียดบัง|คน|จน|asdf\").ids #this is what we will feed to the LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:06:44.310985Z",
     "iopub.status.busy": "2025-01-18T07:06:44.310757Z",
     "iopub.status.idle": "2025-01-18T07:06:52.976286Z",
     "shell.execute_reply": "2025-01-18T07:06:52.975626Z",
     "shell.execute_reply.started": "2025-01-18T07:06:44.310966Z"
    },
    "id": "Fkx6CSoXWXmG",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import lightning as L\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:06:52.977920Z",
     "iopub.status.busy": "2025-01-18T07:06:52.977583Z",
     "iopub.status.idle": "2025-01-18T07:06:52.989001Z",
     "shell.execute_reply": "2025-01-18T07:06:52.988133Z",
     "shell.execute_reply.started": "2025-01-18T07:06:52.977898Z"
    },
    "id": "3XHJsP8_898x",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:06:52.990038Z",
     "iopub.status.busy": "2025-01-18T07:06:52.989719Z",
     "iopub.status.idle": "2025-01-18T07:06:53.022936Z",
     "shell.execute_reply": "2025-01-18T07:06:53.022297Z",
     "shell.execute_reply.started": "2025-01-18T07:06:52.989995Z"
    },
    "id": "-r_kyrrrDHZq",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "  def __init__(self, data, seq_len = 128):\n",
    "    #  data is currently a list of sentences\n",
    "    #  [sent1,\n",
    "    #   sent2,\n",
    "    #   ...,\n",
    "    #  ]\n",
    "\n",
    "    data = [d+'|</s>' for d in data] #append an </s> token to each sentence\n",
    "    encodings = tokenizer.encode_batch(data) #encode (turn token into token_id) data\n",
    "    token_ids = [enc.ids for enc in encodings] #get the token ids for each sentence\n",
    "    flatten_token_ids = list(itertools.chain(*token_ids)) #turn a list of token_ids into one long token_ids\n",
    "    ## now data looks like this [sent1_ids </s> sent2_ids </s> ...]\n",
    "    encoded = torch.LongTensor(flatten_token_ids)\n",
    "\n",
    "    #remove some left over tokens so that we can form batches of seq_len (128 in this case). Optionally, we can use padding tokens instead.\n",
    "    left_over = len(encoded) % seq_len\n",
    "    encoded = encoded[:len(encoded)-left_over]\n",
    "    self.encoded = encoded.view(-1, seq_len) #reshape data so it becomes a 2-D matrix of shape (len(encoded)//128, 128), i.e. each row contains data of len==128\n",
    "    ## now data looks like this\n",
    "    ## [ [1,2,3, ... , 128] (this is just an example, not actual input_ids)\n",
    "    ##   [1,2,3, ... , 128]\n",
    "    ##   [1,2,3, ... , 128]\n",
    "    ## ]\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.encoded[idx]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:06:53.023915Z",
     "iopub.status.busy": "2025-01-18T07:06:53.023703Z",
     "iopub.status.idle": "2025-01-18T07:06:54.736314Z",
     "shell.execute_reply": "2025-01-18T07:06:54.735565Z",
     "shell.execute_reply.started": "2025-01-18T07:06:53.023896Z"
    },
    "id": "YmW-K0XBZ4Dq",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "test_batch_size = 128\n",
    "train_dataset = TextDataset(train)\n",
    "train_loader = DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True) #DataLoader will take care of the random sampling and batching of data\n",
    "\n",
    "test_dataset = TextDataset(test)\n",
    "test_loader = DataLoader(test_dataset, batch_size = test_batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElhZcB94MUtC"
   },
   "source": [
    "## Model : Implement the forward function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:06:54.737565Z",
     "iopub.status.busy": "2025-01-18T07:06:54.737223Z",
     "iopub.status.idle": "2025-01-18T07:06:54.745677Z",
     "shell.execute_reply": "2025-01-18T07:06:54.744801Z",
     "shell.execute_reply.started": "2025-01-18T07:06:54.737533Z"
    },
    "id": "nKNJAolug-1I",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LSTM(L.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, learning_rate, criterion):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) #this will turn the token ids into vectors\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size) #turn the vectors back into token ids\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def forward(self, src):\n",
    "        # TODO: Implement the forward pass\n",
    "        embedded = self.embedding(src)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        src = batch[:, :-1]\n",
    "        target = batch[:, 1:]\n",
    "        prediction = self(src) # run the sequence through the model (the forward method)\n",
    "        prediction = prediction.reshape(-1, vocab_size)\n",
    "        target = target.reshape(-1)\n",
    "        loss = self.criterion(prediction, target)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        src = batch[:, :-1]  #[batch_size (64) , seq_len-1 (127)] except last words\n",
    "        target = batch[:, 1:] #[batch_size (64) , seq_len-1 (127)] except first words\n",
    "        with torch.no_grad(): #disable gradient calculation for faster inference\n",
    "          prediction = self(src) #[batch_size (64), seq_len-1 (127) , vocab size (9000)]\n",
    "        prediction = prediction.reshape(-1, vocab_size) #[batch_size*(seq_len-1) (64*127=8128) , vocab]\n",
    "        target = target.reshape(-1) #[batch_size (64), seq_len-1 (127)] -> [batch_size*(seq_len-1) (8128)]\n",
    "        loss = self.criterion(prediction, target)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:06:54.746661Z",
     "iopub.status.busy": "2025-01-18T07:06:54.746420Z",
     "iopub.status.idle": "2025-01-18T07:06:54.764292Z",
     "shell.execute_reply": "2025-01-18T07:06:54.763403Z",
     "shell.execute_reply.started": "2025-01-18T07:06:54.746631Z"
    },
    "id": "jBnYCh-miOEr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "embedding_dim = 200\n",
    "hidden_dim = 512\n",
    "num_layers = 3\n",
    "dropout_rate = 0.2\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:06:54.765445Z",
     "iopub.status.busy": "2025-01-18T07:06:54.765124Z",
     "iopub.status.idle": "2025-01-18T07:06:54.892732Z",
     "shell.execute_reply": "2025-01-18T07:06:54.891993Z",
     "shell.execute_reply.started": "2025-01-18T07:06:54.765421Z"
    },
    "id": "HHWXaPsvigPq",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:06:54.893692Z",
     "iopub.status.busy": "2025-01-18T07:06:54.893477Z",
     "iopub.status.idle": "2025-01-18T07:06:54.897743Z",
     "shell.execute_reply": "2025-01-18T07:06:54.896886Z",
     "shell.execute_reply.started": "2025-01-18T07:06:54.893672Z"
    },
    "id": "_yNEZ4jwXumR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import CSVLogger\n",
    "csv_logger = CSVLogger(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZwqhWicMdH0"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:06:54.898963Z",
     "iopub.status.busy": "2025-01-18T07:06:54.898714Z",
     "iopub.status.idle": "2025-01-18T07:06:54.989637Z",
     "shell.execute_reply": "2025-01-18T07:06:54.989057Z",
     "shell.execute_reply.started": "2025-01-18T07:06:54.898943Z"
    },
    "id": "kr0zdeMAjD1U",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=20,\n",
    "    logger=csv_logger,\n",
    "    deterministic=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:06:54.990654Z",
     "iopub.status.busy": "2025-01-18T07:06:54.990374Z",
     "iopub.status.idle": "2025-01-18T07:16:47.525067Z",
     "shell.execute_reply": "2025-01-18T07:16:47.524422Z",
     "shell.execute_reply.started": "2025-01-18T07:06:54.990623Z"
    },
    "id": "A9qcwNA0mN6J",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "INFO: \n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | embedding | Embedding        | 1.8 M  | train\n",
      "1 | lstm      | LSTM             | 5.7 M  | train\n",
      "2 | dropout   | Dropout          | 0      | train\n",
      "3 | fc        | Linear           | 4.6 M  | train\n",
      "4 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "12.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.1 M    Total params\n",
      "48.504    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c1fd2a80354d0cbe2bc4edd9b2ca55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=train_loader) # takes about 8 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:16:47.526155Z",
     "iopub.status.busy": "2025-01-18T07:16:47.525880Z",
     "iopub.status.idle": "2025-01-18T07:16:47.581891Z",
     "shell.execute_reply": "2025-01-18T07:16:47.580928Z",
     "shell.execute_reply.started": "2025-01-18T07:16:47.526128Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# save to local\n",
    "torch.save(model, 'lstm-t4-model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUfWF_V6Me9H"
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:16:47.583149Z",
     "iopub.status.busy": "2025-01-18T07:16:47.582914Z",
     "iopub.status.idle": "2025-01-18T07:16:54.020285Z",
     "shell.execute_reply": "2025-01-18T07:16:54.019689Z",
     "shell.execute_reply.started": "2025-01-18T07:16:47.583128Z"
    },
    "id": "WXVj9ewNqweZ",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2369e72d17f944a2a0fb48a47e50236c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    4.1082844734191895     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   4.1082844734191895    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_result = trainer.test(model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:16:54.021123Z",
     "iopub.status.busy": "2025-01-18T07:16:54.020927Z",
     "iopub.status.idle": "2025-01-18T07:16:54.024854Z",
     "shell.execute_reply": "2025-01-18T07:16:54.024074Z",
     "shell.execute_reply.started": "2025-01-18T07:16:54.021105Z"
    },
    "id": "4pVjEyYDtnc-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:16:54.025790Z",
     "iopub.status.busy": "2025-01-18T07:16:54.025510Z",
     "iopub.status.idle": "2025-01-18T07:16:54.037302Z",
     "shell.execute_reply": "2025-01-18T07:16:54.036687Z",
     "shell.execute_reply.started": "2025-01-18T07:16:54.025769Z"
    },
    "id": "uuIPToGQs-ZG",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity : 60.84225148840851\n"
     ]
    }
   ],
   "source": [
    "print(f\"Perplexity : {np.exp(test_result[0]['test_loss'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:16:54.038352Z",
     "iopub.status.busy": "2025-01-18T07:16:54.038084Z",
     "iopub.status.idle": "2025-01-18T07:16:54.050484Z",
     "shell.execute_reply": "2025-01-18T07:16:54.049824Z",
     "shell.execute_reply.started": "2025-01-18T07:16:54.038324Z"
    },
    "id": "pAZwiRqsnOPe",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(9062, 200)\n",
       "  (lstm): LSTM(200, 512, num_layers=3, batch_first=True, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=9062, bias=True)\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() #disable dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:16:54.053705Z",
     "iopub.status.busy": "2025-01-18T07:16:54.053513Z",
     "iopub.status.idle": "2025-01-18T07:16:54.062071Z",
     "shell.execute_reply": "2025-01-18T07:16:54.061308Z",
     "shell.execute_reply.started": "2025-01-18T07:16:54.053688Z"
    },
    "id": "VFtebDAmVh_T",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "unk_token_id = tokenizer.encode(\"[UNK]\").ids\n",
    "eos_token_id = tokenizer.encode(\"</s>\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:16:54.063263Z",
     "iopub.status.busy": "2025-01-18T07:16:54.063013Z",
     "iopub.status.idle": "2025-01-18T07:16:54.075774Z",
     "shell.execute_reply": "2025-01-18T07:16:54.075045Z",
     "shell.execute_reply.started": "2025-01-18T07:16:54.063243Z"
    },
    "id": "hj-V4OsDqpBO",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_seq(context, max_new_token = 10):\n",
    "  encoded = tokenizer.encode(context).ids\n",
    "  with torch.no_grad():\n",
    "      for i in range(max_new_token):\n",
    "          src = torch.LongTensor([encoded]).to(model.device)\n",
    "          prediction = model(src)\n",
    "          probs = torch.softmax(prediction[:, -1] / 1, dim=-1)\n",
    "          prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "          while prediction == unk_token_id:\n",
    "              prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "          if prediction == eos_token_id:\n",
    "              break\n",
    "\n",
    "          encoded.append(prediction)\n",
    "\n",
    "  return tokenizer.decode(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-18T07:16:54.076827Z",
     "iopub.status.busy": "2025-01-18T07:16:54.076550Z",
     "iopub.status.idle": "2025-01-18T07:16:55.155234Z",
     "shell.execute_reply": "2025-01-18T07:16:55.154286Z",
     "shell.execute_reply.started": "2025-01-18T07:16:54.076799Z"
    },
    "id": "u20r9w8zvJi4",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'วัน จันทร์ ที่ 15 สิงหาคม ไม่ สามารถ ตรวจสอบ ตาม เวลา 7   วัน ส่ง ผล ให้ นายก รัฐมนตรี สามารถ สังหาร ให้ สัมปทาน ไป เพียง ครั้ง หนึ่ง เครือข่าย กรมทางหลวง ไม่ ได้ พูด อะไร หาก ไม่ ได้ สนใจ ที่ แต่ เสีย ชีวิต จึง ได้ มุ่ง ลา ออก ต่อ ศาล ว่า การ ดำเนิน กิจการ  '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"<s>|วัน|จันทร์\"\n",
    "generate_seq(context, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fr536NVvGX3"
   },
   "source": [
    "## Questions: Answer the following in MyCourseville\n",
    "\n",
    "1. What is the perplexity of the neural LM you trained?\\\n",
    "**Answer**: 60.84\n",
    "\n",
    "2. Paste your favorite sentence generated with the LM.\\\n",
    "**Answer**: 'วัน จันทร์ ที่ 15 สิงหาคม ไม่ สามารถ ตรวจสอบ ตาม เวลา 7   วัน ส่ง ผล ให้ นายก รัฐมนตรี สามารถ สังหาร ให้ สัมปทาน ไป เพียง ครั้ง หนึ่ง เครือข่าย กรมทางหลวง ไม่ ได้ พูด อะไร หาก ไม่ ได้ สนใจ ที่ แต่ เสีย ชีวิต จึง ได้ มุ่ง ลา ออก ต่อ ศาล ว่า การ ดำเนิน กิจการ  '"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1sCWXCiBDs6UhYtxXTfWfIb30nC7kHTgc",
     "timestamp": 1612276273852
    },
    {
     "file_id": "1FIsDx7KTE5tiF-Xag22pl4VLQZc6G8Yw",
     "timestamp": 1612057948373
    }
   ],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
