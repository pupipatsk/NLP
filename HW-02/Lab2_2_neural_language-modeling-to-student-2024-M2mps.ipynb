{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "15QfB7RAuXAc"
   },
   "source": [
    "# Neural Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gucid6KNuXAe"
   },
   "source": [
    "In this Exercise, we will be using Pytorch Lightning to implement our neural LM. Your job will be just to write the forward method of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yL_M2zf4myYa"
   },
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MRRrn78ZjL54"
   },
   "outputs": [],
   "source": [
    "# # # #download corpus\n",
    "# # !wget --no-check-certificate https://github.com/ekapolc/nlp_2019/raw/master/HW4/BEST2010.zip\n",
    "# !curl -L -o BEST2010.zip https://github.com/ekapolc/nlp_2019/raw/master/HW4/BEST2010.zip\n",
    "# !unzip BEST2010.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SGmYebp38OUl"
   },
   "outputs": [],
   "source": [
    "# !pip install lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IR4HK5jQm17K"
   },
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oPE1RqKOrWJ0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in BEST2010 news training dataset :\t21678\n",
      "Total word counts in BEST2010 news training dataset :\t1042797\n"
     ]
    }
   ],
   "source": [
    "total_word_count = 0\n",
    "best2010 = []\n",
    "with open('BEST2010/news.txt','r',encoding='utf-8') as f:\n",
    "  for i,line in enumerate(f):\n",
    "    line=line.strip()[:-1] #remove the trailing |\n",
    "    total_word_count += len(line.split(\"|\"))\n",
    "    best2010.append(line)\n",
    "\n",
    "train = best2010[:int(len(best2010)*0.7)]\n",
    "test = best2010[int(len(best2010)*0.7):]\n",
    "#Training data\n",
    "train_word_count =0\n",
    "for line in train:\n",
    "    for word in line.split('|'):\n",
    "        train_word_count+=1\n",
    "print ('Total sentences in BEST2010 news training dataset :\\t'+ str(len(train)))\n",
    "print ('Total word counts in BEST2010 news training dataset :\\t'+ str(train_word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQBjqe5arHGX"
   },
   "source": [
    "Here we are going to use a library from huggingface called `tokenizers`. This will help us create a vocabulary and handle the encoding and decoding, i.e., convert text to its corresponding ID (which will be learned by the tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "elwE0gh2rE3C"
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import CharDelimiterSplit\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "#Basically, we just use the new tokenizer as our vocab building tool.\n",
    "#In practice, you will have to use a compatible tokenizer like newmm to tokenize the corpus first then do this step\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = CharDelimiterSplit(delimiter=\"|\") #now the tokenizer will split \"|\" for us\n",
    "trainer = WordLevelTrainer(min_frequency=3,  #we can set a frequency threshold for taking a word into our vocab. for this example, words with freq < 3 will be excluded from the vocab.\n",
    "                           special_tokens=[\"[UNK]\", \"<s>\", \"</s>\"]) #these are our special tokens: for unknown, begin-of-sentence, and end-of-sentence, respectively.\n",
    "tokenizer.train_from_iterator(train, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TrKtjv4PJpg2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9062"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_vocab()) #same as nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WqM_jrZwrJpB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['กฎหมาย', 'กับ', 'การ', 'เบียดบัง', 'คน', 'จน', '[UNK]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"กฎหมาย|กับ|การ|เบียดบัง|คน|จน|asdf\").tokens #tokens we get after tokenizing this sentence. unknown words will be tokenized as [UNK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1r1pJ1B_sp9j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[242, 28, 5, 8883, 22, 190, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"กฎหมาย|กับ|การ|เบียดบัง|คน|จน|asdf\").ids #this is what we will feed to the LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Fkx6CSoXWXmG"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import lightning as L\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3XHJsP8_898x"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-r_kyrrrDHZq"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "  def __init__(self, data, seq_len = 128):\n",
    "    #  data is currently a list of sentences\n",
    "    #  [sent1,\n",
    "    #   sent2,\n",
    "    #   ...,\n",
    "    #  ]\n",
    "\n",
    "    data = [d+'|</s>' for d in data] #append an </s> token to each sentence\n",
    "    encodings = tokenizer.encode_batch(data) #encode (turn token into token_id) data\n",
    "    token_ids = [enc.ids for enc in encodings] #get the token ids for each sentence\n",
    "    flatten_token_ids = list(itertools.chain(*token_ids)) #turn a list of token_ids into one long token_ids\n",
    "    ## now data looks like this [sent1_ids </s> sent2_ids </s> ...]\n",
    "    encoded = torch.LongTensor(flatten_token_ids)\n",
    "\n",
    "    #remove some left over tokens so that we can form batches of seq_len (128 in this case). Optionally, we can use padding tokens instead.\n",
    "    left_over = len(encoded) % seq_len\n",
    "    encoded = encoded[:len(encoded)-left_over]\n",
    "    self.encoded = encoded.view(-1, seq_len) #reshape data so it becomes a 2-D matrix of shape (len(encoded)//128, 128), i.e. each row contains data of len==128\n",
    "    ## now data looks like this\n",
    "    ## [ [1,2,3, ... , 128] (this is just an example, not actual input_ids)\n",
    "    ##   [1,2,3, ... , 128]\n",
    "    ##   [1,2,3, ... , 128]\n",
    "    ## ]\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.encoded[idx]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YmW-K0XBZ4Dq"
   },
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "test_batch_size = 128\n",
    "train_dataset = TextDataset(train)\n",
    "train_loader = DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True) #DataLoader will take care of the random sampling and batching of data\n",
    "\n",
    "test_dataset = TextDataset(test)\n",
    "test_loader = DataLoader(test_dataset, batch_size = test_batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElhZcB94MUtC"
   },
   "source": [
    "## Model : Implement the forward function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nKNJAolug-1I"
   },
   "outputs": [],
   "source": [
    "class LSTM(L.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, learning_rate, criterion):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim) #this will turn the token ids into vectors\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size) #turn the vectors back into token ids\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def forward(self, src):\n",
    "        # TODO: Implement the forward pass\n",
    "        embedded = self.embedding(src)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        src = batch[:, :-1]\n",
    "        target = batch[:, 1:]\n",
    "        prediction = self(src) # run the sequence through the model (the forward method)\n",
    "        prediction = prediction.reshape(-1, vocab_size)\n",
    "        target = target.reshape(-1)\n",
    "        loss = self.criterion(prediction, target)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        src = batch[:, :-1]  #[batch_size (64) , seq_len-1 (127)] except last words\n",
    "        target = batch[:, 1:] #[batch_size (64) , seq_len-1 (127)] except first words\n",
    "        with torch.no_grad(): #disable gradient calculation for faster inference\n",
    "          prediction = self(src) #[batch_size (64), seq_len-1 (127) , vocab size (9000)]\n",
    "        prediction = prediction.reshape(-1, vocab_size) #[batch_size*(seq_len-1) (64*127=8128) , vocab]\n",
    "        target = target.reshape(-1) #[batch_size (64), seq_len-1 (127)] -> [batch_size*(seq_len-1) (8128)]\n",
    "        loss = self.criterion(prediction, target)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jBnYCh-miOEr"
   },
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "embedding_dim = 200\n",
    "hidden_dim = 512\n",
    "num_layers = 3\n",
    "dropout_rate = 0.2\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HHWXaPsvigPq"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, lr, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_yNEZ4jwXumR"
   },
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import CSVLogger\n",
    "csv_logger = CSVLogger(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZwqhWicMdH0"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kr0zdeMAjD1U"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=20,\n",
    "    logger=csv_logger,\n",
    "    deterministic=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "A9qcwNA0mN6J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | embedding | Embedding        | 1.8 M  | train\n",
      "1 | lstm      | LSTM             | 5.7 M  | train\n",
      "2 | dropout   | Dropout          | 0      | train\n",
      "3 | fc        | Linear           | 4.6 M  | train\n",
      "4 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "12.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.1 M    Total params\n",
      "48.504    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/pupipatsingkhorn/miniconda3/envs/datascience/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3f79aed92149dc9103f1c8da7647d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=train_loader) # takes about 8 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to local\n",
    "# torch.save(model, 'lstm_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUfWF_V6Me9H"
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WXVj9ewNqweZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pupipatsingkhorn/miniconda3/envs/datascience/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89eb5c07c04440e0881a2bc15379be9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     4.483534336090088     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    4.483534336090088    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_result = trainer.test(model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4pVjEyYDtnc-"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "uuIPToGQs-ZG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity : 88.54707540661047\n"
     ]
    }
   ],
   "source": [
    "print(f\"Perplexity : {np.exp(test_result[0]['test_loss'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "pAZwiRqsnOPe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(9062, 200)\n",
       "  (lstm): LSTM(200, 512, num_layers=3, batch_first=True, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=9062, bias=True)\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() #disable dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "VFtebDAmVh_T"
   },
   "outputs": [],
   "source": [
    "unk_token_id = tokenizer.encode(\"[UNK]\").ids\n",
    "eos_token_id = tokenizer.encode(\"</s>\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "hj-V4OsDqpBO"
   },
   "outputs": [],
   "source": [
    "def generate_seq(context, max_new_token = 10):\n",
    "  encoded = tokenizer.encode(context).ids\n",
    "  with torch.no_grad():\n",
    "      for i in range(max_new_token):\n",
    "          src = torch.LongTensor([encoded]).to(model.device)\n",
    "          prediction = model(src)\n",
    "          probs = torch.softmax(prediction[:, -1] / 1, dim=-1)\n",
    "          prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "          while prediction == unk_token_id:\n",
    "              prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "          if prediction == eos_token_id:\n",
    "              break\n",
    "\n",
    "          encoded.append(prediction)\n",
    "\n",
    "  return tokenizer.decode(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "u20r9w8zvJi4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'วัน จันทร์ เดียว มา แน่ เฉลิมฉลอง โลก ทุจริต บะหมี่ กรุงเทพฯ ลักลอบ แพร่ 1 โปรตุเกส ขี้ เต็ม ค่า บริการ ต่อ แผ่นดิน ปรากฎ 80 พุ่ง ต่อ ชั่วโมง เด็ก คว้า เคลื่อน \" 7 อาทิตย์ นาที 2 15 ปี ทรง ฟื้นฟู 12 เยอรมนี อันดับ ประกาศ ประกาศ คะแนน โอเพ่น น้ำหนัก โลก 35 กว่า 5 เขายายเที่ยง 4'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"<s>|วัน|จันทร์\"\n",
    "generate_seq(context, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fr536NVvGX3"
   },
   "source": [
    "## Questions: Answer the following in MyCourseville\n",
    "\n",
    "1. What is the perplexity of the neural LM you trained?\n",
    "2. Paste your favorite sentence generated with the LM."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1sCWXCiBDs6UhYtxXTfWfIb30nC7kHTgc",
     "timestamp": 1612276273852
    },
    {
     "file_id": "1FIsDx7KTE5tiF-Xag22pl4VLQZc6G8Yw",
     "timestamp": 1612057948373
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
