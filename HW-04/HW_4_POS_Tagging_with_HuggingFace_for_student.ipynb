{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"toc_visible":true},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HW 4 - POS Tagging with Hugging Face\n\nIn this exercise, you will create a part-of-speech (POS) tagging system for Thai text using NECTEC’s ORCHID corpus. Instead of building your own deep learning architecture from scratch, you will leverage a pretrained tokenizer and a pretrained token classification model from Hugging Face.\n\nWe have provided some starter code for data cleaning and preprocessing in this notebook, but feel free to modify those parts to suit your needs. You are welcome to use additional libraries (e.g., scikit-learn) as long as you incorporate the pretrained Hugging Face model. Specifically, you will need to:\n\n1. Load a pretrained tokenizer and token classification model.\n2. Fine-tune it on the ORCHID corpus for POS tagging.\n3. Evaluate and report the performance of your model on the test data.\n\n### Don't forget to change hardware accelrator to GPU in runtime on Google Colab ###","metadata":{"id":"UxOTS6n1ikVL"}},{"cell_type":"markdown","source":"## 1. Setup and Preprocessing","metadata":{"id":"bQ1Uqldlj81G"}},{"cell_type":"code","source":"# Install transformers and thai2transformers\n!pip install wandb\n!pip install -q transformers==4.30.1 datasets evaluate thaixtransformers\n!pip install -q emoji pythainlp sefr_cut tinydb seqeval sentencepiece pydantic jsonlines\n!pip install peft==0.10.0","metadata":{"id":"kyb4FhsEEeH8","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:33.056482Z","iopub.execute_input":"2025-01-30T08:07:33.056832Z","iopub.status.idle":"2025-01-30T08:07:47.412061Z","shell.execute_reply.started":"2025-01-30T08:07:33.056802Z","shell.execute_reply":"2025-01-30T08:07:47.411041Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nRequirement already satisfied: peft==0.10.0 in /usr/local/lib/python3.10/dist-packages (0.10.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (2.5.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (4.30.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (1.2.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.10.0) (0.27.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2024.9.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft==0.10.0) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.10.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.10.0) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.10.0) (2024.11.6)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.10.0) (0.13.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.10.0) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft==0.10.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft==0.10.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft==0.10.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->peft==0.10.0) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (2024.12.14)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->peft==0.10.0) (2024.2.0)\n","output_type":"stream"}],"execution_count":283},{"cell_type":"markdown","source":"## Setup\n\n1. Register [Wandb account](https://wandb.ai/login?signup=true) (and confirm your email)\n\n2. `wandb login` and copy paste the API key when prompt","metadata":{"id":"fTgw8WW3BxWZ"}},{"cell_type":"code","source":"import wandb\n# import os\n# wandb.login(key=os.environ.get(\"WANDB_API_KEY\"))\n# wandb.login(key='')","metadata":{"id":"4qSh7MXZB74z","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:47.413643Z","iopub.execute_input":"2025-01-30T08:07:47.413894Z","iopub.status.idle":"2025-01-30T08:07:47.417865Z","shell.execute_reply.started":"2025-01-30T08:07:47.413873Z","shell.execute_reply":"2025-01-30T08:07:47.417042Z"}},"outputs":[],"execution_count":284},{"cell_type":"markdown","source":"We encourage you to login to your `Hugging Face` account so you can upload and share your model with the community. When prompted, enter your token to login","metadata":{"id":"i-BR7danGv6W"}},{"cell_type":"code","source":"# from huggingface_hub import notebook_login\n\n# notebook_login()","metadata":{"id":"n7h8NENllZK2","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:47.419192Z","iopub.execute_input":"2025-01-30T08:07:47.419392Z","iopub.status.idle":"2025-01-30T08:07:47.433567Z","shell.execute_reply.started":"2025-01-30T08:07:47.419375Z","shell.execute_reply":"2025-01-30T08:07:47.432786Z"}},"outputs":[],"execution_count":285},{"cell_type":"markdown","source":"Download the dataset from Hugging Face","metadata":{"id":"XqkDkseilv19"}},{"cell_type":"code","source":"from datasets import load_dataset\n\norchid = load_dataset(\"Thichow/orchid_corpus\", trust_remote_code=True)","metadata":{"id":"kRksERXFEngl","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:47.434882Z","iopub.execute_input":"2025-01-30T08:07:47.435192Z","iopub.status.idle":"2025-01-30T08:07:48.217125Z","shell.execute_reply.started":"2025-01-30T08:07:47.435165Z","shell.execute_reply":"2025-01-30T08:07:48.216475Z"}},"outputs":[],"execution_count":286},{"cell_type":"code","source":"orchid","metadata":{"id":"T_AWd4d5lCYd","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.217911Z","iopub.execute_input":"2025-01-30T08:07:48.218221Z","iopub.status.idle":"2025-01-30T08:07:48.223385Z","shell.execute_reply.started":"2025-01-30T08:07:48.218198Z","shell.execute_reply":"2025-01-30T08:07:48.222730Z"}},"outputs":[{"execution_count":287,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'label_tokens', 'pos_tags', 'sentence'],\n        num_rows: 18500\n    })\n    test: Dataset({\n        features: ['id', 'label_tokens', 'pos_tags', 'sentence'],\n        num_rows: 4625\n    })\n})"},"metadata":{}}],"execution_count":287},{"cell_type":"code","source":"orchid['train'][0]","metadata":{"id":"QNmIqSo0FkAx","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.224344Z","iopub.execute_input":"2025-01-30T08:07:48.224596Z","iopub.status.idle":"2025-01-30T08:07:48.238675Z","shell.execute_reply.started":"2025-01-30T08:07:48.224568Z","shell.execute_reply":"2025-01-30T08:07:48.238025Z"}},"outputs":[{"execution_count":288,"output_type":"execute_result","data":{"text/plain":"{'id': '0',\n 'label_tokens': ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1'],\n 'pos_tags': [21, 39, 26, 26, 37, 4, 18],\n 'sentence': 'การประชุมทางวิชาการ ครั้งที่ 1'}"},"metadata":{}}],"execution_count":288},{"cell_type":"code","source":"orchid['train'][0][\"sentence\"]","metadata":{"id":"hUuz3dLGlI_S","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.239432Z","iopub.execute_input":"2025-01-30T08:07:48.239692Z","iopub.status.idle":"2025-01-30T08:07:48.252623Z","shell.execute_reply.started":"2025-01-30T08:07:48.239650Z","shell.execute_reply":"2025-01-30T08:07:48.252005Z"}},"outputs":[{"execution_count":289,"output_type":"execute_result","data":{"text/plain":"'การประชุมทางวิชาการ ครั้งที่ 1'"},"metadata":{}}],"execution_count":289},{"cell_type":"code","source":"''.join(orchid['train'][0]['label_tokens'])","metadata":{"id":"bh7fX19zI85W","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.255065Z","iopub.execute_input":"2025-01-30T08:07:48.255257Z","iopub.status.idle":"2025-01-30T08:07:48.266396Z","shell.execute_reply.started":"2025-01-30T08:07:48.255240Z","shell.execute_reply":"2025-01-30T08:07:48.265712Z"}},"outputs":[{"execution_count":290,"output_type":"execute_result","data":{"text/plain":"'การประชุมทางวิชาการ ครั้งที่ 1'"},"metadata":{}}],"execution_count":290},{"cell_type":"code","source":"label_list = orchid[\"train\"].features[f\"pos_tags\"].feature.names\nprint('total type of pos_tags :', len(label_list))\nprint(label_list)","metadata":{"id":"38jM9YcSFmjV","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.267794Z","iopub.execute_input":"2025-01-30T08:07:48.268061Z","iopub.status.idle":"2025-01-30T08:07:48.279422Z","shell.execute_reply.started":"2025-01-30T08:07:48.268035Z","shell.execute_reply":"2025-01-30T08:07:48.278767Z"}},"outputs":[{"name":"stdout","text":"total type of pos_tags : 47\n['ADVI', 'ADVN', 'ADVP', 'ADVS', 'CFQC', 'CLTV', 'CMTR', 'CMTR@PUNC', 'CNIT', 'CVBL', 'DCNM', 'DDAC', 'DDAN', 'DDAQ', 'DDBQ', 'DIAC', 'DIAQ', 'DIBQ', 'DONM', 'EAFF', 'EITT', 'FIXN', 'FIXV', 'JCMP', 'JCRG', 'JSBR', 'NCMN', 'NCNM', 'NEG', 'NLBL', 'NONM', 'NPRP', 'NTTL', 'PDMN', 'PNTR', 'PPRS', 'PREL', 'PUNC', 'RPRE', 'VACT', 'VATT', 'VSTA', 'XVAE', 'XVAM', 'XVBB', 'XVBM', 'XVMM']\n","output_type":"stream"}],"execution_count":291},{"cell_type":"code","source":"import numpy as np\nimport numpy.random\nimport torch\n\nfrom tqdm.auto import tqdm\nfrom functools import partial\n\n#transformers\nfrom transformers import (\n    CamembertTokenizer,\n    AutoTokenizer,\n    AutoModel,\n    AutoModelForMaskedLM,\n    AutoModelForSequenceClassification,\n    AutoModelForTokenClassification,\n    TrainingArguments,\n    Trainer,\n    pipeline,\n)\n\n#thaixtransformers\nfrom thaixtransformers import Tokenizer\nfrom thaixtransformers.preprocess import process_transformers","metadata":{"id":"Uf_NDWg7F6z_","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.280184Z","iopub.execute_input":"2025-01-30T08:07:48.280460Z","iopub.status.idle":"2025-01-30T08:07:48.292592Z","shell.execute_reply.started":"2025-01-30T08:07:48.280395Z","shell.execute_reply":"2025-01-30T08:07:48.291812Z"}},"outputs":[],"execution_count":292},{"cell_type":"markdown","source":"Next, we load a pretrained tokenizer from Hugging Face. In this work, we utilize WangchanBERTa, a Thai-specific pretrained model, as the tokenizer.","metadata":{"id":"T1suScqmntBW"}},{"cell_type":"markdown","source":"# Choose Pretrained Model","metadata":{"id":"jt3ASYUVm54n"}},{"cell_type":"markdown","source":"In this notebook, you can choose from 5 versions of WangchanBERTa, XLMR and mBERT to perform downstream tasks on Thai datasets. The datasets are:\n\n* `wangchanberta-base-att-spm-uncased` (recommended) - Largest WangchanBERTa trained on 78.5GB of Assorted Thai Texts with subword tokenizer SentencePiece\n* `xlm-roberta-base` - Facebook's [XLMR](https://arxiv.org/abs/1911.02116) trained on 100 languages\n* `bert-base-multilingual-cased` - Google's [mBERT](https://arxiv.org/abs/1911.03310) trained on 104 languages\n* `wangchanberta-base-wiki-newmm` - WangchanBERTa trained on Thai Wikipedia Dump with PyThaiNLP's word-level tokenizer  `newmm`\n* `wangchanberta-base-wiki-syllable` - WangchanBERTa trained on Thai Wikipedia Dump with PyThaiNLP's syllabel-level tokenizer `syllable`\n* `wangchanberta-base-wiki-sefr` - WangchanBERTa trained on Thai Wikipedia Dump with word-level tokenizer  `SEFR`\n* `wangchanberta-base-wiki-spm` - WangchanBERTa trained on Thai Wikipedia Dump with subword-level tokenizer SentencePiece\n\nIn the first part, we require you to select the wangchanberta-base-att-spm-uncased.","metadata":{"id":"sFBKLqbIm23-"}},{"cell_type":"markdown","source":"<b> Learn more about using wangchanberta at [wangchanberta_getting_started_ai_reseach](https://colab.research.google.com/github/PyThaiNLP/thaixtransformers/blob/main/notebooks/wangchanberta_getting_started_aireseach.ipynb?fbclid=IwY2xjawH61XZleHRuA2FlbQIxMAABHZUaAmHobzmCMHpX0EgdLdjDAEwSX0bjqpo5xPUSIx9b4O_dsIvvG8KVNA_aem_IyKkvzy-VPf9k2pYAFf6Nw#scrollTo=n5IaCot9b3cF) <b>\n\n\n\n*   You need to set the transformers version to transformers==4.30.1.\n\n","metadata":{"id":"6HbZo_TZDn17"}},{"cell_type":"markdown","source":"`In the first part, we require you to select the wangchanberta-base-att-spm-uncased.`","metadata":{"id":"kl2NposVIh9-"}},{"cell_type":"code","source":"model_names = [\n    'airesearch/wangchanberta-base-att-spm-uncased',\n    'airesearch/wangchanberta-base-wiki-newmm',\n    'airesearch/wangchanberta-base-wiki-ssg',\n    'airesearch/wangchanberta-base-wiki-sefr',\n    'airesearch/wangchanberta-base-wiki-spm',\n]\n\n#@title Choose Pretrained Model\nmodel_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n\n#create tokenizer\ntokenizer = Tokenizer(model_name).from_pretrained(\n                f'{model_name}',\n                revision='main',\n                model_max_length=416,)\n","metadata":{"id":"n5IaCot9b3cF","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.293296Z","iopub.execute_input":"2025-01-30T08:07:48.293473Z","iopub.status.idle":"2025-01-30T08:07:48.705243Z","shell.execute_reply.started":"2025-01-30T08:07:48.293457Z","shell.execute_reply":"2025-01-30T08:07:48.704328Z"}},"outputs":[{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \nThe class this function is called from is 'WangchanbertaTokenizer'.\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \nThe class this function is called from is 'WangchanbertaTokenizer'.\n","output_type":"stream"}],"execution_count":293},{"cell_type":"markdown","source":"Let's try using a pretrained tokenizer.","metadata":{"id":"LzdbERHLwd0X"}},{"cell_type":"code","source":"text = 'ศิลปะไม่เป็นเจ้านายใคร และไม่เป็นขี้ข้าใคร'\nprint('text :', text)\ntokens = []\nfor i in tokenizer([text], is_split_into_words=True)['input_ids']:\n  tokens.append(tokenizer.decode(i))\nprint('tokens :', tokens)","metadata":{"id":"qwrwXsHFwl-G","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.706491Z","iopub.execute_input":"2025-01-30T08:07:48.706788Z","iopub.status.idle":"2025-01-30T08:07:48.713980Z","shell.execute_reply.started":"2025-01-30T08:07:48.706763Z","shell.execute_reply":"2025-01-30T08:07:48.713167Z"}},"outputs":[{"name":"stdout","text":"text : ศิลปะไม่เป็นเจ้านายใคร และไม่เป็นขี้ข้าใคร\ntokens : ['<s>', '', 'ศิลปะ', 'ไม่เป็น', 'เจ้านาย', 'ใคร', '<_>', 'และ', 'ไม่เป็น', 'ขี้ข้า', 'ใคร', '</s>']\n","output_type":"stream"}],"execution_count":294},{"cell_type":"markdown","source":"model : * `wangchanberta-base-att-spm-uncased`\n\nFirst, we print examples of label tokens from our dataset for inspection.","metadata":{"id":"dEQAVqO8pDhK"}},{"cell_type":"code","source":"example = orchid[\"train\"][0]\nfor i in example :\n    print(i, ':', example[i])","metadata":{"id":"Vw_GdRdlpAhu","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.714757Z","iopub.execute_input":"2025-01-30T08:07:48.714958Z","iopub.status.idle":"2025-01-30T08:07:48.730805Z","shell.execute_reply.started":"2025-01-30T08:07:48.714932Z","shell.execute_reply":"2025-01-30T08:07:48.729907Z"}},"outputs":[{"name":"stdout","text":"id : 0\nlabel_tokens : ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']\npos_tags : [21, 39, 26, 26, 37, 4, 18]\nsentence : การประชุมทางวิชาการ ครั้งที่ 1\n","output_type":"stream"}],"execution_count":295},{"cell_type":"markdown","source":"Then, we use the sentence 'การประชุมทางวิชาการ<space>ครั้งที่ 1' to be tokenized by the pretrained tokenizer model.","metadata":{"id":"yTuiwEWkppdA"}},{"cell_type":"code","source":"text = 'การประชุมทางวิชาการ ครั้งที่ 1'\ntokenizer(text)","metadata":{"id":"BRCxMtHToN16","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.731931Z","iopub.execute_input":"2025-01-30T08:07:48.732226Z","iopub.status.idle":"2025-01-30T08:07:48.745017Z","shell.execute_reply.started":"2025-01-30T08:07:48.732194Z","shell.execute_reply":"2025-01-30T08:07:48.744370Z"}},"outputs":[{"execution_count":296,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [5, 10, 882, 8222, 8, 10, 1014, 8, 10, 59, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}],"execution_count":296},{"cell_type":"markdown","source":"These are already mapped into discrete values. We can uncover the original token text from the tokens by.","metadata":{"id":"kxi8WqZnGa5F"}},{"cell_type":"code","source":"for i in tokenizer(text)['input_ids']:\n  print(tokenizer.convert_ids_to_tokens(i))","metadata":{"id":"optGK_eco3K6","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.745623Z","iopub.execute_input":"2025-01-30T08:07:48.745831Z","iopub.status.idle":"2025-01-30T08:07:48.759448Z","shell.execute_reply.started":"2025-01-30T08:07:48.745812Z","shell.execute_reply":"2025-01-30T08:07:48.758681Z"}},"outputs":[{"name":"stdout","text":"<s>\n▁\nการประชุม\nทางวิชาการ\n<_>\n▁\nครั้งที่\n<_>\n▁\n1\n</s>\n","output_type":"stream"}],"execution_count":297},{"cell_type":"markdown","source":"Now let's look at another example.","metadata":{"id":"T3l13UKnwK-d"}},{"cell_type":"code","source":"example = orchid[\"train\"][1899]\nprint('sentence :', example[\"sentence\"])\ntokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\ntokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\nprint('tokens :',tokens)\nprint('label tokens :', example[\"label_tokens\"])\nprint('label pos :', example[\"pos_tags\"])","metadata":{"id":"UyfIR3BowU84","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.760353Z","iopub.execute_input":"2025-01-30T08:07:48.760634Z","iopub.status.idle":"2025-01-30T08:07:48.775112Z","shell.execute_reply.started":"2025-01-30T08:07:48.760611Z","shell.execute_reply":"2025-01-30T08:07:48.774262Z"}},"outputs":[{"name":"stdout","text":"sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (Bilingual transfer dictionary)\ntokens : ['<s>', '▁โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '▁(', '<unk>', 'i', 'ling', 'ual', '<_>', '▁', 'trans', 'fer', '<_>', '▁', 'di', 'ction', 'ary', ')', '</s>']\nlabel tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'Bilingual transfer dictionary', ')']\nlabel pos : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\n","output_type":"stream"}],"execution_count":298},{"cell_type":"markdown","source":"Notice how `B` becomes an ``<unk>`` token. This is because this is an uncased model, meaning it only handles small English characters.","metadata":{"id":"cmV6M-vAwew5"}},{"cell_type":"markdown","source":"# #TODO 0\n\nConvert the dataset to lowercase.","metadata":{"id":"WniJR47ww7a0"}},{"cell_type":"code","source":"# Create a lowercase dataset for uncased BERT\ndef lower_case_sentences(examples: dict):\n    lower_cased_examples = examples\n    # TODO: fill code here to lower case the \"sentence\" and \"label_tokens\"\n    sentence = lower_cased_examples[\"sentence\"].lower()\n    label_tokens = [token.lower() for token in lower_cased_examples[\"label_tokens\"]]\n    lower_cased_examples[\"sentence\"] = sentence\n    lower_cased_examples[\"label_tokens\"] = label_tokens\n    return lower_cased_examples","metadata":{"id":"RQWm_iWBxFQ8","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.775954Z","iopub.execute_input":"2025-01-30T08:07:48.776273Z","iopub.status.idle":"2025-01-30T08:07:48.788814Z","shell.execute_reply.started":"2025-01-30T08:07:48.776248Z","shell.execute_reply":"2025-01-30T08:07:48.788054Z"}},"outputs":[],"execution_count":299},{"cell_type":"code","source":"orchidl = orchid.map(lower_case_sentences)","metadata":{"id":"ndBIqEpWuqBP","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.789686Z","iopub.execute_input":"2025-01-30T08:07:48.790013Z","iopub.status.idle":"2025-01-30T08:07:48.810619Z","shell.execute_reply.started":"2025-01-30T08:07:48.789969Z","shell.execute_reply":"2025-01-30T08:07:48.809916Z"}},"outputs":[],"execution_count":300},{"cell_type":"code","source":"orchidl","metadata":{"id":"z8xpcCqTrqbc","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.811731Z","iopub.execute_input":"2025-01-30T08:07:48.812033Z","iopub.status.idle":"2025-01-30T08:07:48.817836Z","shell.execute_reply.started":"2025-01-30T08:07:48.811986Z","shell.execute_reply":"2025-01-30T08:07:48.817191Z"}},"outputs":[{"execution_count":301,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'label_tokens', 'pos_tags', 'sentence'],\n        num_rows: 18500\n    })\n    test: Dataset({\n        features: ['id', 'label_tokens', 'pos_tags', 'sentence'],\n        num_rows: 4625\n    })\n})"},"metadata":{}}],"execution_count":301},{"cell_type":"code","source":"orchidl[\"train\"][1899]","metadata":{"id":"ecpDHyTPv2py","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.818526Z","iopub.execute_input":"2025-01-30T08:07:48.818789Z","iopub.status.idle":"2025-01-30T08:07:48.832815Z","shell.execute_reply.started":"2025-01-30T08:07:48.818768Z","shell.execute_reply":"2025-01-30T08:07:48.832140Z"}},"outputs":[{"execution_count":302,"output_type":"execute_result","data":{"text/plain":"{'id': '1899',\n 'label_tokens': ['โดย',\n  'พิจารณา',\n  'จาก',\n  'พจนานุกรม',\n  'ภาษา',\n  'คู่',\n  ' ',\n  '(',\n  'bilingual transfer dictionary',\n  ')'],\n 'pos_tags': [25, 39, 38, 26, 26, 5, 37, 37, 26, 37],\n 'sentence': 'โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)'}"},"metadata":{}}],"execution_count":302},{"cell_type":"markdown","source":"Now let's examine the labels again.","metadata":{"id":"rgV4ohz2xTY9"}},{"cell_type":"code","source":"example = orchidl[\"train\"][1899]\nprint('sentence :', example[\"sentence\"])\ntokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\ntokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\nprint('tokens :',tokens)\nprint('label tokens :', example[\"label_tokens\"])\nprint('label pos :', example[\"pos_tags\"])","metadata":{"id":"DoUDQzM7q265","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.833515Z","iopub.execute_input":"2025-01-30T08:07:48.833766Z","iopub.status.idle":"2025-01-30T08:07:48.849438Z","shell.execute_reply.started":"2025-01-30T08:07:48.833738Z","shell.execute_reply":"2025-01-30T08:07:48.848776Z"}},"outputs":[{"name":"stdout","text":"sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\ntokens : ['<s>', '▁โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '▁(', 'bi', 'ling', 'ual', '<_>', '▁', 'trans', 'fer', '<_>', '▁', 'di', 'ction', 'ary', ')', '</s>']\nlabel tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\nlabel pos : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\n","output_type":"stream"}],"execution_count":303},{"cell_type":"code","source":"example = orchidl[\"train\"][0]\nprint('sentence :', example[\"sentence\"])\ntokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\ntokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\nprint('tokens :',tokens)\nprint('label tokens :', example[\"label_tokens\"])\nprint('label pos :', example[\"pos_tags\"])","metadata":{"id":"aEHgBeX7fQFt","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.850090Z","iopub.execute_input":"2025-01-30T08:07:48.850270Z","iopub.status.idle":"2025-01-30T08:07:48.865705Z","shell.execute_reply.started":"2025-01-30T08:07:48.850254Z","shell.execute_reply":"2025-01-30T08:07:48.865069Z"}},"outputs":[{"name":"stdout","text":"sentence : การประชุมทางวิชาการ ครั้งที่ 1\ntokens : ['<s>', '▁', 'การประชุม', 'ทางวิชาการ', '<_>', '▁', 'ครั้งที่', '<_>', '▁', '1', '</s>']\nlabel tokens : ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']\nlabel pos : [21, 39, 26, 26, 37, 4, 18]\n","output_type":"stream"}],"execution_count":304},{"cell_type":"markdown","source":"In the example above, tokens refer to those tokenized using the pretrained tokenizer, while label tokens refer to tokens tokenized from our dataset.","metadata":{"id":"5dVcLxYbrl4E"}},{"cell_type":"markdown","source":"**Do you see something?**\n\nYes, the tokens from the two tokenizers do not match.\n\n- sentence : `การประชุมทางวิชาการ ครั้งที่ 1`\n\n---\n\n- tokens : `['<s>', '▁', 'การประชุม', 'ทางวิชาการ', '<_>', '▁', 'ครั้งที่', '<_>', '▁', '1', '</s>']`\n\n\n---\n\n\n- label tokens : `['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']`\n- label pos : `[21, 39, 26, 26, 37, 4, 18]`\n\nYou can see that in our label tokens, 'การ' has a POS tag of 21, and 'ประชุม' has a POS tag of 39. However, when we tokenize the sentence using WangchanBERTa, we get the token 'การประชุม'. What POS tag should we assign to this new token?\n\n**What should we do ?**\n\nBased on this example, we found that the tokens from the WangchanBERTa do not directly align with our label tokens. This means we cannot directly use the label POS tags. Therefore, we need to reassign POS tags to the tokens produced by WangchanBERTa tokenization. The method we will use is majority voting:\n- If a token from the WangchanBERTa matches a label token exactly, we will directly assign the POS tag from the label POS.\n- If the token generated overlaps or combines multiple label tokens, we assign the POS tag based on the number of characters in each token: If the token contains the most characters from any label token, we assign the POS tag from that label token.\n\n**Example :**\n\n    # \"การประชุม\" (9 chars) is formed from \"การ\" (3 chars) + \"ประชุม\" (6 chars).\n    # \"การ\" has a POS tag of 21,\n    # and \"ประชุม\" has a POS tag of 39.\n    # Therefore, the POS tag for \"การประชุม\" is 39,\n    # as \"การประชุม\" is derived more from the \"ประชุม\" part than from the \"การ\" part.\n\n    # 'ทางวิชาการ' (10 chars) is formed from 'ทาง' (3 chars) + 'วิชาการ' (7 chars)\n    # \"ทาง\" has a POS tag of 26,\n    # and \"วิชาการ\" has a POS tag of 2.\n    # Therefore, the POS tag for \"ทางวิชาการ\" is 2,\n    # as \"ทางวิชาการ\" is derived more from the \"ทาง\" part than from the \"วิชาการ\" part.","metadata":{"id":"r1inxbOYuBpB"}},{"cell_type":"markdown","source":"# #TODO 1","metadata":{"id":"jTkgye8K8sd8"}},{"cell_type":"markdown","source":"`**Warning: Please be careful of <unk>, an unknown word token.**`\n\n`**Warning: Please be careful of \" ำ \", the 'am' vowel. WangchanBERTa's internal preprocessing replaces all \" ำ \" to 'ํ' and 'า'**`\n\nAssigning the label -100 to the special tokens `[<s>]` and `[</s>]` and `[_]`  so they’re ignored by the PyTorch loss function (see [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html): ignore_index)","metadata":{"id":"lgU8Nudh2rUJ"}},{"cell_type":"code","source":"# example.keys(), example.values()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.869768Z","iopub.execute_input":"2025-01-30T08:07:48.869953Z","iopub.status.idle":"2025-01-30T08:07:48.880254Z","shell.execute_reply.started":"2025-01-30T08:07:48.869937Z","shell.execute_reply":"2025-01-30T08:07:48.879389Z"}},"outputs":[],"execution_count":305},{"cell_type":"code","source":"def majority_vote_pos(examples: dict):\n    \"\"\"\n    Args:\n        examples (dict):\n            id: int\n            label_tokens: list of str\n            pos_tags: list of int\n            sentence: str\n            input_ids: list of int\n            attention_mask: list of int\n            tokens: list of str\n            labels: list of int\n    \"\"\"\n    ####################################################################################################################\n    # TO DO: Since the tokens from the output of the pretrained tokenizer\n    # do not match the tokens in the label tokens of the dataset,\n    # the task is to create a function to determine the POS tags of the tokens generated by the pretrained tokenizer.\n    # This should be done by referencing the POS tags in the label tokens. If a token partially overlaps with others,\n    # the POS tag from the segment with the greater number of characters should be assigned.\n    #\n    # Example :\n    # \"การประชุม\" (9 chars) is formed from \"การ\" (3 chars) + \"ประชุม\" (6 chars).\n    # \"การ\" has a POS tag of 21,\n    # and \"ประชุม\" has a POS tag of 39.\n    # Therefore, the POS tag for \"การประชุม\" is 39,\n    # as \"การประชุม\" is derived more from the \"ประชุม\" part than from the \"การ\" part.\n    #\n    # 'ทางวิชาการ' (10 chars) is formed from 'ทาง' (3 chars) + 'วิชาการ' (7 chars)\n    # \"ทาง\" has a POS tag of 26,\n    # and \"วิชาการ\" has a POS tag of 2.\n    # Therefore, the POS tag for \"ทางวิชาการ\" is 2,\n    # as \"ทางวิชาการ\" is derived more from the \"ทาง\" part than from the \"วิชาการ\" part.\n\n    # tokenize word by pretrained tokenizer\n    tokenized_inputs = tokenizer([examples[\"sentence\"]], is_split_into_words=True)\n    # print(tokenized_inputs) # input_ids (list of int), attention_mask (list of int)\n\n    # TODO: FILL CODE HERE\n\n    new_tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"])\n    label_tokens = examples[\"label_tokens\"]\n    pos_tags = examples[\"pos_tags\"]\n\n    new_pos_result = []\n\n    label_idx = 0  # Index for label_tokens\n    char_idx = 0  # Pointer for matching characters within label tokens\n\n    for token in new_tokens:\n        # PyTorch special tokens\n        if token in [\"<s>\", \"</s>\", \"_\"]:\n            new_pos_result.append(-100)\n            continue\n\n        # Handle space characters and Thai character normalization\n        token = token.replace('ํา', \"ำ\")\n        token = token.replace(\"<_>\", \" \")\n        if token.startswith(\"▁\"):\n            token = token[1:]\n\n        # Ensure token is not empty\n        if not token:\n            new_pos_result.append(-100)\n            continue\n\n        # Move label pointer to correct position\n        while label_idx < len(label_tokens) and char_idx < len(label_tokens[label_idx]):\n            if label_tokens[label_idx][char_idx] == token[0]:\n                break\n            # Move to next label token\n            char_idx += 1\n            if char_idx >= len(label_tokens[label_idx]):\n                label_idx += 1\n                char_idx = 0\n\n        # Align token with label tokens\n        accumulated_text = \"\"\n        pos_weight = {}\n        # Accumulate characters and determine correct POS tag\n        while accumulated_text != token and label_idx < len(label_tokens):\n            accumulated_text += label_tokens[label_idx][char_idx]\n            pos_tag = pos_tags[label_idx]\n\n            if pos_tag not in pos_weight:\n                pos_weight[pos_tag] = 0\n            pos_weight[pos_tag] += 1\n\n            # Move idx\n            char_idx += 1\n            if char_idx >= len(label_tokens[label_idx]):\n                label_idx += 1\n                char_idx = 0\n\n        # Assign POS tag based on majority voting\n        best_pos_tag = max(pos_weight, key=pos_weight.get) if pos_weight else -100\n        new_pos_result.append(best_pos_tag)\n\n    # Update example dictionary with new tokens and labels\n    tokenized_inputs['tokens'] = new_tokens\n    tokenized_inputs['labels'] = new_pos_result\n\n    return tokenized_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.882562Z","iopub.execute_input":"2025-01-30T08:07:48.882779Z","iopub.status.idle":"2025-01-30T08:07:48.894281Z","shell.execute_reply.started":"2025-01-30T08:07:48.882760Z","shell.execute_reply":"2025-01-30T08:07:48.893571Z"}},"outputs":[],"execution_count":306},{"cell_type":"code","source":"tokenized_orchid = orchidl.map(majority_vote_pos)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.894910Z","iopub.execute_input":"2025-01-30T08:07:48.895166Z","iopub.status.idle":"2025-01-30T08:07:48.929182Z","shell.execute_reply.started":"2025-01-30T08:07:48.895145Z","shell.execute_reply":"2025-01-30T08:07:48.928395Z"}},"outputs":[],"execution_count":307},{"cell_type":"code","source":"tokenized_orchid","metadata":{"id":"uvdDnWeOJYpv","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.930390Z","iopub.execute_input":"2025-01-30T08:07:48.930616Z","iopub.status.idle":"2025-01-30T08:07:48.935347Z","shell.execute_reply.started":"2025-01-30T08:07:48.930596Z","shell.execute_reply":"2025-01-30T08:07:48.934631Z"}},"outputs":[{"execution_count":308,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'label_tokens', 'pos_tags', 'sentence', 'input_ids', 'attention_mask', 'tokens', 'labels'],\n        num_rows: 18500\n    })\n    test: Dataset({\n        features: ['id', 'label_tokens', 'pos_tags', 'sentence', 'input_ids', 'attention_mask', 'tokens', 'labels'],\n        num_rows: 4625\n    })\n})"},"metadata":{}}],"execution_count":308},{"cell_type":"code","source":"tokenized_orchid['train'][0]","metadata":{"id":"ojrRF85dJbwf","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.936053Z","iopub.execute_input":"2025-01-30T08:07:48.936251Z","iopub.status.idle":"2025-01-30T08:07:48.948932Z","shell.execute_reply.started":"2025-01-30T08:07:48.936225Z","shell.execute_reply":"2025-01-30T08:07:48.948139Z"}},"outputs":[{"execution_count":309,"output_type":"execute_result","data":{"text/plain":"{'id': '0',\n 'label_tokens': ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1'],\n 'pos_tags': [21, 39, 26, 26, 37, 4, 18],\n 'sentence': 'การประชุมทางวิชาการ ครั้งที่ 1',\n 'input_ids': [5, 10, 882, 8222, 8, 10, 1014, 8, 10, 59, 6],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n 'tokens': ['<s>',\n  '▁',\n  'การประชุม',\n  'ทางวิชาการ',\n  '<_>',\n  '▁',\n  'ครั้งที่',\n  '<_>',\n  '▁',\n  '1',\n  '</s>'],\n 'labels': [-100, -100, 39, 26, 37, -100, 4, 18, -100, 18, -100]}"},"metadata":{}}],"execution_count":309},{"cell_type":"code","source":"example = tokenized_orchid[\"train\"][0]\nfor i in example :\n    print(i, \":\", example[i])","metadata":{"id":"KMfzFnjSdCGI","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.949762Z","iopub.execute_input":"2025-01-30T08:07:48.950058Z","iopub.status.idle":"2025-01-30T08:07:48.965116Z","shell.execute_reply.started":"2025-01-30T08:07:48.950027Z","shell.execute_reply":"2025-01-30T08:07:48.964426Z"}},"outputs":[{"name":"stdout","text":"id : 0\nlabel_tokens : ['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']\npos_tags : [21, 39, 26, 26, 37, 4, 18]\nsentence : การประชุมทางวิชาการ ครั้งที่ 1\ninput_ids : [5, 10, 882, 8222, 8, 10, 1014, 8, 10, 59, 6]\nattention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\ntokens : ['<s>', '▁', 'การประชุม', 'ทางวิชาการ', '<_>', '▁', 'ครั้งที่', '<_>', '▁', '1', '</s>']\nlabels : [-100, -100, 39, 26, 37, -100, 4, 18, -100, 18, -100]\n","output_type":"stream"}],"execution_count":310},{"cell_type":"markdown","source":"This is the result after we realigned the POS based on the majority vote.\n- label_tokens : `['การ', 'ประชุม', 'ทาง', 'วิชาการ', ' ', 'ครั้ง', 'ที่ 1']`\n- pos_tags : `[21, 39, 26, 26, 37, 4, 18]`\n- tokens : `['<s>', '▁', 'การประชุม', 'ทางวิชาการ', '<_>', '▁', 'ครั้งที่', '<_>', '▁', '1', '</s>']`\n- labels : `[-100, -100, 39, 26, 37, -100, 4, 18, -100, 18, -100]`\n\n`['<s>', '▁', '</s>'] : -100`\n\n**Check :**\n\n> \"การประชุม\" (9 chars) is formed from \"การ\" (3 chars) + \"ประชุม\" (6 chars).\n\n\n> \"การ\" has a POS tag of 21,\n\n> and \"ประชุม\" has a POS tag of 39.\n\n> Therefore, the POS tag for \"การประชุม\" is 39,\n\n> as \"การประชุม\" is derived more from the \"ประชุม\" part than from the \"การ\" part.\n\n\n\n","metadata":{"id":"9lhsQcdL6H3J"}},{"cell_type":"code","source":"# hard test case\nexample = tokenized_orchid[\"train\"][1899]\nfor i in example :\n    print(i, \":\", example[i])\n\n# test\nprint(f\"id: {example['id']=='1899'}\")\nprint(f\"label_tokens: {example['label_tokens']==['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']}\")\nprint(f\"pos_tags: {example['pos_tags']==[25, 39, 38, 26, 26, 5, 37, 37, 26, 37]}\")\nprint(f\"sentence: {example['sentence']=='โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)'}\")\nprint(f\"input_ids: {example['input_ids']==[5, 489, 15617, 19737, 958, 493, 8, 1241, 4906, 11608, 12177, 8, 10, 11392, 9806, 8, 10, 2951, 15779, 8001, 29, 6]}\")\nprint(f\"attention_mask: {example['attention_mask']==[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\")\nprint(f\"tokens: {example['tokens']==['<s>', '▁โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '▁(', 'bi', 'ling', 'ual', '<_>', '▁', 'trans', 'fer', '<_>', '▁', 'di', 'ction', 'ary', ')', '</s>']}\")\nprint(f\"labels: {example['labels']==[-100, 25, 39, 26, 26, 5, 37, 37, 26, 26, 26, 26, -100, 26, 26, 26, -100, 26, 26, 26, 37, -100]}\")","metadata":{"id":"iOE5CEgZdO9c","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.965933Z","iopub.execute_input":"2025-01-30T08:07:48.966236Z","iopub.status.idle":"2025-01-30T08:07:48.986158Z","shell.execute_reply.started":"2025-01-30T08:07:48.966208Z","shell.execute_reply":"2025-01-30T08:07:48.985350Z"}},"outputs":[{"name":"stdout","text":"id : 1899\nlabel_tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\npos_tags : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\nsentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\ninput_ids : [5, 489, 15617, 19737, 958, 493, 8, 1241, 4906, 11608, 12177, 8, 10, 11392, 9806, 8, 10, 2951, 15779, 8001, 29, 6]\nattention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\ntokens : ['<s>', '▁โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '▁(', 'bi', 'ling', 'ual', '<_>', '▁', 'trans', 'fer', '<_>', '▁', 'di', 'ction', 'ary', ')', '</s>']\nlabels : [-100, 25, 39, 26, 26, 5, 37, 37, 26, 26, 26, 26, -100, 26, 26, 26, -100, 26, 26, 26, 37, -100]\nid: True\nlabel_tokens: True\npos_tags: True\nsentence: True\ninput_ids: True\nattention_mask: True\ntokens: True\nlabels: True\n","output_type":"stream"}],"execution_count":311},{"cell_type":"markdown","source":"Expected output\n\n\n```\nid : 1899\nlabel_tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\npos_tags : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\nsentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\ninput_ids : [5, 489, 15617, 19737, 958, 493, 8, 1241, 4906, 11608, 12177, 8, 10, 11392, 9806, 8, 10, 2951, 15779, 8001, 29, 6]\nattention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\ntokens : ['<s>', '▁โดย', 'พิจารณาจาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '▁(', 'bi', 'ling', 'ual', '<_>', '▁', 'trans', 'fer', '<_>', '▁', 'di', 'ction', 'ary', ')', '</s>']\nlabels : [-100, 25, 39, 26, 26, 5, 37, 37, 26, 26, 26, 26, -100, 26, 26, 26, -100, 26, 26, 26, 37, -100]\n```","metadata":{"id":"Fv_NkVQ6qsJe"}},{"cell_type":"markdown","source":"# Train and Evaluate model","metadata":{"id":"0pwADd1a85bn"}},{"cell_type":"markdown","source":"We will create a batch of examples using [DataCollatorWithPadding.](https://huggingface.co/docs/transformers/v4.48.0/en/main_classes/data_collator#transformers.DataCollatorWithPadding)  \n\nData collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset.\n\nDataCollatorWithPadding will help us pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length. This allows for efficient computation during each batch.\n\n*   DataCollatorForTokenClassification : `padding (bool, str or PaddingStrategy, optional, defaults to True)`\n*   `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single sequence is provided).\n\n","metadata":{"id":"TsnlIUJvYEy2"}},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)","metadata":{"id":"CcAY4-E2J6e5","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:48.987021Z","iopub.execute_input":"2025-01-30T08:07:48.987298Z","iopub.status.idle":"2025-01-30T08:07:49.003782Z","shell.execute_reply.started":"2025-01-30T08:07:48.987270Z","shell.execute_reply":"2025-01-30T08:07:49.003088Z"}},"outputs":[],"execution_count":312},{"cell_type":"markdown","source":"For evaluating your model’s performance. You can quickly load a evaluation method with the [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [seqeval](https://huggingface.co/spaces/evaluate-metric/seqeval) framework (see the Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric). Seqeval actually produces several scores: precision, recall, F1, and accuracy.","metadata":{"id":"jg4v14KcElbY"}},{"cell_type":"code","source":"import evaluate\n\nseqeval = evaluate.load(\"seqeval\")","metadata":{"id":"cZk3PjndK-Q8","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:49.004927Z","iopub.execute_input":"2025-01-30T08:07:49.005227Z","iopub.status.idle":"2025-01-30T08:07:49.555005Z","shell.execute_reply.started":"2025-01-30T08:07:49.005198Z","shell.execute_reply":"2025-01-30T08:07:49.554212Z"}},"outputs":[],"execution_count":313},{"cell_type":"markdown","source":"Huggingface requires us to write a ``compute_metrics()`` function. This will be invoked when huggingface evalutes a model.\n\nNote that we ignore to evaluate on -100 labels.","metadata":{"id":"FllGDNO5RUIA"}},{"cell_type":"code","source":"import numpy as np\nimport warnings\n\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\")\n        results = seqeval.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }","metadata":{"id":"vDwNPItNLTM1","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:49.555826Z","iopub.execute_input":"2025-01-30T08:07:49.556111Z","iopub.status.idle":"2025-01-30T08:07:49.561816Z","shell.execute_reply.started":"2025-01-30T08:07:49.556088Z","shell.execute_reply":"2025-01-30T08:07:49.560940Z"}},"outputs":[],"execution_count":314},{"cell_type":"markdown","source":"The total number of labels in our POS tag set.","metadata":{"id":"kt13vTldvTw4"}},{"cell_type":"code","source":"id2label = {\n    0: 'ADVI',\n    1: 'ADVN',\n    2: 'ADVP',\n    3: 'ADVS',\n    4: 'CFQC',\n    5: 'CLTV',\n    6: 'CMTR',\n    7: 'CMTR@PUNC',\n    8: 'CNIT',\n    9: 'CVBL',\n    10: 'DCNM',\n    11: 'DDAC',\n    12: 'DDAN',\n    13: 'DDAQ',\n    14: 'DDBQ',\n    15: 'DIAC',\n    16: 'DIAQ',\n    17: 'DIBQ',\n    18: 'DONM',\n    19: 'EAFF',\n    20: 'EITT',\n    21: 'FIXN',\n    22: 'FIXV',\n    23: 'JCMP',\n    24: 'JCRG',\n    25: 'JSBR',\n    26: 'NCMN',\n    27: 'NCNM',\n    28: 'NEG',\n    29: 'NLBL',\n    30: 'NONM',\n    31: 'NPRP',\n    32: 'NTTL',\n    33: 'PDMN',\n    34: 'PNTR',\n    35: 'PPRS',\n    36: 'PREL',\n    37: 'PUNC',\n    38: 'RPRE',\n    39: 'VACT',\n    40: 'VATT',\n    41: 'VSTA',\n    42: 'XVAE',\n    43: 'XVAM',\n    44: 'XVBB',\n    45: 'XVBM',\n    46: 'XVMM',\n    # 47: 'O'\n}\nlabel2id = {}\nfor k, v in id2label.items() :\n    label2id[v] = k\n\nlabel2id","metadata":{"id":"JD84B79-Lxwf","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:49.562625Z","iopub.execute_input":"2025-01-30T08:07:49.562896Z","iopub.status.idle":"2025-01-30T08:07:49.582146Z","shell.execute_reply.started":"2025-01-30T08:07:49.562862Z","shell.execute_reply":"2025-01-30T08:07:49.581353Z"}},"outputs":[{"execution_count":315,"output_type":"execute_result","data":{"text/plain":"{'ADVI': 0,\n 'ADVN': 1,\n 'ADVP': 2,\n 'ADVS': 3,\n 'CFQC': 4,\n 'CLTV': 5,\n 'CMTR': 6,\n 'CMTR@PUNC': 7,\n 'CNIT': 8,\n 'CVBL': 9,\n 'DCNM': 10,\n 'DDAC': 11,\n 'DDAN': 12,\n 'DDAQ': 13,\n 'DDBQ': 14,\n 'DIAC': 15,\n 'DIAQ': 16,\n 'DIBQ': 17,\n 'DONM': 18,\n 'EAFF': 19,\n 'EITT': 20,\n 'FIXN': 21,\n 'FIXV': 22,\n 'JCMP': 23,\n 'JCRG': 24,\n 'JSBR': 25,\n 'NCMN': 26,\n 'NCNM': 27,\n 'NEG': 28,\n 'NLBL': 29,\n 'NONM': 30,\n 'NPRP': 31,\n 'NTTL': 32,\n 'PDMN': 33,\n 'PNTR': 34,\n 'PPRS': 35,\n 'PREL': 36,\n 'PUNC': 37,\n 'RPRE': 38,\n 'VACT': 39,\n 'VATT': 40,\n 'VSTA': 41,\n 'XVAE': 42,\n 'XVAM': 43,\n 'XVBB': 44,\n 'XVBM': 45,\n 'XVMM': 46}"},"metadata":{}}],"execution_count":315},{"cell_type":"code","source":"labels = [i for i in id2label.values()]\nlabels","metadata":{"id":"mQtGN8QQQLME","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:49.583011Z","iopub.execute_input":"2025-01-30T08:07:49.583296Z","iopub.status.idle":"2025-01-30T08:07:49.598754Z","shell.execute_reply.started":"2025-01-30T08:07:49.583268Z","shell.execute_reply":"2025-01-30T08:07:49.597912Z"}},"outputs":[{"execution_count":316,"output_type":"execute_result","data":{"text/plain":"['ADVI',\n 'ADVN',\n 'ADVP',\n 'ADVS',\n 'CFQC',\n 'CLTV',\n 'CMTR',\n 'CMTR@PUNC',\n 'CNIT',\n 'CVBL',\n 'DCNM',\n 'DDAC',\n 'DDAN',\n 'DDAQ',\n 'DDBQ',\n 'DIAC',\n 'DIAQ',\n 'DIBQ',\n 'DONM',\n 'EAFF',\n 'EITT',\n 'FIXN',\n 'FIXV',\n 'JCMP',\n 'JCRG',\n 'JSBR',\n 'NCMN',\n 'NCNM',\n 'NEG',\n 'NLBL',\n 'NONM',\n 'NPRP',\n 'NTTL',\n 'PDMN',\n 'PNTR',\n 'PPRS',\n 'PREL',\n 'PUNC',\n 'RPRE',\n 'VACT',\n 'VATT',\n 'VSTA',\n 'XVAE',\n 'XVAM',\n 'XVBB',\n 'XVBM',\n 'XVMM']"},"metadata":{}}],"execution_count":316},{"cell_type":"markdown","source":"## Load pretrained model","metadata":{"id":"Nu7Z3QH_BJe4"}},{"cell_type":"markdown","source":"Select a pretrained model for fine-tuning to develop a POS Tagger model using the Orchid corpus dataset.\n\n\n\n*   model : `wangchanberta-base-att-spm-uncased`\n*   Don't forget to update the num_labels.\n\nYou’re ready to start training your model now! Load pretrained model with AutoModelForTokenClassification along with the number of expected labels, and the label mappings:\n\n\n","metadata":{"id":"cBYlcF-gDZcF"}},{"cell_type":"markdown","source":"`In the first part, we require you to select the wangchanberta-base-att-spm-uncased.`","metadata":{"id":"6OOu8s-mO_Fw"}},{"cell_type":"code","source":"model_names = [\n    'wangchanberta-base-att-spm-uncased',\n    'wangchanberta-base-wiki-newmm',\n    'wangchanberta-base-wiki-ssg',\n    'wangchanberta-base-wiki-sefr',\n    'wangchanberta-base-wiki-spm',\n]\n\n#@title Choose Pretrained Model\nmodel_name = \"wangchanberta-base-att-spm-uncased\"\n\n#create model\nmodel = AutoModelForTokenClassification.from_pretrained(\n    f\"airesearch/{model_name}\",\n    revision='main',\n    num_labels=47, id2label=id2label, label2id=label2id\n)\n","metadata":{"id":"OOsnubHyDMmA","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:49.599757Z","iopub.execute_input":"2025-01-30T08:07:49.600062Z","iopub.status.idle":"2025-01-30T08:07:50.636700Z","shell.execute_reply.started":"2025-01-30T08:07:49.600033Z","shell.execute_reply":"2025-01-30T08:07:50.635833Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForTokenClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of CamembertForTokenClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":317},{"cell_type":"markdown","source":"### #TODO 2","metadata":{"id":"-H2OExQrCAfX"}},{"cell_type":"markdown","source":"* Configure your training hyperparameters using `**TrainingArguments**`. The only required parameter is is `output_dir`, which determines the directory where your model will be saved. To upload the model to the Hugging Face Hub, set push_to_hub=True (note: you must be logged into Hugging Face for this). During training, the Trainer will compute seqeval metrics at the end of each epoch and store the training checkpoint.\n* Provide the `**Trainer**` with the training arguments, as well as the model, dataset, tokenizer, data collator, and compute_metrics function.\n* Use `**train()**` to fine-tune the model.\n\n\nRead [huggingface's tutorial](https://huggingface.co/docs/transformers/en/tasks/token_classification) for more details.","metadata":{"id":"FBZKrz8nFXyT"}},{"cell_type":"code","source":"# from transformers import TrainingArguments\n\n# training_args = TrainingArguments(\n#     output_dir=\"./models\",           \n#     evaluation_strategy=\"epoch\",      \n#     save_strategy=\"epoch\",            \n#     learning_rate=2e-5,               \n#     per_device_train_batch_size=16,   \n#     per_device_eval_batch_size=16,    \n#     num_train_epochs=3,               \n#     weight_decay=0.01,                \n#     logging_dir=\"./logs\",             \n#     logging_steps=10,                 \n#     report_to=\"none\",                 \n#     save_total_limit=2,               \n#     fp16=True,                        \n#     push_to_hub=True,  # <-- This must be True\n#     hub_model_id=\"pupipatsk/pos-wangchanberta-base-att-spm-uncased\",  # Specify model name\n#     hub_strategy=\"every_save\",  # Upload model after each save\n# )\n\n# from transformers import Trainer\n\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=tokenized_orchid[\"train\"],\n#     eval_dataset=tokenized_orchid[\"test\"],\n#     tokenizer=tokenizer,\n#     data_collator=data_collator,\n#     compute_metrics=compute_metrics\n# )\n\n# trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:50.637622Z","iopub.execute_input":"2025-01-30T08:07:50.637830Z","iopub.status.idle":"2025-01-30T08:07:50.641535Z","shell.execute_reply.started":"2025-01-30T08:07:50.637811Z","shell.execute_reply":"2025-01-30T08:07:50.640567Z"}},"outputs":[],"execution_count":318},{"cell_type":"markdown","source":"# Inference","metadata":{"id":"XiXUG6aakihd"}},{"cell_type":"markdown","source":"With your model fine-tuned, you can now perform inference.","metadata":{"id":"2OPVknoQk4wL"}},{"cell_type":"code","source":"text = \"การประชุมทางวิชาการ ครั้งที่ 1\"","metadata":{"id":"mMyq6I9CkZ1R","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:50.642406Z","iopub.execute_input":"2025-01-30T08:07:50.642646Z","iopub.status.idle":"2025-01-30T08:07:50.656798Z","shell.execute_reply.started":"2025-01-30T08:07:50.642615Z","shell.execute_reply":"2025-01-30T08:07:50.655904Z"}},"outputs":[],"execution_count":319},{"cell_type":"markdown","source":"`In the first part, we require you to select the wangchanberta-base-att-spm-uncased.`","metadata":{"id":"FL5qxD3EPLFt"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load pretrained tokenizer from Hugging Face\n#@title Choose Pretrained Model\nmodel_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n\ntokenizer = Tokenizer(model_name).from_pretrained(model_name)\ninputs = tokenizer(text, return_tensors=\"pt\")","metadata":{"id":"sKgM-EaGfxA4","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:50.657614Z","iopub.execute_input":"2025-01-30T08:07:50.657813Z","iopub.status.idle":"2025-01-30T08:07:51.061317Z","shell.execute_reply.started":"2025-01-30T08:07:50.657795Z","shell.execute_reply":"2025-01-30T08:07:51.060312Z"}},"outputs":[{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \nThe class this function is called from is 'WangchanbertaTokenizer'.\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'CamembertTokenizer'. \nThe class this function is called from is 'WangchanbertaTokenizer'.\n","output_type":"stream"}],"execution_count":320},{"cell_type":"code","source":"inputs","metadata":{"id":"PcRf-Q9nf4-t","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:51.062316Z","iopub.execute_input":"2025-01-30T08:07:51.062544Z","iopub.status.idle":"2025-01-30T08:07:51.068609Z","shell.execute_reply.started":"2025-01-30T08:07:51.062524Z","shell.execute_reply":"2025-01-30T08:07:51.067865Z"}},"outputs":[{"execution_count":321,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[   5,   10,  882, 8222,    8,   10, 1014,    8,   10,   59,    6]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":321},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification\n\n## Load your fine-tuned model from Hugging Face\nmodel = AutoModelForTokenClassification.from_pretrained(\"pupipatsk/pos-wangchanberta-base-att-spm-uncased\") ## your model path from Hugging Face\nwith torch.no_grad():\n    logits = model(**inputs).logits","metadata":{"id":"6ADY5OuqkkHb","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:51.069402Z","iopub.execute_input":"2025-01-30T08:07:51.069604Z","iopub.status.idle":"2025-01-30T08:07:52.353527Z","shell.execute_reply.started":"2025-01-30T08:07:51.069585Z","shell.execute_reply":"2025-01-30T08:07:52.352718Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(checkpoint_file, map_location=\"cpu\")\n","output_type":"stream"}],"execution_count":322},{"cell_type":"code","source":"predictions = torch.argmax(logits, dim=2)\npredicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\npredicted_token_class","metadata":{"id":"AACsd7VZgT1E","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:52.354442Z","iopub.execute_input":"2025-01-30T08:07:52.354786Z","iopub.status.idle":"2025-01-30T08:07:52.361842Z","shell.execute_reply.started":"2025-01-30T08:07:52.354749Z","shell.execute_reply":"2025-01-30T08:07:52.360952Z"}},"outputs":[{"execution_count":323,"output_type":"execute_result","data":{"text/plain":"['PUNC',\n 'PUNC',\n 'VACT',\n 'NCMN',\n 'PUNC',\n 'PUNC',\n 'CFQC',\n 'DONM',\n 'DONM',\n 'DONM',\n 'PUNC']"},"metadata":{}}],"execution_count":323},{"cell_type":"code","source":"id2label","metadata":{"id":"kJKGbf4Rk0c9","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:52.362731Z","iopub.execute_input":"2025-01-30T08:07:52.362953Z","iopub.status.idle":"2025-01-30T08:07:52.377273Z","shell.execute_reply.started":"2025-01-30T08:07:52.362927Z","shell.execute_reply":"2025-01-30T08:07:52.376511Z"}},"outputs":[{"execution_count":324,"output_type":"execute_result","data":{"text/plain":"{0: 'ADVI',\n 1: 'ADVN',\n 2: 'ADVP',\n 3: 'ADVS',\n 4: 'CFQC',\n 5: 'CLTV',\n 6: 'CMTR',\n 7: 'CMTR@PUNC',\n 8: 'CNIT',\n 9: 'CVBL',\n 10: 'DCNM',\n 11: 'DDAC',\n 12: 'DDAN',\n 13: 'DDAQ',\n 14: 'DDBQ',\n 15: 'DIAC',\n 16: 'DIAQ',\n 17: 'DIBQ',\n 18: 'DONM',\n 19: 'EAFF',\n 20: 'EITT',\n 21: 'FIXN',\n 22: 'FIXV',\n 23: 'JCMP',\n 24: 'JCRG',\n 25: 'JSBR',\n 26: 'NCMN',\n 27: 'NCNM',\n 28: 'NEG',\n 29: 'NLBL',\n 30: 'NONM',\n 31: 'NPRP',\n 32: 'NTTL',\n 33: 'PDMN',\n 34: 'PNTR',\n 35: 'PPRS',\n 36: 'PREL',\n 37: 'PUNC',\n 38: 'RPRE',\n 39: 'VACT',\n 40: 'VATT',\n 41: 'VSTA',\n 42: 'XVAE',\n 43: 'XVAM',\n 44: 'XVBB',\n 45: 'XVBM',\n 46: 'XVMM'}"},"metadata":{}}],"execution_count":324},{"cell_type":"code","source":"# Inference\n# ignore special tokens\ntext = 'จะว่าไปแล้วเชิงเทียนของผมก็สวยดีเหมือนกัน'\ninputs = tokenizer(text, return_tensors=\"pt\")\ntokenized_input = tokenizer([text], is_split_into_words=True)\ntokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\nprint('tokens :', tokens)\nwith torch.no_grad():\n    logits = model(**inputs).logits\npredictions = torch.argmax(logits, dim=2)\npredicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\nprint('predict pos :', predicted_token_class)","metadata":{"id":"6OikBqcKTcUY","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:52.378199Z","iopub.execute_input":"2025-01-30T08:07:52.378511Z","iopub.status.idle":"2025-01-30T08:07:52.458482Z","shell.execute_reply.started":"2025-01-30T08:07:52.378474Z","shell.execute_reply":"2025-01-30T08:07:52.457563Z"}},"outputs":[{"name":"stdout","text":"tokens : ['<s>', '▁', 'จะว่าไป', 'แล้ว', 'เชิง', 'เทียน', 'ของ', 'ผมก็', 'สวยดี', 'เหมือนกัน', '</s>']\npredict pos : ['PUNC', 'PUNC', 'JSBR', 'ADVS', 'NCMN', 'NCMN', 'RPRE', 'PPRS', 'VATT', 'ADVN', 'PUNC']\n","output_type":"stream"}],"execution_count":325},{"cell_type":"markdown","source":"**Evaluate model :**\n\nThe output from the model is a softmax over classes. We choose the maximum class as the answer for evaluation. Again, we will ignore the -100 labels.","metadata":{"id":"D-TtwYTCXs2O"}},{"cell_type":"code","source":"import pandas as pd\nfrom IPython.display import display\n\ndef evaluation_report(y_true, y_pred, get_only_acc=False):\n    # retrieve all tags in y_true\n    tag_set = set()\n    for sent in y_true:\n        for tag in sent:\n            tag_set.add(tag)\n    for sent in y_pred:\n        for tag in sent:\n            tag_set.add(tag)\n    tag_list = sorted(list(tag_set))\n\n    # count correct points\n    tag_info = dict()\n    for tag in tag_list:\n        tag_info[tag] = {'correct_tagged': 0, 'y_true': 0, 'y_pred': 0}\n\n    all_correct = 0\n    all_count = sum([len(sent) for sent in y_true])\n    speacial_tag = 0\n    for sent_true, sent_pred in zip(y_true, y_pred):\n        for tag_true, tag_pred in zip(sent_true, sent_pred):\n            # pass special token\n            if tag_true == -100 :\n                speacial_tag += 1\n                pass\n            if tag_true == tag_pred:\n                tag_info[tag_true]['correct_tagged'] += 1\n                all_correct += 1\n            tag_info[tag_true]['y_true'] += 1\n            tag_info[tag_pred]['y_pred'] += 1\n    print('speacial_tag :',speacial_tag) # delete number of special token from all_count\n    accuracy = (all_correct / (all_count-speacial_tag))\n\n    # get only accuracy for testing\n    if get_only_acc:\n      return accuracy\n\n    accuracy *= 100\n\n\n    # summarize and make evaluation result\n    eval_list = list()\n    for tag in tag_list:\n        eval_result = dict()\n        eval_result['tag'] = tag\n        eval_result['correct_count'] = tag_info[tag]['correct_tagged']\n        precision = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_pred'])*100 if tag_info[tag]['y_pred'] else '-'\n        recall = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_true'])*100 if (tag_info[tag]['y_true'] > 0) else 0\n        eval_result['precision'] = precision\n        eval_result['recall'] = recall\n        eval_result['f1_score'] = (2*precision*recall)/(precision+recall) if (type(precision) is float and recall > 0) else '-'\n\n        eval_list.append(eval_result)\n\n    eval_list.append({'tag': 'accuracy=%.2f' % accuracy, 'correct_count': '', 'precision': '', 'recall': '', 'f1_score': ''})\n\n    df = pd.DataFrame.from_dict(eval_list)\n    df = df[['tag', 'precision', 'recall', 'f1_score', 'correct_count']]\n\n    display(df)\n","metadata":{"id":"Hm_adLrcXyOe","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:52.459411Z","iopub.execute_input":"2025-01-30T08:07:52.459804Z","iopub.status.idle":"2025-01-30T08:07:52.471679Z","shell.execute_reply.started":"2025-01-30T08:07:52.459767Z","shell.execute_reply":"2025-01-30T08:07:52.470628Z"}},"outputs":[],"execution_count":326},{"cell_type":"code","source":"# prepare test set\ntest_data = tokenized_orchid[\"test\"]","metadata":{"id":"x7Lryj2aYCdn","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:52.472608Z","iopub.execute_input":"2025-01-30T08:07:52.472850Z","iopub.status.idle":"2025-01-30T08:07:52.488577Z","shell.execute_reply.started":"2025-01-30T08:07:52.472818Z","shell.execute_reply":"2025-01-30T08:07:52.487946Z"}},"outputs":[],"execution_count":327},{"cell_type":"code","source":"# labels for test set\ny_test = []\nfor inputs in test_data:\n  y_test.append(inputs['labels'])","metadata":{"id":"FGXWNs9RY2Zv","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:52.489352Z","iopub.execute_input":"2025-01-30T08:07:52.489601Z","iopub.status.idle":"2025-01-30T08:07:53.177383Z","shell.execute_reply.started":"2025-01-30T08:07:52.489578Z","shell.execute_reply":"2025-01-30T08:07:53.176663Z"}},"outputs":[],"execution_count":328},{"cell_type":"code","source":"# y_pred = []\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n# for inputs in tqdm(test_data):\n#     text = inputs['sentence']\n#     inputs = tokenizer(text, return_tensors=\"pt\")\n#     with torch.no_grad():\n#         pred = model(**inputs).logits\n#         predictions = torch.argmax(pred, dim=2)\n#         # Append padded predictions to y_pred\n#         y_pred.append(predictions.tolist()[0])\n\n# # save y_pred to local\n# import json\n# with open('y_pred-pos-wangchanberta-base-att-spm-uncased.json', 'w') as f:\n#     json.dump(y_pred, f)","metadata":{"id":"U6__09qnX1DW","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:53.178190Z","iopub.execute_input":"2025-01-30T08:07:53.178481Z","iopub.status.idle":"2025-01-30T08:07:53.182329Z","shell.execute_reply.started":"2025-01-30T08:07:53.178452Z","shell.execute_reply":"2025-01-30T08:07:53.181369Z"}},"outputs":[],"execution_count":329},{"cell_type":"code","source":"# load y_pred from local\nwith open('y_pred-pos-wangchanberta-base-att-spm-uncased.json', 'r') as f:\n    y_pred = json.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:53.183074Z","iopub.execute_input":"2025-01-30T08:07:53.183283Z","iopub.status.idle":"2025-01-30T08:07:53.211402Z","shell.execute_reply.started":"2025-01-30T08:07:53.183264Z","shell.execute_reply":"2025-01-30T08:07:53.210777Z"}},"outputs":[],"execution_count":330},{"cell_type":"code","source":"# check our prediction with label\n# -100 is special tokens : [<s>, </s>, _]\nprint(y_pred[0])\nprint(y_test[0])","metadata":{"id":"yX0BhOe7g3eh","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:53.212151Z","iopub.execute_input":"2025-01-30T08:07:53.212337Z","iopub.status.idle":"2025-01-30T08:07:53.216369Z","shell.execute_reply.started":"2025-01-30T08:07:53.212320Z","shell.execute_reply":"2025-01-30T08:07:53.215698Z"}},"outputs":[{"name":"stdout","text":"[37, 29, 39, 26, 26, 26, 37, 37, 26, 26, 26, 41, 37, 37, 26, 26, 39, 26, 37]\n[-100, 29, 39, 26, 26, 26, 37, -100, 26, 26, 26, 41, 37, -100, 26, 26, 39, 26, -100]\n","output_type":"stream"}],"execution_count":331},{"cell_type":"code","source":"evaluation_report(y_test, y_pred)","metadata":{"id":"Na0L6NUdgLaE","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:53.217215Z","iopub.execute_input":"2025-01-30T08:07:53.217492Z","iopub.status.idle":"2025-01-30T08:07:53.291426Z","shell.execute_reply.started":"2025-01-30T08:07:53.217464Z","shell.execute_reply":"2025-01-30T08:07:53.290685Z"}},"outputs":[{"name":"stdout","text":"speacial_tag : 21042\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"               tag  precision     recall   f1_score correct_count\n0             -100          -        0.0          -             0\n1                0       50.0       25.0  33.333333             4\n2                1  76.384535  72.376238  74.326385           731\n3                2       50.0   4.310345   7.936508             5\n4                3       57.5  39.655172  46.938776            23\n5                4  88.333333  94.642857   91.37931            53\n6                5  65.317919  65.317919  65.317919           113\n7                6   93.39498    98.6053  95.929444           707\n8                7          -        0.0          -             0\n9                8  62.711864  75.126904  68.360277           296\n10              10  91.772772   90.26975  91.015056           937\n11              11  94.660194  85.526316  89.861751           390\n12              12   54.83871  81.730769  65.637066            85\n13              13       50.0  27.272727  35.294118             3\n14              14  68.253968  83.495146   75.10917            86\n15              15  89.473684  93.865031  91.616766           306\n16              16          -        0.0          -             0\n17              17  97.142857  95.967742  96.551724           238\n18              18   69.70091  98.168498  81.520913          1072\n19              19          -        0.0          -             0\n20              20      100.0  76.470588  86.666667            13\n21              21  93.619792  95.802798  94.698716          1438\n22              22   72.39819  95.238095  82.262211           160\n23              23  83.636364  96.842105  89.756098            92\n24              24  88.725986  97.391787  92.857143          1755\n25              25  84.262149  86.538462  85.385137          1890\n26              26  89.422783  93.336315  91.337647         29218\n27              27  82.302772  62.764228  71.217712           386\n28              28  89.256198  93.103448  91.139241           108\n29              29  88.811189  98.755832  93.519882           635\n30              31  71.401274  86.430224  78.200209          2242\n31              32       87.5      100.0  93.333333           147\n32              33  65.116279  60.869565  62.921348            56\n33              34       64.0       64.0       64.0            16\n34              35   78.26087       60.0  67.924528            54\n35              36   94.34365  87.698413  90.899743           884\n36              37  40.123377  98.477966  57.016309         12358\n37              38   92.34181  91.395961  91.866451          3123\n38              39  88.809369  91.267719  90.021764          6825\n39              40  69.061708  71.666667  70.340077           817\n40              41  80.286468  78.743068  79.507279          2130\n41              42  85.553279  93.609865  89.400428           835\n42              43  95.211268  99.411765  97.266187           676\n43              45  90.927022  96.645702  93.699187           461\n44              46  96.065574   91.84953  93.910256           293\n45  accuracy=91.89                                               ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tag</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1_score</th>\n      <th>correct_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-100</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>50.0</td>\n      <td>25.0</td>\n      <td>33.333333</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>76.384535</td>\n      <td>72.376238</td>\n      <td>74.326385</td>\n      <td>731</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>50.0</td>\n      <td>4.310345</td>\n      <td>7.936508</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>57.5</td>\n      <td>39.655172</td>\n      <td>46.938776</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4</td>\n      <td>88.333333</td>\n      <td>94.642857</td>\n      <td>91.37931</td>\n      <td>53</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5</td>\n      <td>65.317919</td>\n      <td>65.317919</td>\n      <td>65.317919</td>\n      <td>113</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>6</td>\n      <td>93.39498</td>\n      <td>98.6053</td>\n      <td>95.929444</td>\n      <td>707</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>7</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8</td>\n      <td>62.711864</td>\n      <td>75.126904</td>\n      <td>68.360277</td>\n      <td>296</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>91.772772</td>\n      <td>90.26975</td>\n      <td>91.015056</td>\n      <td>937</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>94.660194</td>\n      <td>85.526316</td>\n      <td>89.861751</td>\n      <td>390</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>54.83871</td>\n      <td>81.730769</td>\n      <td>65.637066</td>\n      <td>85</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>50.0</td>\n      <td>27.272727</td>\n      <td>35.294118</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>68.253968</td>\n      <td>83.495146</td>\n      <td>75.10917</td>\n      <td>86</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>89.473684</td>\n      <td>93.865031</td>\n      <td>91.616766</td>\n      <td>306</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>97.142857</td>\n      <td>95.967742</td>\n      <td>96.551724</td>\n      <td>238</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>69.70091</td>\n      <td>98.168498</td>\n      <td>81.520913</td>\n      <td>1072</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>100.0</td>\n      <td>76.470588</td>\n      <td>86.666667</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>93.619792</td>\n      <td>95.802798</td>\n      <td>94.698716</td>\n      <td>1438</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>72.39819</td>\n      <td>95.238095</td>\n      <td>82.262211</td>\n      <td>160</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>23</td>\n      <td>83.636364</td>\n      <td>96.842105</td>\n      <td>89.756098</td>\n      <td>92</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>24</td>\n      <td>88.725986</td>\n      <td>97.391787</td>\n      <td>92.857143</td>\n      <td>1755</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>25</td>\n      <td>84.262149</td>\n      <td>86.538462</td>\n      <td>85.385137</td>\n      <td>1890</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>26</td>\n      <td>89.422783</td>\n      <td>93.336315</td>\n      <td>91.337647</td>\n      <td>29218</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>27</td>\n      <td>82.302772</td>\n      <td>62.764228</td>\n      <td>71.217712</td>\n      <td>386</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>28</td>\n      <td>89.256198</td>\n      <td>93.103448</td>\n      <td>91.139241</td>\n      <td>108</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>29</td>\n      <td>88.811189</td>\n      <td>98.755832</td>\n      <td>93.519882</td>\n      <td>635</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>31</td>\n      <td>71.401274</td>\n      <td>86.430224</td>\n      <td>78.200209</td>\n      <td>2242</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>32</td>\n      <td>87.5</td>\n      <td>100.0</td>\n      <td>93.333333</td>\n      <td>147</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>33</td>\n      <td>65.116279</td>\n      <td>60.869565</td>\n      <td>62.921348</td>\n      <td>56</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>34</td>\n      <td>64.0</td>\n      <td>64.0</td>\n      <td>64.0</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>35</td>\n      <td>78.26087</td>\n      <td>60.0</td>\n      <td>67.924528</td>\n      <td>54</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>36</td>\n      <td>94.34365</td>\n      <td>87.698413</td>\n      <td>90.899743</td>\n      <td>884</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>37</td>\n      <td>40.123377</td>\n      <td>98.477966</td>\n      <td>57.016309</td>\n      <td>12358</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>38</td>\n      <td>92.34181</td>\n      <td>91.395961</td>\n      <td>91.866451</td>\n      <td>3123</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>39</td>\n      <td>88.809369</td>\n      <td>91.267719</td>\n      <td>90.021764</td>\n      <td>6825</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>40</td>\n      <td>69.061708</td>\n      <td>71.666667</td>\n      <td>70.340077</td>\n      <td>817</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>41</td>\n      <td>80.286468</td>\n      <td>78.743068</td>\n      <td>79.507279</td>\n      <td>2130</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>42</td>\n      <td>85.553279</td>\n      <td>93.609865</td>\n      <td>89.400428</td>\n      <td>835</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>43</td>\n      <td>95.211268</td>\n      <td>99.411765</td>\n      <td>97.266187</td>\n      <td>676</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>45</td>\n      <td>90.927022</td>\n      <td>96.645702</td>\n      <td>93.699187</td>\n      <td>461</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>46</td>\n      <td>96.065574</td>\n      <td>91.84953</td>\n      <td>93.910256</td>\n      <td>293</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>accuracy=91.89</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":332},{"cell_type":"markdown","source":"# Other Pretrained model","metadata":{"id":"F0BZzVgjm4l_"}},{"cell_type":"markdown","source":"In this section, we will experiment by fine-tuning other pretrained models, such as airesearch/wangchanberta-base-wiki-newmm, to see how about their performance.\n\nSince each model uses a different word-tokenization method.\nfor example, **airesearch/wangchanberta-base-wiki-newmm uses newmm**,\nwhile **airesearch/wangchanberta-base-att-spm-uncased uses SentencePiece**.\nplease try fine-tuning and compare the performance of these models.","metadata":{"id":"ssZdIst48AwH"}},{"cell_type":"markdown","source":"### #TODO 3","metadata":{"id":"GUCWuhrl91mj"}},{"cell_type":"code","source":"model_names = [\n    'airesearch/wangchanberta-base-att-spm-uncased',\n    'airesearch/wangchanberta-base-wiki-newmm',\n    'airesearch/wangchanberta-base-wiki-ssg',\n    'airesearch/wangchanberta-base-wiki-sefr',\n    'airesearch/wangchanberta-base-wiki-spm',\n]\n\n#@title Choose Pretrained Model\nmodel_name = \"airesearch/wangchanberta-base-wiki-newmm\" #@param [\"airesearch/wangchanberta-base-att-spm-uncased\", \"airesearch/wangchanberta-base-wiki-newmm\", \"airesearch/wangchanberta-base-wiki-syllable\", \"airesearch/wangchanberta-base-wiki-sefr\", \"airesearch/wangchanberta-base-wiki-spm\"]\n\n#create tokenizer\ntokenizer = Tokenizer(model_name).from_pretrained(\n                f'{model_name}',\n                revision='main',\n                model_max_length=416,)\n","metadata":{"id":"9etT-A_anBfi","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:53.292335Z","iopub.execute_input":"2025-01-30T08:07:53.292576Z","iopub.status.idle":"2025-01-30T08:07:53.828194Z","shell.execute_reply.started":"2025-01-30T08:07:53.292557Z","shell.execute_reply":"2025-01-30T08:07:53.827431Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \nThe class this function is called from is 'ThaiWordsNewmmTokenizer'.\nThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \nThe class this function is called from is 'ThaiWordsNewmmTokenizer'.\n","output_type":"stream"}],"execution_count":333},{"cell_type":"code","source":"example = orchidl[\"train\"][1899]\nprint('sentence :', example[\"sentence\"])\ntokenized_input = tokenizer([example[\"sentence\"]], is_split_into_words=True)\ntokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\nprint('tokens :',tokens)\nprint('label tokens :', example[\"label_tokens\"])","metadata":{"id":"LFXdV8V5nGk2","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:07:53.829291Z","iopub.execute_input":"2025-01-30T08:07:53.829627Z","iopub.status.idle":"2025-01-30T08:07:53.837289Z","shell.execute_reply.started":"2025-01-30T08:07:53.829595Z","shell.execute_reply":"2025-01-30T08:07:53.836557Z"}},"outputs":[{"name":"stdout","text":"sentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\ntokens : ['<s>', 'โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '<unk>', '<_>', 'transfer', '<_>', 'dictionary', ')', '</s>']\nlabel tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\n","output_type":"stream"}],"execution_count":334},{"cell_type":"markdown","source":"It's the same problem as above.\n\n`**Warning: Can we use same function as above ?**`\n\n`**Warning: Please beware of <unk>, an unknown word token.**`\n\n`**Warning: Please be careful of \" ำ \", the 'am' vowel. WangchanBERTa's internal preprocessing replaces all \" ำ \" to 'ํ' and 'า'**`","metadata":{"id":"I7QMwcbK5Ibj"}},{"cell_type":"code","source":"def majority_vote_pos(examples):\n    \"\"\"\n    # TO DO: Since the tokens from the output of the pretrained tokenizer\n    # do not match the tokens in the label tokens of the dataset,\n    # the task is to create a function to determine the POS tags of the tokens generated by the pretrained tokenizer.\n    # This should be done by referencing the POS tags in the label tokens. If a token partially overlaps with others,\n    # the POS tag from the segment with the greater number of characters should be assigned.\n    #\n    # Example :\n    # \"การประชุม\" (9 chars) is formed from \"การ\" (3 chars) + \"ประชุม\" (6 chars).\n    # \"การ\" has a POS tag of 21,\n    # and \"ประชุม\" has a POS tag of 39.\n    # Therefore, the POS tag for \"การประชุม\" is 39,\n    # as \"การประชุม\" is derived more from the \"ประชุม\" part than from the \"การ\" part.\n    #\n    # 'ทางวิชาการ' (10 chars) is formed from 'ทาง' (3 chars) + 'วิชาการ' (7 chars)\n    # \"ทาง\" has a POS tag of 26,\n    # and \"วิชาการ\" has a POS tag of 2.\n    # Therefore, the POS tag for \"ทางวิชาการ\" is 2,\n    # as \"ทางวิชาการ\" is derived more from the \"ทาง\" part than from the \"วิชาการ\" part.\n    \"\"\"\n    # TODO: FILL CODE HERE\n    tokenized_inputs = tokenizer([examples[\"sentence\"]], is_split_into_words=True)\n    new_tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"])\n    label_tokens = examples[\"label_tokens\"]\n    pos_tags = examples[\"pos_tags\"]\n    \n    new_pos_result = []\n    label_idx = 0  # Current index in label_tokens\n    char_idx = 0   # Current character index within the current label token\n\n    for token in new_tokens:\n        if token in [\"<s>\", \"</s>\", \"_\", \"<unk>\"]:\n            new_pos_result.append(-100)\n            continue\n        \n        token = token.replace('ํา', 'ำ').replace(\"<_>\", \" \")\n        if token.startswith(\"▁\"):\n            token = token[1:]\n        \n        if not token:\n            new_pos_result.append(-100)\n            continue\n        \n        pos_counts = {}\n        accumulated = \"\"\n        token_length = len(token)\n        prev_accumulated_len = -1  # Track previous length to detect no progress\n\n        while accumulated != token and label_idx < len(label_tokens):\n            current_label = label_tokens[label_idx]\n            remaining_in_label = current_label[char_idx:] if char_idx < len(current_label) else ''\n            max_overlap = token_length - len(accumulated)\n            overlap = min(len(remaining_in_label), max_overlap)\n            \n            # Check if no progress is made\n            if overlap == 0 and len(accumulated) == prev_accumulated_len:\n                break  # Exit loop to avoid infinite iteration\n            prev_accumulated_len = len(accumulated)\n            \n            accumulated += remaining_in_label[:overlap]\n            for _ in range(overlap):\n                pos_tag = pos_tags[label_idx]\n                pos_counts[pos_tag] = pos_counts.get(pos_tag, 0) + 1\n            char_idx += overlap\n            \n            if char_idx >= len(current_label):\n                label_idx += 1\n                char_idx = 0\n        \n        best_pos = max(pos_counts, key=pos_counts.get) if pos_counts else -100\n        new_pos_result.append(best_pos)\n    \n    tokenized_inputs['tokens'] = new_tokens\n    tokenized_inputs['labels'] = new_pos_result\n    return tokenized_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T10:33:07.367170Z","iopub.execute_input":"2025-01-30T10:33:07.367461Z","iopub.status.idle":"2025-01-30T10:33:07.376835Z","shell.execute_reply.started":"2025-01-30T10:33:07.367438Z","shell.execute_reply":"2025-01-30T10:33:07.375887Z"}},"outputs":[],"execution_count":384},{"cell_type":"code","source":"tokenized_orchid = orchidl.map(majority_vote_pos)","metadata":{"id":"O5n4veYxo3rR","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T10:33:08.188689Z","iopub.execute_input":"2025-01-30T10:33:08.189089Z","iopub.status.idle":"2025-01-30T10:33:28.709466Z","shell.execute_reply.started":"2025-01-30T10:33:08.189059Z","shell.execute_reply":"2025-01-30T10:33:28.708583Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/18500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c28d3bdda70f4259a6da5a194314b495"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4625 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d385342e528e42ccb2545d8457ff3a4f"}},"metadata":{}}],"execution_count":385},{"cell_type":"code","source":"# hard test case\nexample = tokenized_orchid[\"train\"][1899]\nfor i in example :\n    print(i, \":\", example[i])","metadata":{"id":"ashRh72szWiM","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T10:33:28.711145Z","iopub.execute_input":"2025-01-30T10:33:28.711375Z","iopub.status.idle":"2025-01-30T10:33:28.721659Z","shell.execute_reply.started":"2025-01-30T10:33:28.711355Z","shell.execute_reply":"2025-01-30T10:33:28.720834Z"}},"outputs":[{"name":"stdout","text":"id : 1899\nlabel_tokens : ['โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', ' ', '(', 'bilingual transfer dictionary', ')']\npos_tags : [25, 39, 38, 26, 26, 5, 37, 37, 26, 37]\nsentence : โดยพิจารณาจากพจนานุกรมภาษาคู่ (bilingual transfer dictionary)\ninput_ids : [0, 80, 3973, 45, 12252, 3496, 592, 5, 3, 5, 30055, 5, 63190, 178, 2]\ntoken_type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nattention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\ntokens : ['<s>', 'โดย', 'พิจารณา', 'จาก', 'พจนานุกรม', 'ภาษา', 'คู่', '<_>', '<unk>', '<_>', 'transfer', '<_>', 'dictionary', ')', '</s>']\nlabels : [-100, 25, 39, 38, 26, 26, 5, 37, -100, 37, 26, 26, 26, 26, -100]\n","output_type":"stream"}],"execution_count":386},{"cell_type":"code","source":"model_names = [\n    'wangchanberta-base-att-spm-uncased',\n    'wangchanberta-base-wiki-newmm',\n    'wangchanberta-base-wiki-ssg',\n    'wangchanberta-base-wiki-sefr',\n    'wangchanberta-base-wiki-spm',\n]\n\n#@title Choose Pretrained Model\nmodel_name = \"wangchanberta-base-wiki-newmm\" #@param [\"wangchanberta-base-att-spm-uncased\", \"wangchanberta-base-wiki-newmm\", \"wangchanberta-base-wiki-syllable\", \"wangchanberta-base-wiki-sefr\", \"wangchanberta-base-wiki-spm\"]\n\n#create model\nmodel = AutoModelForTokenClassification.from_pretrained(\n    f\"airesearch/{model_name}\",\n    revision='main',\n    num_labels=47, id2label=id2label, label2id=label2id\n)\n","metadata":{"id":"7AL0Vqbv7cfb","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:45:14.867811Z","iopub.execute_input":"2025-01-30T09:45:14.868048Z","iopub.status.idle":"2025-01-30T09:45:16.824071Z","shell.execute_reply.started":"2025-01-30T09:45:14.868027Z","shell.execute_reply":"2025-01-30T09:45:16.823077Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at airesearch/wangchanberta-base-wiki-newmm were not used when initializing RobertaForTokenClassification: ['lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForTokenClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-wiki-newmm and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":369},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)","metadata":{"id":"pWTh0bMfP70g","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:45:16.825882Z","iopub.execute_input":"2025-01-30T09:45:16.826178Z","iopub.status.idle":"2025-01-30T09:45:16.829982Z","shell.execute_reply.started":"2025-01-30T09:45:16.826156Z","shell.execute_reply":"2025-01-30T09:45:16.829103Z"}},"outputs":[],"execution_count":370},{"cell_type":"markdown","source":"### #TODO 4","metadata":{"id":"QkhYDS4q7oxK"}},{"cell_type":"markdown","source":"Fine-tuning other pretrained model with our orchid corpus.","metadata":{"id":"XVdITM5E7tQ5"}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./models-pos-wangchanberta-base-wiki-newmm\",           \n    evaluation_strategy=\"epoch\",      \n    save_strategy=\"epoch\",            \n    learning_rate=2e-5,               \n    per_device_train_batch_size=16,   \n    per_device_eval_batch_size=16,    \n    num_train_epochs=3,               \n    weight_decay=0.01,                \n    logging_dir=\"./logs\",             \n    logging_steps=10,                 \n    report_to=\"none\",                 \n    save_total_limit=2,               \n    fp16=True,                        \n    push_to_hub=True,\n    hub_model_id=\"pupipatsk/pos-wangchanberta-base-wiki-newmm\",  # Specify model name\n    hub_strategy=\"every_save\",  # Upload model after each save\n)\n\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_orchid[\"train\"],\n    eval_dataset=tokenized_orchid[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()","metadata":{"id":"hBHlUamr7syk","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:45:32.734664Z","iopub.execute_input":"2025-01-30T09:45:32.735043Z","iopub.status.idle":"2025-01-30T09:56:59.570227Z","shell.execute_reply.started":"2025-01-30T09:45:32.734988Z","shell.execute_reply":"2025-01-30T09:56:59.569025Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\nFor more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n  warnings.warn(warning_message, FutureWarning)\nCloning https://huggingface.co/pupipatsk/pos-wangchanberta-base-wiki-newmm into local empty directory.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1737' max='1737' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1737/1737 11:20, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.672700</td>\n      <td>0.564963</td>\n      <td>0.779672</td>\n      <td>0.744131</td>\n      <td>0.761487</td>\n      <td>0.824691</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.529700</td>\n      <td>0.527385</td>\n      <td>0.767056</td>\n      <td>0.768697</td>\n      <td>0.767875</td>\n      <td>0.834717</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.488600</td>\n      <td>0.515150</td>\n      <td>0.767087</td>\n      <td>0.766050</td>\n      <td>0.766568</td>\n      <td>0.835976</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":372,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1737, training_loss=0.6481462119294091, metrics={'train_runtime': 681.0439, 'train_samples_per_second': 81.493, 'train_steps_per_second': 2.55, 'total_flos': 1371637012822776.0, 'train_loss': 0.6481462119294091, 'epoch': 3.0})"},"metadata":{}}],"execution_count":372},{"cell_type":"code","source":"######## EVALUATE YOUR MODEL ########\ntest_data = tokenized_orchid[\"test\"]\ny_test = [inputs['labels'] for inputs in test_data]\n\nmodel = AutoModelForTokenClassification.from_pretrained(\"pupipatsk/pos-wangchanberta-base-wiki-newmm\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\ny_pred = []\n# run inference\nfor inputs in tqdm(test_data):\n    text = inputs['sentence']\n    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        pred = model(**inputs).logits  # Model prediction\n        predictions = torch.argmax(pred, dim=2)  # Convert logits to labels\n        y_pred.append(predictions.cpu().tolist()[0])\n\n# save y_pred to local\nwith open('y_pred-pos-wangchanberta-base-wiki-newmm.json', 'w') as f:\n    json.dump(y_pred, f)\n\n# load y_pred from local\nwith open('y_pred-pos-wangchanberta-base-wiki-newmm.json', 'r') as f:\n    y_pred = json.load(f)\n\nprint(y_pred[0])\nprint(y_test[0])\n\nevaluation_report(y_test, y_pred)","metadata":{"id":"KIdxvgUGCVsm","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T10:13:34.528305Z","iopub.execute_input":"2025-01-30T10:13:34.528669Z","iopub.status.idle":"2025-01-30T10:14:46.002231Z","shell.execute_reply.started":"2025-01-30T10:13:34.528643Z","shell.execute_reply":"2025-01-30T10:14:46.001338Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f287fe5ab634af0ac5ca7edfd8a8eda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/643M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca95e1edad864730bbe97f7ac524f6dd"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(checkpoint_file, map_location=\"cpu\")\n100%|██████████| 4625/4625 [00:52<00:00, 88.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"[26, 29, 37, 21, 39, 26, 26, 37, 26, 26, 41, 37, 26, 39, 39, 26, 37]\n[-100, 29, 37, 21, 39, 26, 26, 37, 26, 26, 41, 37, 26, 39, 39, 26, -100]\nspeacial_tag : 11485\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"               tag  precision     recall   f1_score correct_count\n0             -100          -        0.0          -             0\n1                0  22.222222  26.666667  24.242424             4\n2                1  71.910112  60.606061  65.775951           640\n3                2       30.0   2.678571   4.918033             3\n4                3  57.692308  26.315789  36.144578            15\n5                4  83.636364  75.409836  79.310345            46\n6                5   70.16129     54.375  61.267606            87\n7                6  73.298429  62.921348  67.714631           280\n8                7        0.0        0.0          -             0\n9                8  63.207547  62.229102  62.714509           201\n10              10  74.610778  74.969916  74.789916           623\n11              11   91.59292  82.142857  86.610879           414\n12              12  49.285714   65.09434  56.097561            69\n13              13          -        0.0          -             0\n14              14  79.230769  72.027972  75.457875           103\n15              15  90.604027  85.173502  87.804878           270\n16              16          -        0.0          -             0\n17              17  84.722222  89.705882  87.142857           244\n18              18  83.923941  74.235474  78.782961           971\n19              19          -        0.0          -             0\n20              20  81.818182       60.0  69.230769             9\n21              21  87.617766  85.507881  86.549967          1953\n22              22  74.675325  81.560284  77.966102           115\n23              23  82.608696  73.786408  77.948718            76\n24              24  92.113734   88.09646   90.06032          1717\n25              25  85.757576  82.576073  84.136759          1981\n26              26   71.50413  87.867603  78.845808         21901\n27              27  60.616438  38.064516   46.76354           177\n28              28   91.41791   91.41791   91.41791           245\n29              29  77.934936  88.585209  82.919488           551\n30              31  63.113093  76.286134  69.077196          1557\n31              32  83.333333  77.777778   80.45977            70\n32              33   59.52381  60.240964   59.88024            50\n33              34  68.181818  68.181818  68.181818            15\n34              35  89.130435  66.666667   76.27907            82\n35              36  92.911392  81.314623  86.727058          1101\n36              37  60.502981  85.346129  70.808731          7915\n37              38  91.606399  84.683955  88.009264          4180\n38              39   65.52838  86.606489  74.607261          7100\n39              40  65.050167  58.805745  61.770544           778\n40              41  64.250582  74.467447  68.982768          2482\n41              42  86.284289  84.390244  85.326757          1038\n42              43  91.119221   92.69802   91.90184           749\n43              45  94.230769  95.516569  94.869313           980\n44              46  88.950276  90.449438  89.693593           322\n45  accuracy=83.60                                               ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tag</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1_score</th>\n      <th>correct_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-100</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>22.222222</td>\n      <td>26.666667</td>\n      <td>24.242424</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>71.910112</td>\n      <td>60.606061</td>\n      <td>65.775951</td>\n      <td>640</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>30.0</td>\n      <td>2.678571</td>\n      <td>4.918033</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>57.692308</td>\n      <td>26.315789</td>\n      <td>36.144578</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4</td>\n      <td>83.636364</td>\n      <td>75.409836</td>\n      <td>79.310345</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5</td>\n      <td>70.16129</td>\n      <td>54.375</td>\n      <td>61.267606</td>\n      <td>87</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>6</td>\n      <td>73.298429</td>\n      <td>62.921348</td>\n      <td>67.714631</td>\n      <td>280</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>7</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8</td>\n      <td>63.207547</td>\n      <td>62.229102</td>\n      <td>62.714509</td>\n      <td>201</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>74.610778</td>\n      <td>74.969916</td>\n      <td>74.789916</td>\n      <td>623</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>91.59292</td>\n      <td>82.142857</td>\n      <td>86.610879</td>\n      <td>414</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>49.285714</td>\n      <td>65.09434</td>\n      <td>56.097561</td>\n      <td>69</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>79.230769</td>\n      <td>72.027972</td>\n      <td>75.457875</td>\n      <td>103</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>90.604027</td>\n      <td>85.173502</td>\n      <td>87.804878</td>\n      <td>270</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>84.722222</td>\n      <td>89.705882</td>\n      <td>87.142857</td>\n      <td>244</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>83.923941</td>\n      <td>74.235474</td>\n      <td>78.782961</td>\n      <td>971</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>-</td>\n      <td>0.0</td>\n      <td>-</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>81.818182</td>\n      <td>60.0</td>\n      <td>69.230769</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>87.617766</td>\n      <td>85.507881</td>\n      <td>86.549967</td>\n      <td>1953</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>74.675325</td>\n      <td>81.560284</td>\n      <td>77.966102</td>\n      <td>115</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>23</td>\n      <td>82.608696</td>\n      <td>73.786408</td>\n      <td>77.948718</td>\n      <td>76</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>24</td>\n      <td>92.113734</td>\n      <td>88.09646</td>\n      <td>90.06032</td>\n      <td>1717</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>25</td>\n      <td>85.757576</td>\n      <td>82.576073</td>\n      <td>84.136759</td>\n      <td>1981</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>26</td>\n      <td>71.50413</td>\n      <td>87.867603</td>\n      <td>78.845808</td>\n      <td>21901</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>27</td>\n      <td>60.616438</td>\n      <td>38.064516</td>\n      <td>46.76354</td>\n      <td>177</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>28</td>\n      <td>91.41791</td>\n      <td>91.41791</td>\n      <td>91.41791</td>\n      <td>245</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>29</td>\n      <td>77.934936</td>\n      <td>88.585209</td>\n      <td>82.919488</td>\n      <td>551</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>31</td>\n      <td>63.113093</td>\n      <td>76.286134</td>\n      <td>69.077196</td>\n      <td>1557</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>32</td>\n      <td>83.333333</td>\n      <td>77.777778</td>\n      <td>80.45977</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>33</td>\n      <td>59.52381</td>\n      <td>60.240964</td>\n      <td>59.88024</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>34</td>\n      <td>68.181818</td>\n      <td>68.181818</td>\n      <td>68.181818</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>35</td>\n      <td>89.130435</td>\n      <td>66.666667</td>\n      <td>76.27907</td>\n      <td>82</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>36</td>\n      <td>92.911392</td>\n      <td>81.314623</td>\n      <td>86.727058</td>\n      <td>1101</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>37</td>\n      <td>60.502981</td>\n      <td>85.346129</td>\n      <td>70.808731</td>\n      <td>7915</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>38</td>\n      <td>91.606399</td>\n      <td>84.683955</td>\n      <td>88.009264</td>\n      <td>4180</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>39</td>\n      <td>65.52838</td>\n      <td>86.606489</td>\n      <td>74.607261</td>\n      <td>7100</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>40</td>\n      <td>65.050167</td>\n      <td>58.805745</td>\n      <td>61.770544</td>\n      <td>778</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>41</td>\n      <td>64.250582</td>\n      <td>74.467447</td>\n      <td>68.982768</td>\n      <td>2482</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>42</td>\n      <td>86.284289</td>\n      <td>84.390244</td>\n      <td>85.326757</td>\n      <td>1038</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>43</td>\n      <td>91.119221</td>\n      <td>92.69802</td>\n      <td>91.90184</td>\n      <td>749</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>45</td>\n      <td>94.230769</td>\n      <td>95.516569</td>\n      <td>94.869313</td>\n      <td>980</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>46</td>\n      <td>88.950276</td>\n      <td>90.449438</td>\n      <td>89.693593</td>\n      <td>322</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>accuracy=83.60</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":381},{"cell_type":"markdown","source":"### #TODO 5\n\nCompare the results between both models. Are they comparable? (Think about the ground truths of both models).\n\nPropose a way to fairly evaluate the models.","metadata":{"id":"AMpXErqPBv2I"}},{"cell_type":"markdown","source":"<b>Write your answer here :</b>\n\nWangchanBERTa-base-att-spm-uncased (SPM): 91.89% due to its ability to handle complex terms and unseen words.\\\nWangchanBERTa-base-wiki-newmm (Newmm): 83.60% aligns better with human-readable segmentation ('โดย' instead of '▁โดย')\\\nattributed to its subword tokenization handling complex terms better, while Newmm’s word-level tokenization aligned more closely with the ORCHID corpus’s original segmentation but struggled with unseen words. \n\nTo fairly evaluate models with differing tokenization strategies, predictions should be mapped to the dataset’s original word-level labels, alongside stratified evaluation of POS categories, cross-validation, error analysis, and statistical testing. \n\nWhile SPM’s flexibility enhances performance, Newmm’s alignment with human-interpretable word tokens offers practical advantages.","metadata":{"id":"UigMzZVNCDuS"}},{"cell_type":"markdown","source":"A note on preprocessing data.\n\n``process_transformers`` in ``thaixtransformers.preprocess`` also provides a preprocess code that deals with many issues such as casing, text cleaning, and white space replacement with <_>. You can also use this to preprocess your text. Note that space replacement is done automatically without preprocessing in thaixtransformers.\n","metadata":{"id":"bYN0X9HWVuUe"}}]}