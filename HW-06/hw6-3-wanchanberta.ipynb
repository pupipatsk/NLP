{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ8FRFIYMc5X"
   },
   "source": [
    "# HOMEWORK 6: TEXT CLASSIFICATION\n",
    "In this homework, you will create models to classify texts from TRUE call-center. There are two classification tasks:\n",
    "1. Action Classification: Identify which action the customer would like to take (e.g. enquire, report, cancle)\n",
    "2. Object Classification: Identify which object the customer is referring to (e.g. payment, truemoney, internet, roaming)\n",
    "\n",
    "We will focus only on the Object Classification task for this homework.\n",
    "\n",
    "In this homework, you are asked compare different text classification models in terms of accuracy and inference time.\n",
    "\n",
    "You will need to build 3 different models.\n",
    "\n",
    "1. A model based on tf-idf\n",
    "2. A model based on MUSE\n",
    "3. A model based on wangchanBERTa\n",
    "\n",
    "**You will be ask to submit 3 different files (.pdf from .ipynb) that does the 3 different models. Finally, answer the accuracy and runtime numbers in MCV.**\n",
    "\n",
    "This homework is quite free form, and your answer may vary. We hope that the processing during the course of this assignment will make you think more about the design choices in text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-14T07:03:53.369323Z",
     "iopub.status.busy": "2025-02-14T07:03:53.369000Z",
     "iopub.status.idle": "2025-02-14T07:03:54.635897Z",
     "shell.execute_reply": "2025-02-14T07:03:54.635100Z",
     "shell.execute_reply.started": "2025-02-14T07:03:53.369292Z"
    },
    "id": "kHqkFSyaNvOt",
    "outputId": "879b17f1-0fb2-455c-ca37-b5a4aecd7b1c",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-14 07:03:53--  https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.dropbox.com/scl/fi/8h8hvsw9uj6o0524lfe4i/clean-phone-data-for-students.csv?rlkey=lwv5xbf16jerehnv3lfgq5ue6 [following]\n",
      "--2025-02-14 07:03:53--  https://www.dropbox.com/scl/fi/8h8hvsw9uj6o0524lfe4i/clean-phone-data-for-students.csv?rlkey=lwv5xbf16jerehnv3lfgq5ue6\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc279ae1207599e9d9326875105f.dl.dropboxusercontent.com/cd/0/inline/CkH0W4KoKEeh3GbbXsnCY1KAfk4Byed22ey5qjE_wN9TVvHj71kfAdPkDI7F7HLgZNUi_w5EgTmOKEajEE9E01F2yz52dY8VZcxwtbc5o0BsELsNWccsg8xqOJAnJObI7SI/file# [following]\n",
      "--2025-02-14 07:03:54--  https://uc279ae1207599e9d9326875105f.dl.dropboxusercontent.com/cd/0/inline/CkH0W4KoKEeh3GbbXsnCY1KAfk4Byed22ey5qjE_wN9TVvHj71kfAdPkDI7F7HLgZNUi_w5EgTmOKEajEE9E01F2yz52dY8VZcxwtbc5o0BsELsNWccsg8xqOJAnJObI7SI/file\n",
      "Resolving uc279ae1207599e9d9326875105f.dl.dropboxusercontent.com (uc279ae1207599e9d9326875105f.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
      "Connecting to uc279ae1207599e9d9326875105f.dl.dropboxusercontent.com (uc279ae1207599e9d9326875105f.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2518977 (2.4M) [text/plain]\n",
      "Saving to: ‘clean-phone-data-for-students.csv’\n",
      "\n",
      "clean-phone-data-fo 100%[===================>]   2.40M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2025-02-14 07:03:54 (32.7 MB/s) - ‘clean-phone-data-for-students.csv’ saved [2518977/2518977]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate https://www.dropbox.com/s/37u83g55p19kvrl/clean-phone-data-for-students.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-02-14T07:03:54.637268Z",
     "iopub.status.busy": "2025-02-14T07:03:54.636984Z",
     "iopub.status.idle": "2025-02-14T07:04:00.111816Z",
     "shell.execute_reply": "2025-02-14T07:04:00.110802Z",
     "shell.execute_reply.started": "2025-02-14T07:03:54.637244Z"
    },
    "id": "qRlx5Mb5zkXw",
    "outputId": "18d913e0-aa6d-435b-931d-591386cb4ba8",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pythainlp\n",
      "  Downloading pythainlp-5.0.5-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from pythainlp) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->pythainlp) (2025.1.31)\n",
      "Downloading pythainlp-5.0.5-py3-none-any.whl (17.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.9/17.9 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pythainlp\n",
      "Successfully installed pythainlp-5.0.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pythainlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YprqbOPMc5a"
   },
   "source": [
    "## Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T07:04:00.113942Z",
     "iopub.status.busy": "2025-02-14T07:04:00.113694Z",
     "iopub.status.idle": "2025-02-14T07:04:04.845378Z",
     "shell.execute_reply": "2025-02-14T07:04:04.844634Z",
     "shell.execute_reply.started": "2025-02-14T07:04:00.113919Z"
    },
    "id": "heICP79cMc5e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from IPython.display import display\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T07:04:04.847229Z",
     "iopub.status.busy": "2025-02-14T07:04:04.846749Z",
     "iopub.status.idle": "2025-02-14T07:04:04.850628Z",
     "shell.execute_reply": "2025-02-14T07:04:04.849733Z",
     "shell.execute_reply.started": "2025-02-14T07:04:04.847197Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPaUf4PLMc5k"
   },
   "source": [
    "## Loading data\n",
    "First, we load the data from disk into a Dataframe.\n",
    "\n",
    "A Dataframe is essentially a table, or 2D-array/Matrix with a name for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T07:04:04.851542Z",
     "iopub.status.busy": "2025-02-14T07:04:04.851320Z",
     "iopub.status.idle": "2025-02-14T07:04:04.924414Z",
     "shell.execute_reply": "2025-02-14T07:04:04.923708Z",
     "shell.execute_reply.started": "2025-02-14T07:04:04.851520Z"
    },
    "id": "JhZ2eBAWMc5l",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('clean-phone-data-for-students.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cje3yruTMc5p"
   },
   "source": [
    "Let's preview the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "execution": {
     "iopub.execute_input": "2025-02-14T07:04:04.925287Z",
     "iopub.status.busy": "2025-02-14T07:04:04.925033Z",
     "iopub.status.idle": "2025-02-14T07:04:04.975270Z",
     "shell.execute_reply": "2025-02-14T07:04:04.974430Z",
     "shell.execute_reply.started": "2025-02-14T07:04:04.925266Z"
    },
    "id": "aNqRNz1PMc5q",
    "outputId": "e129a502-1420-476c-dc50-46c293a01b56",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Utterance</th>\n",
       "      <th>Action</th>\n",
       "      <th>Object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;PHONE_NUMBER_REMOVED&gt; ผมไปจ่ายเงินที่ Counte...</td>\n",
       "      <td>enquire</td>\n",
       "      <td>payment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>internet ยังความเร็วอยุ่เท่าไหร ครับ</td>\n",
       "      <td>enquire</td>\n",
       "      <td>package</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้...</td>\n",
       "      <td>report</td>\n",
       "      <td>suspend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อ...</td>\n",
       "      <td>enquire</td>\n",
       "      <td>internet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโ...</td>\n",
       "      <td>report</td>\n",
       "      <td>phone_issues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Sentence Utterance   Action        Object\n",
       "0   <PHONE_NUMBER_REMOVED> ผมไปจ่ายเงินที่ Counte...  enquire       payment\n",
       "1               internet ยังความเร็วอยุ่เท่าไหร ครับ  enquire       package\n",
       "2   ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้...   report       suspend\n",
       "3   พี่ค่ะยังใช้ internet ไม่ได้เลยค่ะ เป็นเครื่อ...  enquire      internet\n",
       "4   ฮาโหล คะ พอดีว่าเมื่อวานเปิดซิมทรูมูฟ แต่มันโ...   report  phone_issues"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Utterance</th>\n",
       "      <th>Action</th>\n",
       "      <th>Object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16175</td>\n",
       "      <td>16175</td>\n",
       "      <td>16175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13389</td>\n",
       "      <td>10</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>บริการอื่นๆ</td>\n",
       "      <td>enquire</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>97</td>\n",
       "      <td>10377</td>\n",
       "      <td>2525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence Utterance   Action   Object\n",
       "count               16175    16175    16175\n",
       "unique              13389       10       33\n",
       "top           บริการอื่นๆ  enquire  service\n",
       "freq                   97    10377     2525"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the top 5 rows\n",
    "display(data_df.head())\n",
    "# Summarize the data\n",
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGd8BNvMMc5y"
   },
   "source": [
    "## Data cleaning\n",
    "\n",
    "We call the DataFrame.describe() again.\n",
    "Notice that there are 33 unique labels/classes for object and 10 unique labels for action that the model will try to predict.\n",
    "But there are unwanted duplications e.g. Idd,idd,lotalty_card,Lotalty_card\n",
    "\n",
    "Also note that, there are 13389 unqiue sentence utterances from 16175 utterances. You have to clean that too!\n",
    "\n",
    "## #TODO 0.1:\n",
    "- You will have to remove unwanted label duplications as well as duplications in text inputs.\n",
    "- Also, you will have to trim out unwanted whitespaces from the text inputs.\n",
    "\n",
    "This shouldn't be too hard, as you have already seen it in the demo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "execution": {
     "iopub.execute_input": "2025-02-14T07:04:04.976584Z",
     "iopub.status.busy": "2025-02-14T07:04:04.976253Z",
     "iopub.status.idle": "2025-02-14T07:04:05.004056Z",
     "shell.execute_reply": "2025-02-14T07:04:05.003458Z",
     "shell.execute_reply.started": "2025-02-14T07:04:04.976549Z"
    },
    "id": "V0bGLblVMc5z",
    "outputId": "1a65aff5-6196-4674-fb5d-36aa1afcfdba",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Utterance</th>\n",
       "      <th>Action</th>\n",
       "      <th>Object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16175</td>\n",
       "      <td>16175</td>\n",
       "      <td>16175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13389</td>\n",
       "      <td>10</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>บริการอื่นๆ</td>\n",
       "      <td>enquire</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>97</td>\n",
       "      <td>10377</td>\n",
       "      <td>2525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sentence Utterance   Action   Object\n",
       "count               16175    16175    16175\n",
       "unique              13389       10       33\n",
       "top           บริการอื่นๆ  enquire  service\n",
       "freq                   97    10377     2525"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['payment', 'package', 'suspend', 'internet', 'phone_issues',\n",
       "       'service', 'nonTrueMove', 'balance', 'detail', 'bill', 'credit',\n",
       "       'promotion', 'mobile_setting', 'iservice', 'roaming', 'truemoney',\n",
       "       'information', 'lost_stolen', 'balance_minutes', 'idd',\n",
       "       'TrueMoney', 'garbage', 'Payment', 'IDD', 'ringtone', 'Idd',\n",
       "       'rate', 'loyalty_card', 'contact', 'officer', 'Balance', 'Service',\n",
       "       'Loyalty_card'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['enquire', 'report', 'cancel', 'Enquire', 'buy', 'activate',\n",
       "       'request', 'Report', 'garbage', 'change'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data_df.describe())\n",
    "display(data_df.Object.unique())\n",
    "display(data_df.Action.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T07:04:05.006049Z",
     "iopub.status.busy": "2025-02-14T07:04:05.005832Z",
     "iopub.status.idle": "2025-02-14T07:04:05.045543Z",
     "shell.execute_reply": "2025-02-14T07:04:05.044874Z",
     "shell.execute_reply.started": "2025-02-14T07:04:05.006031Z"
    },
    "id": "19onNNUZMc54",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>clean_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13367</td>\n",
       "      <td>13367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13367</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>2108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       input clean_label\n",
       "count                                  13367       13367\n",
       "unique                                 13367          26\n",
       "top     สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ     service\n",
       "freq                                       1        2108"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['payment', 'package', 'suspend', 'internet', 'phone_issues',\n",
       "       'service', 'nontruemove', 'balance', 'detail', 'bill', 'credit',\n",
       "       'promotion', 'mobile_setting', 'iservice', 'roaming', 'truemoney',\n",
       "       'information', 'lost_stolen', 'balance_minutes', 'idd', 'garbage',\n",
       "       'ringtone', 'rate', 'loyalty_card', 'contact', 'officer'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO 1: Data Cleaning\n",
    "\n",
    "# Filter cols\n",
    "cols = [\"Sentence Utterance\", \"Object\"]\n",
    "data_df = data_df[cols]\n",
    "data_df.columns = ['input', 'raw_label']\n",
    "\n",
    "# Lowercase: label\n",
    "data_df['clean_label']=data_df['raw_label'].str.lower().copy()\n",
    "data_df.drop('raw_label', axis=1, inplace=True)\n",
    "\n",
    "# Trim white spaces: input\n",
    "data_df['input'] = data_df['input'].str.strip()\n",
    "\n",
    "# Remove duplicate: input\n",
    "data_df = data_df.drop_duplicates(subset=['input'], keep='first')\n",
    "\n",
    "# Display summary\n",
    "display(data_df.describe())\n",
    "display(data_df['clean_label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIxnPRiAmrhN"
   },
   "source": [
    "Split data into train, valdation, and test sets (normally the ratio will be 80:10:10 , respectively). We recommend to use train_test_spilt from scikit-learn to split the data into train, validation, test set.\n",
    "\n",
    "In addition, it should split the data that distribution of the labels in train, validation, test set are similar. There is **stratify** option to handle this issue.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "Make sure the same data splitting is used for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T07:04:05.046852Z",
     "iopub.status.busy": "2025-02-14T07:04:05.046625Z",
     "iopub.status.idle": "2025-02-14T07:04:05.097845Z",
     "shell.execute_reply": "2025-02-14T07:04:05.097032Z",
     "shell.execute_reply.started": "2025-02-14T07:04:05.046831Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Mappings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'payment',\n",
       " 1: 'package',\n",
       " 2: 'suspend',\n",
       " 3: 'internet',\n",
       " 4: 'phone_issues',\n",
       " 5: 'service',\n",
       " 6: 'nontruemove',\n",
       " 7: 'balance',\n",
       " 8: 'detail',\n",
       " 9: 'bill',\n",
       " 10: 'credit',\n",
       " 11: 'promotion',\n",
       " 12: 'mobile_setting',\n",
       " 13: 'iservice',\n",
       " 14: 'roaming',\n",
       " 15: 'truemoney',\n",
       " 16: 'information',\n",
       " 17: 'lost_stolen',\n",
       " 18: 'balance_minutes',\n",
       " 19: 'idd',\n",
       " 20: 'garbage',\n",
       " 21: 'ringtone',\n",
       " 22: 'rate',\n",
       " 23: 'loyalty_card',\n",
       " 24: 'contact',\n",
       " 25: 'officer'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'payment': 0,\n",
       " 'package': 1,\n",
       " 'suspend': 2,\n",
       " 'internet': 3,\n",
       " 'phone_issues': 4,\n",
       " 'service': 5,\n",
       " 'nontruemove': 6,\n",
       " 'balance': 7,\n",
       " 'detail': 8,\n",
       " 'bill': 9,\n",
       " 'credit': 10,\n",
       " 'promotion': 11,\n",
       " 'mobile_setting': 12,\n",
       " 'iservice': 13,\n",
       " 'roaming': 14,\n",
       " 'truemoney': 15,\n",
       " 'information': 16,\n",
       " 'lost_stolen': 17,\n",
       " 'balance_minutes': 18,\n",
       " 'idd': 19,\n",
       " 'garbage': 20,\n",
       " 'ringtone': 21,\n",
       " 'rate': 22,\n",
       " 'loyalty_card': 23,\n",
       " 'contact': 24,\n",
       " 'officer': 25}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Mappings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['payment', 'package', 'suspend', ..., 'balance', 'balance',\n",
       "       'package'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Mappings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 7, 7, 1], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "[['<PHONE_NUMBER_REMOVED> ผมไปจ่ายเงินที่ Counter Services เค้าเช็ต 3276.25 บาท เมื่อวานที่ผมเช็คที่ศูนย์บอกมียอด 3057.79 บาท'\n",
      "  0]\n",
      " ['internet ยังความเร็วอยุ่เท่าไหร ครับ' 1]\n",
      " ['ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้ ค่ะ' 2]\n",
      " ...\n",
      " ['ยอดเงินเหลือเท่าไหร่ค่ะ' 7]\n",
      " ['ยอดเงินในระบบ' 7]\n",
      " ['สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ' 1]]\n",
      "After\n",
      "[['<PHONE_NUMBER_REMOVED> ผมไปจ่ายเงินที่ Counter Services เค้าเช็ต 3276.25 บาท เมื่อวานที่ผมเช็คที่ศูนย์บอกมียอด 3057.79 บาท'\n",
      "  0]\n",
      " ['internet ยังความเร็วอยุ่เท่าไหร ครับ' 1]\n",
      " ['ตะกี้ไปชำระค่าบริการไปแล้ว แต่ยังใช้งานไม่ได้ ค่ะ' 2]\n",
      " ...\n",
      " ['ยอดเงินเหลือเท่าไหร่ค่ะ' 7]\n",
      " ['ยอดเงินในระบบ' 7]\n",
      " ['สอบถามโปรโมชั่นปัจจุบันที่ใช้อยู่ค่ะ' 1]]\n"
     ]
    }
   ],
   "source": [
    "# Mapping\n",
    "data = data_df.to_numpy()\n",
    "\n",
    "unique_label = data_df.clean_label.unique()\n",
    "\n",
    "label_2_num_map = dict(zip(unique_label, range(len(unique_label))))\n",
    "num_2_label_map = dict(zip(range(len(unique_label)), unique_label))\n",
    "\n",
    "print(\"Create Mappings\")\n",
    "display(num_2_label_map)\n",
    "display(label_2_num_map)\n",
    "\n",
    "print(\"Before Mappings\")\n",
    "display(data[:, 1])\n",
    "data[:,1] = np.vectorize(label_2_num_map.get)(data[:,1]) # Mapping...\n",
    "print(\"After Mappings\")\n",
    "display(data[:, 1])\n",
    "\n",
    "# Trim\n",
    "def strip_str(string):\n",
    "    return string.strip()\n",
    "print(\"Before\")\n",
    "print(data)\n",
    "data[:,0] = np.vectorize(strip_str)(data[:,0]) # Trimming...\n",
    "print(\"After\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T07:04:05.098785Z",
     "iopub.status.busy": "2025-02-14T07:04:05.098570Z",
     "iopub.status.idle": "2025-02-14T07:04:05.116454Z",
     "shell.execute_reply": "2025-02-14T07:04:05.115766Z",
     "shell.execute_reply.started": "2025-02-14T07:04:05.098766Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def filter_data(df):\n",
    "    class_counts = df[\"label\"].value_counts()\n",
    "    valid_classes = class_counts[class_counts >= 10].index\n",
    "    invalid_classes = class_counts[class_counts < 10].index\n",
    "\n",
    "    correct_classes = [i for i, _ in enumerate(valid_classes.sort_values())]\n",
    "    change_map = dict(zip(valid_classes.sort_values(), correct_classes))\n",
    "\n",
    "    # drop invalid classes\n",
    "    df = df[df[\"label\"].isin(valid_classes)]\n",
    "    for num in invalid_classes:\n",
    "        label = num_2_label_map[num]\n",
    "        del num_2_label_map[num]\n",
    "        del label_2_num_map[label]\n",
    "\n",
    "    # update change\n",
    "    df[\"label\"] = df[\"label\"].apply(lambda x: change_map.get(x, -1))\n",
    "    for old_num, new_num in change_map.items():\n",
    "        num_2_label_map[new_num] = num_2_label_map.pop(old_num)\n",
    "        label_2_num_map[num_2_label_map[new_num]] = new_num\n",
    "    return df\n",
    "\n",
    "df = pd.DataFrame(data, columns=['input', 'label'])\n",
    "df_filtered = filter_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T07:04:05.117492Z",
     "iopub.status.busy": "2025-02-14T07:04:05.117293Z",
     "iopub.status.idle": "2025-02-14T07:04:05.152512Z",
     "shell.execute_reply": "2025-02-14T07:04:05.151743Z",
     "shell.execute_reply.started": "2025-02-14T07:04:05.117474Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 10690\n",
      "Validation size: 1336\n",
      "Test size: 1337\n"
     ]
    }
   ],
   "source": [
    "# TODO: Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(data_df, random_state=SEED):\n",
    "    \"\"\"split_data splits the data into train:validation:test=8:1:1 sets.\"\"\"\n",
    "    X = data_df[\"input\"]\n",
    "    y = data_df[\"label\"]\n",
    "\n",
    "    # First split: Train (80%) and Temp (20%)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.20, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Second split: Validation (10%) and Test (10%)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Display dataset sizes\n",
    "    print(f\"Train size: {len(X_train)}\")\n",
    "    print(f\"Validation size: {len(X_val)}\")\n",
    "    print(f\"Test size: {len(X_test)}\")\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Split\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDHfX377rnp_"
   },
   "source": [
    "# Model 3 WangchanBERTa\n",
    "\n",
    "We ask you to train a WangchanBERTa-based model.\n",
    "\n",
    "We recommend you use the thaixtransformers fork (which we used in the PoS homework).\n",
    "https://github.com/PyThaiNLP/thaixtransformers\n",
    "\n",
    "The structure of the code will be very similar to the PoS homework. You will also find the huggingface [tutorial](https://huggingface.co/docs/transformers/en/tasks/sequence_classification) useful. Or you can also add a softmax layer by yourself just like in the previous homework.\n",
    "\n",
    "Which WangchanBERTa model will you use? Why? (Don't forget to clean your text accordingly).\n",
    "\n",
    "**Ans:** `airesearch/wangchanberta-base-att-spm-uncased` because\n",
    "- specifically trained for Thai text.\n",
    "- SentencePiece tokenization, which is ideal for Thai, unlike space-based tokenization.\n",
    "- It has state-of-the-art performance for Thai text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T07:04:05.153592Z",
     "iopub.status.busy": "2025-02-14T07:04:05.153339Z",
     "iopub.status.idle": "2025-02-14T07:04:05.165587Z",
     "shell.execute_reply": "2025-02-14T07:04:05.164738Z",
     "shell.execute_reply.started": "2025-02-14T07:04:05.153561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "for i in range(len(data)):\n",
    "    data[i][0] = data[i][0].replace('ํา', \"ำ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T07:04:05.166690Z",
     "iopub.status.busy": "2025-02-14T07:04:05.166391Z",
     "iopub.status.idle": "2025-02-14T07:04:13.528230Z",
     "shell.execute_reply": "2025-02-14T07:04:13.527392Z",
     "shell.execute_reply.started": "2025-02-14T07:04:05.166658Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpupipat-sk\u001b[0m (\u001b[33mpupipatsk\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "import wandb\n",
    "wandb.login(key=secret_value_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T07:04:13.530027Z",
     "iopub.status.busy": "2025-02-14T07:04:13.529199Z",
     "iopub.status.idle": "2025-02-14T07:04:13.619225Z",
     "shell.execute_reply": "2025-02-14T07:04:13.618247Z",
     "shell.execute_reply.started": "2025-02-14T07:04:13.529998Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def find_device() -> str:\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    print(f\"device: {device}\")\n",
    "    return device\n",
    "device = find_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T07:04:13.620627Z",
     "iopub.status.busy": "2025-02-14T07:04:13.620282Z"
    },
    "id": "ZI8SvILyub0m",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118e143df9144144bb71038117022cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/282 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02dbe13577634cb08f4fd7e6d61c3746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/546 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692d9284b60f441eae7a46f9379141b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/905k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca46c5d462ab40da81ad44573d7b4128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10690 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ac077a361847769e57d117ea1d5d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1336 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557e7c88161a43968dc950bd7cbe50d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8194a99fee442fa857be81fb11e7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/423M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250214_070439-4cg6xvfb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pupipatsk/huggingface/runs/4cg6xvfb' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/pupipatsk/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pupipatsk/huggingface' target=\"_blank\">https://wandb.ai/pupipatsk/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pupipatsk/huggingface/runs/4cg6xvfb' target=\"_blank\">https://wandb.ai/pupipatsk/huggingface/runs/4cg6xvfb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='841' max='840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [840/840 05:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.219834</td>\n",
       "      <td>0.365269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.281543</td>\n",
       "      <td>0.619760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.880400</td>\n",
       "      <td>0.954770</td>\n",
       "      <td>0.726796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.880400</td>\n",
       "      <td>0.853878</td>\n",
       "      <td>0.759731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.880400</td>\n",
       "      <td>0.833865</td>\n",
       "      <td>0.765719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the WangchanBERTa model\n",
    "MODEL_NAME = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenization function with dynamic padding\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)  # Removed \"max_length\" and fixed padding\n",
    "\n",
    "# Convert data into Hugging Face Dataset format\n",
    "train_data = Dataset.from_dict({\"text\": X_train.tolist(), \"label\": y_train.tolist()})\n",
    "val_data = Dataset.from_dict({\"text\": X_val.tolist(), \"label\": y_val.tolist()})\n",
    "test_data = Dataset.from_dict({\"text\": X_test.tolist(), \"label\": y_test.tolist()})\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_data.map(tokenize_function, batched=True)\n",
    "val_dataset = val_data.map(tokenize_function, batched=True)\n",
    "test_dataset = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Load pre-trained WangchanBERTa model for classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME,\n",
    "                                                           num_labels=len(num_2_label_map),\n",
    "                                                           id2label=num_2_label_map,\n",
    "                                                           label2id=label_2_num_map)\n",
    "model.to(device)\n",
    "\n",
    "# Define training arguments\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    ")\n",
    "\n",
    "# Compute accuracy\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "# Use dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Define trainer with dynamic padding\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator  # Enables dynamic padding\n",
    ")\n",
    "\n",
    "# Train model...\n",
    "print(\"Start training...\")\n",
    "trainer.train()\n",
    "print(\"Finish training.\")\n",
    "\n",
    "# Evaluate model on validation and test sets\n",
    "train_results = trainer.evaluate(train_dataset)\n",
    "val_results = trainer.evaluate(val_dataset)\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(f\"Train Accuracy: {train_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Test Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "end_time = time.time()\n",
    "print(f\"Total Time: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-14T07:29:47.728340Z",
     "iopub.status.busy": "2025-02-14T07:29:47.727964Z",
     "iopub.status.idle": "2025-02-14T07:30:11.468877Z",
     "shell.execute_reply": "2025-02-14T07:30:11.468201Z",
     "shell.execute_reply.started": "2025-02-14T07:29:47.728310Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7889\n",
      "Validation Accuracy: 0.7657\n",
      "Test Accuracy: 0.7450\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on validation and test sets\n",
    "train_results = trainer.evaluate(train_dataset)\n",
    "val_results = trainer.evaluate(val_dataset)\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(f\"Train Accuracy: {train_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Test Accuracy: {test_results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qr9_0DnMBcFZ"
   },
   "source": [
    "# Comparison\n",
    "\n",
    "After you have completed the 3 models, compare the accuracy, ease of implementation, and inference speed (from cleaning, tokenization, till model compute) between the three models in mycourseville."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1: TfidfVectorizer + Logistic Regression + pythainlp.word_tokenize\n",
    "- Training time: 1.5351 seconds\n",
    "- Train Accuracy: 0.7675\n",
    "- Validation Accuracy: 0.6894\n",
    "- Test Accuracy: 0.6911\n",
    "\n",
    "Model 2: MUSE + Logistic Regression\n",
    "- Encoding Time: 47.9169 seconds\n",
    "- Training Time: 25.3683 seconds\n",
    "- Train Accuracy: 0.7367\n",
    "- Validation Accuracy: 0.7066\n",
    "- Test Accuracy: 0.7016\n",
    "- Total Time: 81.2206 seconds\n",
    "\n",
    "Model 3: WangchanBERTa\n",
    "- Train Accuracy: 0.7889\n",
    "- Validation Accuracy: 0.7657\n",
    "- Test Accuracy: 0.7450\n",
    "- Total Time: 5 min 10 sec\n",
    "\n",
    "Based on the performance of the three models, which one do you think is best for this use case (Callcenter Chatbot)?\n",
    "\n",
    "**Answer**: WangchanBERTa\n",
    "\n",
    "because we want CallcenterChatbot that doesn't need that fast respond(compare to current  answer bot when we call for customer service)\n",
    "- Highest accuracy, callcenter question can be repetitive question\n",
    "- Slow inference time, but can speedup by increase computation\n",
    "- Better generalization\n",
    "- Deep contextual understanding\n",
    "for handles nuances and variations in Thai language"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
